---
title: "Session Regularization"
output:
  bookdown::html_document2:
    code_folding: hide
    keep_md: false
    number_sections: false
    toc: TRUE
editor_options:
  chunk_output_type: console
---

```{r setup, message=FALSE, echo=FALSE, show=FALSE}
#setwd("workshop-mlbiostatistics/session-regularization")

library(knitr)
opts_chunk$set(echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE, out.width='100%', fig.height=3) #, fig.width=6)#, knitr.kable.NA = '-')
```

### Learning outcomes

* Likelihood
    + Relation to OLS for linear model
    + Likelihood and an orientation to Bayesian statistics
* Overfitting
* Regularization
    + relation to Bayesian statistics
    + Model comparison 
        - AIC
    + Feature selection
        - LASSO, ridge regression
* Cross validation



# Likelihood

<details>
<summary> Lecture notes </summary>

Likelihood is concerned with estimating how likely it is that a certain model or certain model parameter values are *true* ones.

Consider a generative model, with parameters $\theta$, for how $X\rightarrow Y$. We would like to test if $\theta$ are good parameters. Given the model, we can compute
$Pr[Y| X, \theta]$,
that is, the probability that the model with parameters $\theta$ and given $X$ generates $Y$. 

Likelihood builds on the intuition that if $\theta$ (or the model) is close to the 'truth', then $Pr[Y| X, \theta]$ will be higher than for wrong $\theta$ (model). We should therefore select the $\theta$ that maximizes $Pr[Y| X, \theta]$; this is called maximum likelihood estimation (MLE) of $\theta$.

Since statistical model contain an element of randomness, the reasoning above might not always be correct for any single obeservation. However, if we sum over a large number of observations it will be true on average. Hence the need for datasets that are large enough.

To formalize this intuition, Edwards (1972) defined the likelihood of model parameters being true given observed data as

$$L[\theta|Y,X] \propto Pr[Y|X, \theta]$$
Notice that this means that $L[\theta|Y,X]$ is not a proper probability, hence the term *likelihood* is used.
<details>
<summary> Extra Reading </summary>

Notice that this notation is not uncommonly mixed up, so you might also see the notation $L[Y|X,\theta]$ for the likelihood.

Similarly $\propto Pr[Y|X, \theta]$ is often referred to as the *likelihood function*.

***
</details>

In practice, the proportionality is ignored and we set

$$L[\theta|Y,X] = Pr[Y|X, \theta]$$

<details>
<summary> Extra Reading </summary>
The proportionality (indicated by '$\propto$') means there are some unknown constant factor, $k$, such that $L[\theta|Y,X] = k Pr[Y|X, \theta]$. However, the factor $k$ is assumed to be constant over $\theta$s and over models. 

When the likelihood of two $\theta$s (or models) are compared this is almost always done as a _likelihood ratio_, 

$$\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]} = \frac{k Pr[Y|X, \theta_1]}{ k  Pr[Y|X, \theta_0]} =\frac{Pr[Y|X, \theta_1]}{ Pr[Y|X, \theta_0]}$$

which means that the factor $k$ disappears. Hence the factor $k$ is always ignored. Likelihood ratios is the basis of most model comparison statistics, e.g., the Wald test, the Score test, regularization... 

***
</details>

In maximum likelihood estimation of some parameters $\theta$, one simply selects the estimates $\widehat\theta$ that gives the highest likelihood $max_{\theta}L[\theta|X,Y] = L[\widehat\theta|X,Y]$. In many applications of  likelihood and maximum likelihood, it is practical to instead use the logarithm of the likelihood, the logLikelihood, $\log L[\theta_1|Y,X]$.
<details>
<summary> Extra Reading </summary>
As mentioned above, the logarithm of the likelihood, the logLikelihood, $\log L[\theta_1|Y,X]$, or sometimes the negative logLikelihood, $-\log L[\theta_1|Y,X]$, is often used. Notice, that 

1. The $\theta$ estimates that maximizes  $\log L[\theta|Y,X]$ also maximizes $L[\theta|Y,X]$
2. The $\theta$ estimates that minimizes $-\log L[\theta|Y,X]$ maximizes $L[\theta|Y,X]$
3. A likelihood ratio corresponds to a logLikelihood difference, $$\log\left(\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]}\right) = \frac{\log L[\theta_1|Y,X]}{\log L[\theta_0|Y,X]} = \log L[\theta_1|Y,X] - \log L[\theta_0|Y,X]$$.

***
</details>

Likelihood and maximum likelihood estimation are central concepts in statistics. Many statistical tests and methods uses or is based on the concept of maximum likelihood.

(In the following, I will simplify notation and not differentiate between etimates and random variables, e.g., $\theta$ will be used also for $\widehat\theta$.)

</details>

## Likelihood | `Likelihood and OLS for linear models`

<details>
<summary> Lecture notes </summary>

So, why have we used ordinary least squares (OLS), i.e., minimization of  RSS when estimating linear model parameters $\beta$ rather than maximum likelihood estimation?

Linear models is a special case with some nice properties when it comes to  likelihood. Consider a simple linear regression model,

$$ y = \beta x + \epsilon, $$

where the residuals $\epsilon\sim N(0,\sigma^2)$. 

It turns out that the  likelihood estimates of both $\beta$ and $\sigma^2$ are functions of the  RSS of the residuals, so that the likelihood can be approximated by

$$  \log L[\beta, \sigma^2|Y,X] \approx -\frac{N}{2} \log RSS$$

<details>
<summary> Extra Reading </summary>

The likelihood for given $\beta$ and $\sigma^2$, given observed data $Y$ and $X$ is given by

$$ L[\beta, \sigma^2|Y,X] = \prod_i pdf_{Normal}(y_i, \mu=\beta x_i, \sigma^2=\sigma^2) = \prod_i \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(y_i-\beta x_i)^2}{2\sigma^"}} $$

where $pdf_{Normal}$ denotes the probability distribution function for the Normal distribution. If we work with the logLIkelihood instead, we get 

$$\begin{eqnarray*}
\log L[\beta, \sigma^2|Y,X] 
&=& \sum_{i=1}^N \log\left(\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(y_i-\beta x_i)^2}{2\sigma^2}}\right)\\
&=&   \sum_{i=1}^N \log \left(\frac{1}{\sqrt{2\pi \sigma^2}}\right) -\frac{(y_i-\beta x_i)^2}{2\sigma^2} \\
&=&   N\log \left(2\pi \sigma^2\right)^{-1/2} -\frac{\sum_{i=1}^N (y_i-\beta x_i)^2}{2\sigma^2} \\
&=&   -\frac{N}{2}\log \left(2\pi \sigma^2\right)  -\frac{RSS}{2\sigma^2}
\end{eqnarray*}$$

We see here that minimizing $RSS$ (as in OLS) will maximize the logLikelihood, regardless of the value of $\sigma^2$. Moreover, it turns out that also $\sigma^2$ can be estimated fairly well by $RSS/N$. Hence, we get 

$$\begin{eqnarray*}
\log L[\beta, \sigma^2|Y,X]
&=&   -\frac{N}{2}\log \left(\frac{2\pi RSS}{N}\right)  -\frac{N}{2}\frac{RSS}{RSS}\\
&=&   -\frac{N}{2}\log RSS + \frac{N}{2}\log \frac{2\pi}{N} -\frac{N}{2}\\
&=&   -\frac{N}{2}\log RSS + C
\end{eqnarray*}$$
where $C=\frac{N}{2}\left(\log \frac{2\pi}{N} -1\right)$ is a constant that is usually ignored (in likelihood ratios, which is equivalent to log likelihoods differences, it will disappear).

***
</details>

This means that the maximum likelihood estimates of $\beta$ is exactly the same as those of the minimum RSS.

_**NB!** This is a special case for linear models and are not generally true for other models. For example, logistic regression is typically fitted using maximizing the likelihood _

In general, full-on likelihood computation and maximum likelihood estimation is relatively slow, so alternative and faster methods has been developed, e.g., OLS.

</details>

## Bayesian approach

<details>
<summary> Lecture notes </summary>

Bayes' theorem (Thomas Bayes, 1702-1761) provides a way to obtain the requested $P[\theta|X,Y]$


$$Pr[\theta|D] = \frac{Pr[D| \theta]Pr[\theta]}{Pr[D]}$$
**Posterior probability**

$Pr[\theta|D],$ the probability, computed posterior to analysis, of the parameters $\theta$ conditioned on the observed data, i.e, our requested probability.

An important characteristic of Bayesian statistics is that the focus is not on point estimates, but on the posterior probability distribution over the parameter space of $\theta$, which provides a measure of uncertainty (probabilities) in comparison to other values.

```{r, echo=FALSE}
# fake plots for illustration purpose
theta = seq(0,10, length=100)
par(mfcol=c(1,2))
pp_theta = 75*dnorm(theta,mean=4, sd=0.01)
plot(theta,pp_theta, type='l', ylim=c(0,1))

pp_theta = dnorm(theta,mean=4, sd=5)
plot(theta,pp_theta, type='l', ylim=c(0,1.0))

```

**Prior probability of $\theta$**

$Pr[\theta]$ is the *prior* probability of $\theta$ and should according to Bayesian statistics reflect what we know (or believe to know) about how close $\theta$ is to the true parameters. We can use information from previous studies or we can assign a *uninformative* prior, e.g., $Pr[\theta]$ follows a uniform distribution for all $\theta$ in the interval $[a,b]$. 

It can be shown that the effect of the prior on the posterior probsbiity is largest when the observed data is small. With larger sample sizes, the posterior probability will eventually just depend on $Pr[D|\theta]$.

**Marginal Probability of $D$**

$Pr[D]=\int_{\theta}Pr[D| \theta]Pr[\theta]$ is the probability of $D$ regardless of $\theta$. This can often be difficult difficult to calculate and, for this reason, Bayesian models are often designed so that this can be calculated analystically or some approximation approach, such as Markov chain Monte Carlo (MCMC) is used.

<details>
<summary> Extra reading </summary>

**Probabilistic algebra**

A conditional probability $Pr[A|B]$ is the probability that $A$ happens if we know that $B$ has happened.
To obtain the probability that both $A$ and $B$ happens we need to first take the probability that $B$ happens and then multiply it with the conditional probability that $A$ hapens given $B$, i.e.,:

$$Pr[A,B] = Pr[A|B] Pr[B].$$

From this follows the reverse operation

$$\frac{Pr[A,B]}{Pr[B]} = Pr{A|B}$$
Notice that this also works if we have more than one condition:
 $Pr[A|B,C] * Pr[B] = Pr[A,B|C].$



What happens in Bayes rule is that we first, in the numerator, perform $Pr[B|A]*Pr[A] = Pr[A,B]$ and then divide this with the denominator $\frac{Pr[A,B]}{Pr[B]} = Pr[A|B]$.

***

</details>

***

</details>

## Bayesians vs frequentists

<details>
<summary> Lecture notes </summary>

There is often described a severe controversy between Bayesians and frequentists. However, this controversy represents the extreme hardcore Bayesians and frequentists.

In reality, there is a large gray-zone where frequentists and Bayesians meet and socialize:

* Bayesian models can be viewed as a type of the hierarchical models often used by frequentists
* Frequentist bootstrap analysis is often used to estimate uncertainty of point estimates in relation to alternatives, as is done in Bayesian statistics
* The *Bayes factor* is a Bayesian  version of the likelihood ratio
* Bayesian *posterior intervals* corresponds to frequentist *confidence intervals* (*Note* however, that there are no Bayesian significance test)
* etc.

Most practical statisticians use the tool that is adequate for the problem at hand, whether it is Bayesian or frequentist.

***
</details>


#  Overfitting

We will now look at a general problem in statistical modeling that can be visualized quite well with Likelihoods. We will later look at some solutions to this problem.

## Overfitting | `Example data`

First, you need some test data to play around with. For simplicity and convenience, you will simulate a toy data from a linear model and use this in the exercises. The advantage for us using simulated data is that we know the 'truth', i.e., how the data was simulated and we therefore have *oracle knowledge* about the true parameter values, e.g., for $\beta$.

### Task | `simulation of example data`
* The data should comprise 100 samples. 
* First generate 10 variables $(x_1,x_2,\ldots, x_{0})$ from a uniform distribution (use the function `runif`) and store them in a Matrix $X$. 
* Use an intercept $\beta_0=3$ 
* Generate effect sizes $\beta_1, \beta_2, \beta_3$ from a Uniform distribution in the interval $(0.5, 1.0)$ for the 3 first $X$ variable (use the function `runif`); record the 'true' effect sizes for reference.
* Finally generate outcome variable $Y$ using a linear model $Y = \beta_0 + \beta_1 x_i + \beta_2 x_2 + \beta_3 x_3 + \epsilon$, with $\epsilon\sim N(0,\sigma^2=1)$ (i.e., the residuals are drawn from a Normal distribution with mean=0 and standard deviation=1, *Tip:* use the R function `rnorm`).

```{r, echo=TRUE}
# To obtain exactly the same result as in the demo, set seed to 85
set.seed(85)
```

```{r, echo=TRUE}
N=100 # number of samples
P=10 # number of variables

# Draw variables, x_{i,1},...,x_{i,P} for all N individuals, from a uniform distribution in interval (0,1) (this is the default interval for runif)
X=matrix(round(runif(N*(P+1),min=0, max=2)), nrow=N, ncol=P)

# generate a y variable from a multivarite lm of 3 first X variables only
# intercept
b0=3
# effect sizes for first three variables
b=c(runif(3, min=0.5, max=1.0))

# generate y
Y <- b0 + X[,1] * b[1] + X[,2] * b[2] + X[,3] * b[3] + rnorm(N)

```

#### Think about:

* What can simulation be used for?

<details>
<summary> Some possible answers </summary>
<h4>Some possible answers</h4>

* Oracle knowledge when evaluating performance of methods, e.g., Type I and II errors 
* Estimating probabilities and probability distributions of, e.g., data and summary statistics of data

***

</details>
## Overfitting | `Model comparison`

Now consider the following two models for our data

\begin{eqnarray}
y & \sim & \beta_0 + \beta_1 x_1 & (1) \\
y & \sim &  \beta_0 + \beta_1 x_1 + \beta_2 x_2 & (2)
\end{eqnarray}

What are the max Likelihood estimates of the two models? (we can use the R function `logLik` in the `stats` package)

### Task | `plot two likelihoods`
* Create `lm` models for the two models, and
* store the likelihood (use `logLik`) in a vector
* plot the likelihoods

```{r,echo=T, fig.height=4, echo=TRUE, eval=FALSE}
library(stats)
ll= vector()
for(i in seq(1,2)){
  Xi=X[,seq(1,i)]
  ll[i] <- logLik(lm(Y~Xi))
}
# plot likelihoods for models with 1 and 2 vaiables
plot(ll, ylab="log L", xlab="model #", type = "b", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
# xlim and ylim not really necessary here, but I can reuse the plot statement below, so the plots look similar
```

<details>
<summary> *Show result*</summary>
```{r,echo=T, fig.height=4, echo=FALSE}
library(stats)
ll= vector()
for(i in seq(1,P)){
  Xi=X[,seq(1,i)]
  ll[i] <- logLik(lm(Y~Xi))
}
# plot likelihoods for models with 1 and 2 vaiables
plot(ll[1:2], ylab="log L", xlab="model #", type = "b", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
# xlim and ylim not really necessary here, but I can reuse the plot statement below, so the plots look similar
```

***
</details>

... 2 variables are clearly better than 1 variable -- What if we add more variables?

### Task | `plot all likelihoods`

* Now repeat this for the sequence of models obtained by creating the next model by simply adding the next $X$ variable in order.

```{r,echo=T, fig.height=4, echo=TRUE, eval=FALSE}
# compute loglikelihood (ll) for all models including variables
# 1-i, for i <= P; store results in vector ll
ll= vector()
for(i in seq(1,P)){
  Xi=X[,seq(1,i)]
  ll[i] <- logLik(lm(Y~Xi))
}

# plot ll for all models
plot(ll, ylab="log L", xlab="model #", type = "b", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 

```

<details>
<summary> *Show result*</summary>
```{r,echo=T, fig.height=4, echo =FALSE}
# compute loglikelihood (ll) for all models including variables
# 1-i, for i <= P; store results in vector ll
ll= vector()
for(i in seq(1,P)){
  Xi=X[,seq(1,i)]
  ll[i] <- logLik(lm(Y~Xi))
}

# plot ll for all models
plot(ll, ylab="log L", xlab="model #", type = "b", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 

```

***
</details>

#### Think about:

* How does the Likelihood behave as more variables are added?

* Which is the maximum likelihood model? Is this correct given our *oracle knowledge*?

* What could be the problem with this behaviour? How would we like it to behave?

* How can we obtain the desired behaviour?

<details>
<summary> Some possible answers </summary>
<h4>Some possible answers</h4>
_Nested models_

* Model (1) can be described as a special case of Model (2) with the constraints on $\beta_2=0$

* Therefore Model (2) will always have equal or better ML than Model (1)


_Overfitting_

* Using our *oracle knowledge*, we know that the simulated data was generated from the 3 first variables
  + thus, the subsequent variables increase ML by modeling noise in data
  
* This is difficult to detect by just looking at the likelihoods

* Solutions
  + Seek the simplest model that is "good enough" -> Regularization

***
</details>


<details>
<summary> Extra Reading </summary>
<h2> Model comparison | `Likelihood ratio test`</h1>

For nested models $-2 \max LRT$ is $\chi^2(d)$-distributed, with $d=$ the difference in free params in the two models.

```{r, lrt, echo=F}
library(lmtest)
mprev <- lm(Y ~ X[,1])
lrt=data.frame(models=0, ll1=0, ll2=0, lr=0, P = 0, sign=0)
for(i in seq(2,P)){
  m <- lm(Y ~ X[,seq(1,i)])
  fit=lrtest(mprev,m)
  mprev=m
  sign = ifelse(fit$`Pr(>Chisq)`[2]>0.05,"no","yes")#ifelse(fit$`Pr(>Chisq)`[2]>0.01,"*",ifelse(fit$`Pr(>Chisq)`[2]>0.001, "**","***")))
  lr=fit$LogLik[1]-fit$LogLik[2]
  lrt[i-1,] = list(paste0(i-1," vs ", i," variables"), signif(fit$LogLik[1], 5), signif(fit$LogLik[2], 5), format(lr, digits=4), format(fit$`Pr(>Chisq)`[2],digits=3, scientific=-1), sign)
}
library(kableExtra)
kable(lrt,"html",row.names=F, col.names=c("Compared models","logL 1st model","logL 2nd model","logLR", "P-value", "Sign at 0.05"),digits=30, format.args=list(snsmall=0)) %>% kable_styling(bootstrap="striped", font_size = 14, full_width=F)

```
In our simple test case, the LRT also succeed in picking the correct model. It should be noted that certain issues, such as *lnkage disequiibriium*, may cause problems for LRT (*the example is not optimized to show this*).


***
</details>

#  Regularization

<details>
<summary> Lecture notes </summary>

Regularization is a concept that adds auxiliary criteria, so-called *regularization terms*,  to probabilistic models.  This is called regularized likelihood models or penalized likelihood models. Typically, the regularization term is a function of parameters $\beta$:

$$\log rL[\beta | X, Y]  = \log Pr[Y | X, \beta] - f(\beta),$$


A very simple regularized likelihood model uses $f(\beta) = \#\beta = \#X$, that is the number of $X$ variables.  
$$\log rL[{\beta} | X, Y]  = \log Pr[Y | X, {\beta}] - \#X, $$


Applying this rL to our example data, solves the overfitting problem.

```{r,echo=F, fig.height=4, echo=TRUE}
# compute loglikelihood (ll) for all models including 1-P variables
pl= vector() 
for(i in seq(1,P)){
  xi=X[,seq(1,i)]
  xi=cbind(rep(1,N), xi)
  fit = lm(Y~xi)
  # To make the code simple, we forestall next step and use the AIC function here
  # AIC= -2(pl) so convert back
  pl[i] = -AIC(fit)/2
}
# plot ll of all models
plot(pl, xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab="log pL", xlab="model #", type = "b")
```  

</details>

## Regularization | `Bayesian interpretation`
<details>
<summary> Lecture notes </summary>

Regularization is a canonical example where Bayesian and frequentist statistics meet.

The standard way of writing a regularized likelihood is using the logLikelihood, but what if 'de-log' it:

\begin{eqnarray*}
\log rL[\beta | X, Y]  &=& \log Pr[Y | X, \beta] - f(\beta) \\
\Downarrow\\
rL[\beta | X, Y]  &=& Pr[Y | X, \beta] * e^{- f(\beta)}
\end{eqnarray*}

This looks suspiciously like an un-normalized posterior probability (i.e., lacking the denominator), with an exponential prior $Pr[\beta]=e^{-f(\beta)}.$

As we will see examples of, most regularization techniques have a Bayesian interpretation.


In fact, a standard solution overfitting and, more generally, over-parameterization, i.e., problems where the likelihood function may not have a unique maximmum, is to include prior information, either as Bayesian priors or regularization terms to limit the parameter space. This is an area where Bayesian and frequentist socialize and get on well.

</details>


##  Regularization | `AIC and model testing`

<details>
<summary> Lecture notes </summary>

Coming from a _information theory_ base, Hirotugu Akaike came up with a very similar approach for the overfitting problem.

The Akaike information criterion (AIC), for a model $m$ with variables $X$, is defined as

  $$AIC_m = 2\# X - 2\log \max L[{\beta}|X,Y]$$

We see that $AIC_m = -2 \left(\log \max L[{\beta}|X,Y] - \#X\right)$, i.e., $-2$ times the the simple $\log rL$, we just looked at in our first regularization example. 
    
<details>
<summary> Extra Reading </summary>

The difference in $AIC$ between two models is claimed to estimate the information lost by selecting the worse model.

***
</details>
    
Sometimes, the *relative likelihood* for model $m$ is used, which is
      $$relL = e^\frac{ AIC_{min} - AIC_{m} }{2}$$
where $AIC_{min}$ is the minimum AIC among a set of compared models
      
<details>
<summary> Extra Reading </summary>

* $relL$ can be interpreted as proportional to the probability that the model $m$ minimizes the information loss.
<!--       and can be interpreted as -->
<!-- $rL \propto Pr[m\textrm{ minimizes estimated information loss}]$. -->

   * Notice that
   
$$\log relL = \frac{\#X_m }{\#X_{min}}\log\frac{\max L[{\beta}_{m}|X_m,Y]}{\max L[{\beta}_{min}|X_{min},Y]}$$
  we see that $rL$ can be viewed as a  likelihood ratio weighted by the ratio of number of $X$ variables.
* However, AIC are not limited to nested models

***
</details>

</details>

### Task | `AIC analysis`

* A typical strategy is to select the model, $m$ with $AIC_m=AIC_{min}$ and then evaluate how much better it is than the other candidate models, e.g., using the $relL$.

* Apply this AIC strategy applied to our example data using the R funcion `AIC`
* create a table with the AIC and the $relL$ for the set of models comprising $\{X_1, .\ldots, X_i\} \textrm{ for } i \in [1, \ldots, 10]$; indicate also if a model is the minimum AIC model.

```{r, echo=T, eval=FALSE}
library(stats) 
library(dplyr)      # used for nice table formatting
library(kableExtra) # used for nice table formatting

mprev <- lm(Y ~ X[,1]) # current miminimum AIC model
# dummyentry to be replaced
aic=data.frame(models=0, aic=0, isAICmin="-") 

for(i in seq(1,P)){
  m <- lm(Y ~ X[,seq(1,i)])
  fit = AIC(mprev,m)
  mprev = m
  if(i==2){ #include also the first model
    aic[i-1,] = list(paste0(i-1," variable"), signif(fit$AIC[1],5), "-") 
  }
  aic[i,] = list(paste0(i," variables"), signif(fit$AIC[2],5), "-") 
}
minaic=min(aic$aic)
aic$rl=format(exp((minaic-aic$aic)/2), digits=4)
aic$isAICmin = ifelse(aic$aic==minaic,"Yes","-")

kable(aic, format='html', row.names=F, col.names=c("Compared models","AIC","Minimum AIC","rL"),digits=30,format.args=list(snsmall=0))  %>%  kable_styling( font_size = 14)
```

<details>
<summary> *Show result*</summary>
```{r, echo=FALSE}
library(stats)
library(dplyr)
library(kableExtra)
mprev <- lm(Y ~ X[,1])
aic=data.frame(models=0, aic=0, lowest="-")
for(i in seq(1,P)){
  m <- lm(Y ~ X[,seq(1,i)])
  fit=AIC(mprev,m)
  mprev=m
  if(i==2){
    aic[i-1,] = list(paste0(i-1," variable"), signif(fit$AIC[1],5), "-") 
  }
  aic[i,] = list(paste0(i," variables"), signif(fit$AIC[2],5), "-") 
}
minaic=min(aic$aic)
aic$rl=format(exp((minaic-aic$aic)/2), digits=4)
aic$lowest = ifelse(aic$aic==minaic,"Yes","-")

kable(aic, format='html', row.names=F, col.names=c("Compared models","AIC","Minimum AIC","rL"),digits=30,format.args=list(snsmall=0))  %>%  kable_styling( font_size = 14)
```

***
</details>

* Try to plot the $AIC$ and the $reL$ with the different models on the $X$-axis

```{r,echo=F, fig.height=4, echo=TRUE, eval=FALSE}
library(stats)

# plot AIC of all models
plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(aic$aic)),ceiling(max(aic$aic))),ylab="AIC", xlab="model #", type = "b")

# plot relL of all models
plot(aic$rl, xlim=c(1,P), ylab="relL", xlab="model #", type = "b")
```  

<details>
<summary> *Show result*</summary>
```{r,echo=F, fig.height=4, echo=FALSE}
library(stats)

# plot AIC of all models
plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(aic$aic)),ceiling(max(aic$aic))),ylab="AIC", xlab="model #", type = "b")

# plot relL of all models
plot(aic$rl, xlim=c(1,P), ylab="relL", xlab="model #", type = "b")
```  

***
</details>

#### Think about:

* Which is the best model? Is this correct compared to our *oracle knowledge*?
* How good is it compared to the others?
* Can you see a drawback in our model testing approach above? If so, how can we solve that?


<details>
<summary> Some possible answers </summary>
<h4>Some possible answers</h4>

* We see that the best model is the one with the 3 first X-variables (in line with our *oracle knowledge*) and that the second best model (with the first 2 X-variabels) is $\approx70\%$ worse.

<details>
<summary> Extra Reading </summary>

* Sometimes it is desirable to compute a significance for rejecting a model in favour of another model. A NULL distribution for the $relL$ statistic is usally obtained through simulation, e.g., using parameteric bootstrapping.

***
</details>

* Now, I this case we happened to know that the first 3 variables was the right one, so the order we choose to include them was correct. However, in the general case, we do not know this. How solve this?
  - Best subset method; involves testing all possible subsets, which is computationally time-consuming and dometimes unfeasible
  - Lasso

***
</details>

##  Regularization | `LASSO and Feature selection`

<details>
<summary> Lecture notes </summary>

LASSO  stands for Least absolute shrinkage and selection operator ("shrinkage" is another common term for regularization) and is a method for selecting variables to include in a multivariate model.

Classical LASSO builds on RSS of a linear regression model $Y \sim X{\beta}$ with regularization

<details>
<summary> Extra Reading </summary>

Extensions to glms exists, but then using a regularized likelihood expression
    
***
</details>

The regularization term $f(\beta) = \lambda\sum_{\beta_i\in\beta} |\beta_i-0|= \lambda\sum_{\beta_i\in\beta} |\beta_i|$

<details>
<summary> Extra Reading </summary>

Often the regularization term is expressed in terms of the $\ell_1-norm$, which can be viewed simply a short-hand notation, e.g., the $\ell_1-norm$ of $\beta$ is
$$ ||\beta||_1 = \sum_{\beta_i\in\beta} |\beta_i|$$

There is also a $\ell_2-norm$:
$$ ||\beta||_2 = \sqrt{\sum_{\beta_i\in{\beta}} \beta_i^2}$$
which is used, e.g., in ridge regression.

We note, BTW, that you already have been working with an $\ell_2-norm$: since $RSS = ||Y-X\beta||_2^2$ is simply the square of the $\ell_2$ norm of the residuals.

***
</details>  
    
The $\lambda$ parameter sets a limit on the estimation of $\beta$. 


Lasso is traditionally described as RSS with an auxiliary criterion/constraint: 

$$min_{{\beta}}\left\{RSS\right\} + \lambda\sum_{\beta_i\in\beta} |\beta_i|.$$
Lasso can also be viewed as a un-normalized Bayesian posterior probability, with a LaPlacean prior on $\beta$: $\beta_j ∼ LaPlace(0, 1/\lambda)$ 

<details>
<summary> Extra Reading </summary>

Other common notation for LASSO:

* You might often see the notation $$min_{{\beta}}\left\{RSS\right\} \textrm{ subject to } ||{\beta}||_1 <= t$$
  where $t$ is related to $\lambda$.
* Lasso can also be viewed as a Bayesian posterior probability, with a LaPlacean prior on $\beta$: $\beta_j ∼ LaPlace(0, 1/\lambda)$ 

***
</details>

The optimal values of $\beta$ are then estimated, using some algorithm (lars or coordinate descent).

<details>
<summary> Extra Reading </summary>

The *Coordinate descent* algorithm is used in the R package `glmnet`:

1. Over a grid of $\lambda\in [0, \infty]$, do
    1. Start with all $\beta=0$
    2. until convergence repeat for each $\beta_i$
        1. while keeping all other $\beta$ fixed and $\beta_i=0$, compute partial residuals
        2. estimate $\beta_i$ by RSS on the partial residuals
        3. update $\beta_i using the RSS estimate and $\lambda$.
    
***
</details>
            
<details>
<summary> Extra Reading </summary>

Alternatives to LASSO, differing mainly in the auxiliary criterion

  - *Ridge regression* which uses a $\ell_2$ norm
  - *Elastic-net*, which uses a mixed model combination of the  $\ell_1$ norm and the $\ell_2$ norm.

*** 
</details>

</details>

### Task | `Lasso using the glmnet R-package`

* Use function `glmnet` to perform LASSO analysis on our example data; relevant arguments of the function:
    + linear regression (`family='gaussian'` = default)
    + LASSO (`alpha=1` = default)
    + standardization
        + The variables Y and X must be centered and standardized to ensure that all variables are given equal weight in the model selection.
        + standardization of $X$ to unit variance in `glmnet` is obtained  by setting the argument `standardize=TRUE` which is the default
        + the values of $Y$ is always standardized (?) for `family=gaussian` (LASSO)
         + and the coefficients are back-standardized before reported

<details> 
<summary> Extra Reading </summary>

Standardization in `glmnet`:
$x' = \frac{x-\bar{x}}{1/\sqrt{N}||X-\bar{x}||_2}$

***
</details>

```{r, echo=T, eval=TRUE} 
library(glmnet)
# run lasso (alpha=1) for linear model (family=gaussian)
fit = glmnet(X,Y, family="gaussian", alpha=1, standardize=T)
```

* A graphical way to view the result is to `plot` the paths of $\beta$ for increasing vaules of $\lambda$.

```{r, echo=T, fig.height=5, eval=FALSE} 
par(mfrow=c(1,1))
plot(fit, xvar="lambda",label=T)
```

<details>
<summary> *Show result*</summary>
```{r, echo=FALSE, fig.height=5} 
par(mfrow=c(1,1))
plot(fit, xvar="lambda",label=T)
```

***
</details>

#### Think about
* In which order are variables included (i.e., their $\beta$ becomes non-zero? 
* In which direction is the effect
* Which lambda should we select?
  - Given our *oracle knowledge*, where would an appropriate $\lambda$ be?
  - Can we use that?

<details>
<summary> Some possible answers </summary>

<h4>Some possible answers</h4>
* The order appears to be $(1,2,3,7,6,5,10,9,4,8)$
* $\beta_i > 0, i\in \{1,2,3,4,7,9\}$, while $\beta_i<0, i\in \{5,6,8,10\}$
* Given *oracle knowledge*, the correct $\lambda$ appears lie somewhere in the interval $[\approx \exp(-2.1), \approx\exp(-2.5)]$
* In the normal case, we do not have *oracle knowledge*.

***
</details>

## Cross-validation

<details>
<summary> Lecture notes </summary>

The LASSO model will be different depending on how we set $\lambda$. A problem is to decide the optimal $\lambda$ to use. 

* $\lambda$ too *high*: risk of missing relevant variables
* $\lambda$ too *low*: risk of overfitting 

`glmnet` addresses this using *$k$-fold cross-validation* -- what is that?

</details>

### Cross-validation | `How to test for overfitting`

<details>
<summary> Lecture notes </summary>

The ultimate way of testing an estimated model (with parameters) is to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error*, $MSE$ ($=RSS/N$).
Naturally, we want to minimize $MSE$, i.e., the error of the model. In our LASSO application, this means that we want to select the $\lambda$ that minimizes the $MSE$

In cross validation, this approach is emaulated by partioning the data at hand into a *training* and  *test* (or *validation*) data set. The model parameters are estimated ('trained') on the the training data and the validated on the test data.

By chance, this may fail if the partitioning is 'non-representative'. A solution is to repeat the cross-validation procedure with another partioning.

In $k$-fold cross validation, the original data is split into $k$ sub-datasets $\{D_1,D_2,\ldots, D_k\}$.
For $i \in \{1,2,\ldots, k\}$, set $D_i$ as the test data set and the union of the other datasets be the training data. Perform cross validation as above.

This gives a distribution of $MSE$ from which we can estimate, e.g., mean and standard deviation.

<details>
<summary>Additional reading</summary>

This distribution allows us to use more elaborate means to select $\lambda$. One common suggestion is to use the largest $\lambda$ whose $MSE$ is within 1 standard error from the minimum value (called `lambda.1se` in `glmnet`). The motivation argued for this choice is *parsimony*, in the sense that larger $\lambda$ will include fewer variables (hence it is parsimonious in terms of number of included variables). 

Here we will limit ourselves to finding the minimum $\lambda$, called `lambda.min` in `glmnet`, but anyone is free to test if `lambda.1se` gives a different result.

***
</details>

</details>

### Task | `Determine optimal LASSO `$\lambda$` using cross-validation`
* Use the function `cv.glmnet` to perform cross validation (same options as for `glmnet`)
* `plot` the cross-validation results 
* Compare with the plot of estimated $\beta_i$ under different $\lambda$.
* Determine the optimal $\lambda$ (the one with minimal error)

```{r, echo=T,fig.height=5, eval=FALSE}
library(glmnet)
par(mfrow=c(1,1))
# run lasso (alpha=1) for linear model (family=gaussian)
cvglm=cv.glmnet(X,Y, family="gaussian", alpha=1, standardize=T, nfolds=100)

plot(cvglm)
plot(cvglm$glmnet.fit, xvar="lambda",label=T)
minlambda=cvglm$lambda.min
```

<details>
<summary> *Show result*</summary>
```{r, echo=FALSE,fig.height=5}
library(glmnet)
par(mfrow=c(1,1))
# run lasso (alpha=1) for linear model (family=gaussian)
cvglm=cv.glmnet(X,Y, family="gaussian", alpha=1, standardize=T, nfolds=100)

plot(cvglm)
plot(cvglm$glmnet.fit, xvar="lambda",label=T)
minlambda=cvglm$lambda.min
```

***
</details>

#### Think about
* Which is the $\lambda$ selected by `cv.glmnet`?
* Does this make sense given our *oracle knowledge*?

<details>
<summary> Some possible answers </summary>

<h4>Some possible answers</h4>
* Cross-validation-selected optimal lambda is `r minlambda`
* Yes, this includes only the *oracle knowledge* correct variables $X_1, X_2, X_3$

***
</details>

### Task| `Final LASSO effect sizes`
* Finally print a table with the $\beta$ coefficients (including the intercept, $\beta_0$) for the optimal model (i.e.,  at minimum $\lambda$). (Use function`coef`).

```{r, echo =T, eval=FALSE}
# Actually the following suffice for output on console
#coef(cvglm, s="lambda.min")

# But to get a nice table:
library(dplyr)      # for nice table
library(kableExtra) #for nice table

coefglm=as.data.frame(as.matrix(coef(cvglm, s="lambda.min")))
coefglm=cbind(seq(0,10),c(b0, b, rep(0, 7)),coefglm)
names(coefglm)=c("beta","value (oracle)", paste0("estimate(lambda=",signif(minlambda,3),")"))
kable(coefglm, row.names=F) %>%   kable_styling( font_size = 14)
```

<details>
<summary> *Show result*</summary>

```{r, echo =FALSE}
library(dplyr)      # for nice table
library(kableExtra) #for nice table

coefglm=as.data.frame(as.matrix(coef(cvglm, s="lambda.min")))
coefglm=cbind(seq(0,10),c(b0, b, rep(0, 7)),coefglm)
names(coefglm)=c("beta","value (oracle)", paste0("estimate(lambda=",signif(minlambda,3),")"))
kable(coefglm, row.names=F) %>%   kable_styling( font_size = 14)
```

***
</details>

#### Think about
* Does the effect sizes make sense -- if not can you think of why?

<details>
<summary> Some possible answers </summary>
<h4>Some possible answers</h4>

* Well...yes!
  + $\beta_i$ is non-zero only for _oracle_-known variables $X_1, X_2, X_3$
  + they don't exactly equate our *oracle knowledge* parameter values -- they appear to be scaled.
  + but their relative order of amplitude is right.
* Perhaps the normalization affected scaling.

***
</details>


# Exercises

We will here use biological experimental data, more specifically a skeletal muscle gene expression subset (randomly sampled 1000 genes) from GTEX Human Tissue Gene Expression Consortium ([Lonsdale et al. 2013](https://www.nature.com/articles/ng.2653)). We will use the approaches we learned above to perform feature selection on this data with respect to a logicit regression analysis.

### Task| `Load the GTEX muscle expression data`

* Load the data from the file `GTEX/GTEX_SkeletalMuscles_157Samples_1000Genes.txt` into a data.frame `X`. 
* Check the dimensions of `X` 
* Optionally, you can preview `X` using the function `datatable(X)`:

```{r, echo =T, eval=FALSE}
X<-read.table("GTEX/GTEX_SkeletalMuscles_157Samples_1000Genes.txt", header=TRUE, row.names=1, check.names=FALSE, sep="\t")
X<-X[,colMeans(X)>=1]

dim(X)
library(DT)
datatable(X)
```

<details>
<summary> *Show result*</summary>
```{r, fig.width=10, fig.height=8, echo=FALSE}
X<-read.table("GTEX/GTEX_SkeletalMuscles_157Samples_1000Genes.txt", header=TRUE, row.names=1, check.names=FALSE, sep="\t")
X<-X[,colMeans(X)>=1]

dim(X)
library(DT)
datatable(X)
```
</details>

#### Think about
* What are the dimensions of `X`? What does this mean for multivariate analysis?

<details>
<summary> Some possible answers </summary>
<h4>Some possible answers</h4>
* We can see that the gene expression data set includes p = `r ncol(X)` expressed genes (features) and n = `r nrow(X)` samples, i.e., there are more variables than samples (p >> n). 
* We do not have power to estimate a multivariate model including all variables; we need to do feature selection.

***
</details>

<br>
The phenotype of interest that we address address is *Gender*, i.e. we will figure out which of the `r ncol(X)` genes expressed in human skeletal muscles drive the phenotypic difference between Males and Females. 

### Task| `Load the GTEX Gender data`

* Load the Gender data for the GTEX samples from the file `GTEX/GTEX_SkeletalMuscles_157Samples_Gender.txt` in to a variale `Y` (Hint: keep only the variable `Gender`)
* Check that the the number of samples corresponds to `X` and how many women and men there are.

```{r, echo =T, eval=FALSE}
Y<-read.table("GTEX/GTEX_SkeletalMuscles_157Samples_Gender.txt", header=TRUE, sep="\t", stringsAsFactors=TRUE)$GENDER

summary(Y)
length(Y)
```

<details>
<summary> *Show result*</summary>
```{r, fig.width=10, fig.height=8}
Y<-read.table("GTEX/GTEX_SkeletalMuscles_157Samples_Gender.txt", header=TRUE, sep="\t", stringsAsFactors=TRUE)$GENDER

summary(Y)
length(Y)
```
The samples includes 99 Males and 58 Females, it is not perfectly balanced but still not too bad. 
</details>

### Task| `Visualize the data`
* Do a PCA analysis of `X` and color/group them by `Y`
* Do a barplot of explained variance per principal component

```{r, echo =T, eval=FALSE}
## TODO: Adjust this so that it fit what they learnt from Paya
library("mixOmics")
pca.gtex <- pca(X, ncomp=10)
plotIndiv(pca.gtex, group = Y, ind.names = FALSE, legend = TRUE, title = 'PCA on GTEX Skeletal Muscles')
plot(pca.gtex)
```

<details>
<summary> *Show result*</summary>
```{r, fig.width=10, fig.height=8}
## TODO: Adjust this so that it fit what they learnt from Payam
library("mixOmics")
pca.gtex <- pca(X, ncomp=10)
plotIndiv(pca.gtex, group = Y, ind.names = FALSE, legend = TRUE, title = 'PCA on GTEX Skeletal Muscles')
plot(pca.gtex)
```

The PCA plot demonstrates that there is a lot of variation between samples with respect to both PC1 and PC2, but there is no clear segregation of Males and Females based on their skeletal muscle gene expression data.

***
</details>

We want to investigate if there are a subset of genes that are associated to the phenotype *Gender*, that is, we want to do find multivariate association analyses. 
Since *Gender* is a binary variable, it is appropriate to use logistic *GLM* (`glm` with `family=binomial()`) for all analyses.
However, as suggested by the PCA, the majority of genes are probably not associated to *Gender*. Hence we want to perform *feature selection* to identify a, hopefully, optimal multivariate model to use. 


## AIC

We will first try to use AIC to determine the best multivariate model. A main decision is to choose the order in which to add variables to models. To help with that, we will first perform univariate regressions. 

### Task| `AIC`
* Perform separate univariate logistic regression ($logit(Y)\sim\beta X$) analyses for all genes of `X`
    - Collect the Gene name, P-values and Odds ratios (or alternatively the coeficients) in a data.frame *univariateResults* (one row per gene)
        - Hint: use `coef(summary(<your glm model>)` to extract P-values and coefficients
    - Also add a column with adjusted p-values to the data.frame
    - Optionally show the new data.frame in a `datatable`
* Write an R function `doAIC` that performs a AIC analysis on multivariate GLMs including sequentially more genes from a given matrix `myX` and a fixed outcome `mY` in the models.
    - Hints:
        1. Reuse code from the [AIC task above](.Task | `AIC analysis`). 
        2. Use `myX[,seq(1, i)]` to get the use only the `i` first columns of `myX`.
        3. Use `as.matrix` to convert the given subsetted `X` data.frame to a matrix before unsing it in the GLM.
* Try to run `doAIC` using different orderings and subsets of `X` as `myX`, e.g.:
    - ordered by size of odds ratio, by P-value
    - top 20, top 100 or only significant P-values (unadjusted or adjusted)
    

<details>
<summary> *Possible solutions*</summary>
#### Univariate analysis
```{r warning=FALSE, echo=TRUE}
coefs<-vector()
pvals<-vector()
a<-seq(from=0,to=dim(X)[2],by=100)
for(i in 1:dim(X)[2])
{
  model = glm(Y~X[,i], family=binomial)
  coefs=append(coefs, exp(coef(summary(model))[,1][2])) #
  pvals=append(pvals, coef(summary(model))[,4][2]) #
}
univariateResults<-data.frame(GENE=colnames(X), COEFFICIENTS=coefs, PVALUE=pvals)
univariateResults$FDR<-p.adjust(univariateResults$PVALUE,method="BH")
datatable(univariateResults[order(-abs((1-univariateResults$PVALUE))),]) 
```


#### doAIC function

```{r warning=FALSE, evho=TRUE}
library(stats)
coef<-vector()
pval<-vector()
a<-seq(from=0,to=dim(X)[2],by=100)

doAIC<-function(myX, myY){
  myX = as.matrix(myX)
  mprev <- glm(myY ~ myX[,1], family=binomial()) # current miminimum AIC model
  # dummyentry to be replaced
  aic=data.frame(models=0, aic=0, isAICmin="-") 
  for(i in seq(1,ncol(myX))){
    m <- glm(as.numeric(myY) ~ myX[,seq(1,i)]) #, family=binomial())
    fit = AIC(mprev,m)
    mprev = m
    if(i==2){ #include also the first model
      aic[i-1,] = list(paste0(i-1," variable"), signif(fit$AIC[1],5), "-") 
    }
    aic[i,] = list(paste0(i," variables"), signif(fit$AIC[2],5), "-") 
  }
  minaic=min(aic$aic)
  aic$rl=format(exp((minaic-aic$aic)/2), digits=4)
  aic$isAICmin = ifelse(aic$aic==minaic,"Yes","-")
  
  print("Best model is:")
  print(aic[aic$isAICmin == "Yes",])
  # Table of all models
  kable(aic, format='html', row.names=F, col.names=c("Compared models","AIC","Minimum AIC","rL"),digits=30,format.args=list(snsmall=0))  %>%  kable_styling( font_size = 14)
}
```

#### doAIC| `Sorted on *Odds ratio* -- top 20 genes`

```{r warning=FALSE, echo=TRUE}
# Add variable according to their odds ratio in univariate logistic regression
# Use top 20 variables 
univariateResults<-univariateResults[order(-abs((1-univariateResults$COEFFICIENTS))),]
doAIC(X[, head(univariateResults$GENE,20)], Y)
```

#### doAIC| `Sorted on *Odds ratio* -- top 100 genes`

```{r warning=FALSE, echo=TRUE, echo=TRUE}
# Add variable according to their odds ratio in univariate logistic regression
# Use top 100 variables 
univariateResults<-univariateResults[order(-abs((1-univariateResults$COEFFICIENTS))),]
doAIC(X[, head(univariateResults$GENE,100)], Y)
```

#### doAIC| `Sorted on *P-value* -- top 20 genes`

```{r warning=FALSE, echo=TRUE}
univariateResults<-univariateResults[order(univariateResults$PVALUE),]
doAIC(X[, head(univariateResults$GENE,20)], Y)
```

#### doAIC| `Sorted on *P-value* -- top 100 genes`

```{r warning=FALSE, echo=TRUE}
univariateResults<-univariateResults[order(univariateResults$PVALUE),]
doAIC(X[, head(univariateResults$GENE,100)], Y)
```

#### doAIC| `Only significant P-value -- Sorted on *P-value*`
```{r warning=FALSE, echo=TRUE}
univariateResults<-univariateResults[order(univariateResults$PVALUE),]
doAIC(X[,univariateResults[univariateResults$PVALUE <= 0.05, "GENE"]], Y)
```

#### doAIC| `Only significant P-value -- Sorted on *Odds ratio*`
```{r warning=FALSE, echo=TRUE}
univariateResults<-univariateResults[order(-abs((1-univariateResults$COEFFICIENTS))),]
doAIC(X[,univariateResults[univariateResults$PVALUE <= 0.05, "GENE"]], Y)
```

#### doAICX| `Only significant adjusted p-value (FDR) -- Sorted on *Odds ratio*`

```{r warning=FALSE, echo=TRUE}
univariateResults<-univariateResults[order(-abs((1-univariateResults$COEFFICIENTS))),]
doAIC(X[,univariateResults[univariateResults$FDR <= 0.05, "GENE"]], Y)
```

***

</details>

#### Think about:

* Could you find a good order to sequentially add variables to the tested models?
  - How can one solve this otherwise?
* What's the relation of the best models to the univariate analysis

<details>
<summary> Some possible answers </summary>
<h4>Some possible answers</h4>

* It seems very difficult to outline a general approach to sequentially design that works (i.e., finds a minimal model that optimizes AIC)  "out-of-the-box for" for all cases.
  - Possible other solutions:
      - Naive clever: run several variants as above and try to manually hand-pick the most likely  variables based on substantial AIC changes when added... difficult!
      - Clever step-wise addition: Select the most significant univariate model, take its residuals and use as a new phenotype in a second round of univariate analyses, compare AIC to the previous round, iterate!
      - Raw force: Create models for all possible combinations of variables in `X`, find the model among these that maximises AIC.
* The univariate analyses comprised only one model with a significant adjusted P-value, while our AIC analysis suggest that there may exist multivariate models that has lower AIC
  - The penalty of multiple testing.

***
</details>

## LASSO

We now turn to LASSO feature selection.  We will use `glmnet` to run LASSO on a logisitc GLM `Y` vs `X` and find an optimal value of $\lambda$ via 10-fold cross-validation. 

### Task| `LASSO`

* Run LASSO on `X` and `Y` with 100-fold cross-validation
  - Identify the optimal $\lambda$
      - Optionally, plot the result of the cross-validation
      - Optionally, plot the traces of¢beta$ for the inclusion of variables in the model.
* Create a table of the genes included in the optimal LASSO model and their $beta$, e.g., using `datatable`
* Run `doAIC` on the subset of `X` corresponding to genes in the optimal LASSO model.

<details>
<summary> *Possible solutions*</summary>

```{r LASSO,fig.width=10,fig.height=8}
library(glmnet)
par(mfrow=c(1,2))
# run lasso (alpha=1) for linear model (family=binomial)
cvglm=cv.glmnet(as.matrix(X),Y, family=binomial(), alpha=1, standardize=T, nfolds=10)

plot(cvglm)
plot(cvglm$glmnet.fit, xvar="lambda",label=T)
minlambda=cvglm$lambda.min

#lasso_fit <- cv.glmnet(as.matrix(X), Y, family="binomial", alpha=1)
#plot(lasso_fit)
#lasso_fit$lambda.min
#og(lasso_fit$lambda.min)
```

Once we know the optimal $\lambda$, we can display the names of the most informative features selected by LASSO for that optimal $\lambda$.

```{r bestmodel}
genes<-colnames(X)[unlist(predict(cvglm, s = "lambda.min", type = "nonzero"))]
betas= data.frame(betas=unlist(coef(cvglm, s="lambda.min")[genes,]))
betas<-betas[order(-abs(betas$betas)), , drop =FALSE]
datatable(betas)
```

```{r warning=FALSE, echo=TRUE}
doAIC(X[,genes], Y)
```
</details>

#### Think about:
* How does the LASSO result compare to the results form AIC and univariate analyses?
* Think about the reasons for similarity/dissimilarity

<details>
<summary> Some possible answers </summary>
<h4>Some possible answers</h4>

* The features selected by LASSO includes the gene from the univariate approach, but also several others
  - This is often the case in practice 
  - Multiple test correction reduces the power of the univariate approach
* The optimal LASSO model is different from those obtained from the various AIC approach (most likely:); moreover, the optimal LASSO model has a lower AIC than those investigated in the AIC approach (most likely)
  - In general, unless we test all models corresponding all possible combinatorial subsets of variables or apply some clever search algorithm (similar to the one LASSO uses), we are unlikely to find the optimal model.
      - Even then, we might get idifferent answers, because the regularization applied in AIC and LASSO are different.

</details>

<br><br><br>

## Thank you

## Session info

```{r, echo=FALSE}
library(pander)
pander(sessionInfo())
```
