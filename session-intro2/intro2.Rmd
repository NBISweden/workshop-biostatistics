---
title: "statsintro"
author: "Bengt Sennblad"
institute: "NBIS"
date: "11/16/2020"
output: 
  xaringan::moon_reader:
    encoding: 'UTF-8'
    self_contained: false
    css: [default, metropolis, metropolis-fonts]
    lib_dir: 'libs'
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
---
class: inverse, middle, center
<html><div style='float:left'></div><hr color='#EB811B' size=1px width=800px></html> 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)

library(reticulate)
library(knitr)
knit_engines$set(python = reticulate::eng_python)  

library(ggplot2)
library(qqman)
```


# Why Statistics?

---

# Content/aim



.pull-left[
## What do we want you to gain from this course?
- Learn a selection statistical methods


]
---

# Content/aim



.pull-left[
## What do we want you to gain from this course?
- Learn a selection statistical methods

### ...but, more importantly,
- be able to, by yourselves, explore additional methods
  - finding
  - get a intuitive understanding
  - evaluate assumptions
    - know when to apply
    - know when not to apply
]
--
.pull-right[
### How?
- Go from black-box to understanding
    -several levels
      - models = assumptions
      - tests -- test statistics
      - overparmeterizations
- Appreciate theory as a friend
    - needed for understanding assumptions
    - help when evaluating methods
- basis/framework for learning more
    - reading about statistical methods
    - Basic math language
      - not afraid of equations
      - know some basic terminology

]
---
class: inverse, middle, center
<html><div style='float:left'></div><hr color='#EB811B' size=1px width=800px></html> 

# Examples
## When is statistics needed?

---

# Example: Mouse knockout experiment

### Setup
Hypothesis: The low density lipoprotein (LDL) receptor gene *LDLR* affects the prevalence of hypercholsterolaemia.

- 10 wildtype (wt) mice
- 10 *LDLR* knockout (KO) mice

Measure plasma concentration of total cholesterol at one time point after feeeding on high fat diet.

???

low-density lipoprotein
---
layout: true
# Example: Mouse knockout experiment
.pull-left[
```{r mice1}
set.seed(85)

mean = c(0.55, 0.45)
stddev = c(0.025, 0.025)
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2], stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)
row.names(mice)=c(paste0("KO", 1:10), paste0("wt", 1:10))

ggplot(mice, aes(y=prevalence, x=sample(row.names(mice))))+
  xlab("mice") +
  geom_point() +
  theme_bw()

```
]
---
.pull-right[
### Visualize results in a plot

]
---
.pull-right[
### Visualize results in a plot
- Not so informative!
- Let's improve
]
---
layout: false
layout: true
# Example: Mouse knockout experiment

.pull-left[
```{r mice1a}
ggplot(mice, aes(y=prevalence, x=model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()

```
]

---
.pull-right[

### Improved visualization
  - Collect KO and wt separately as columns in a **boxplot**
]

---
.pull-right[

### Improved visualization
  - Collect KO and wt separately as columns in a boxplot
  ![black box warning](assets/blackbox.jpg)
]

---
.pull-right[

### Improved visualization
  - Collect KO and wt separately as columns in a boxplot
  - Visualize **distribution** of sample values using **descriptive statistics**
      - *mean* 
      - *quartiles*
]
---
.pull-right[

### Interpretation

Intuitively, there is a clear difference between KO and wt in this plot...
]
???
- 

-- 
.pull-right[
What if the result had looked different?

]
---
layout: false
# Example: Mouse knockout experiment

.pull-left[
```{r mice3}
set.seed(85)
mean = c(0.5, 0.5)
stddev = c(0.025, 0.025)
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2], stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)

ggplot(mice, aes(y=prevalence, x=model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()

```
]
.pull-right[

### Interpretation

...equally clearly, here is no difference between KO and wt...

]
---
layout: true
# Example: Mouse knockout experiment

.pull-left[
```{r mice4}
set.seed(85)
mean = c(0.525, 0.475)
stddev = c(0.05, 0.05)
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2], stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)

ggplot(mice, aes(y=prevalence, x=model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()

```
]

---
.pull-right[

### Interpretation

... Now, the difference is less clear

]
---
.pull-right[

### Interpretation

... Now, the difference is less clear

... we get uncertain

]
---
.pull-right[

### Interpretation

... Now, the difference is less clear

... we get uncertain

... Uncertainty best measured as probabilities!

... with probabilities we can perform a **statistical test**
]
---
.pull-right[

### Interpretation

... Now, the difference is less clear

... we get uncertain

... Uncertainty best measured as probabilities!

... with probabilities we can perform a **statistical test** of the difference

  ![black box warning](assets/blackbox.jpg)
]

---
.pull-right[
### Statistical tests

#### Model
- Abstractions of reality  probabilities
- Simplifying assumptions
  - Variation approximately $N(\mu,\sigma)$
    
#### Hypotheses
- H0: $\mu$ and $\sigma$ same for KO and wt 
- H1: $\mu$ might be different 

#### Test
- **P-value** $(p)$ =  probability of the observed or more extreme difference  under H0
- **significance level** $p\leq \alpha$
]

---
layout: false 
layout: true
# Example: Mouse knockout experiment

.pull-left[
```{r mice5}
set.seed(22)
mean = c(0.5, 0.5)
stddev = c(0.05, 0.5)
mult = 0.5
mice = data.frame(
  prevalence = c(rnorm(10, mean[1], stddev[1]), rnorm(10, mean[2],  stddev[2])),
  model = c(rep("KO", 10), rep("wt", 10))
)

ggplot(mice, aes(y=prevalence, x = model, fill=model))+
  geom_boxplot() +
  geom_point() +
  theme_bw()
```
]
---
.pull-right[

### Interpretation

... what about here?

]


---
.pull-right[

### Interpretation

... what about here?

#### Tentative answer
This might require testing hypotheses that also look at the *variance*

]


---
layout: false
layout: true
# Example: regression
```{r, regression1}
alpha = 1.5
beta = 1.5
beta2 = 0.5
stddev = 1
rlin<-function(x, n=1, a=alpha, b=beta, s=stddev){
  m = a+b*x
  return(rnorm(n, m, s))
}

rlog<-function(x, n=1, a=alpha, b=beta, p){
  p = exp(-(a+b*x))
  return(rbinom(n, 1, 1/(1+p)))
}

log<-function(x, n=1, a=alpha, b=beta, p){
  p = exp(-(a+b*x))
  return(1/(1+p))
}

x = rnorm(10000, 0, 2)
ylin1 = unlist(lapply(x, rlin))
ylin2 = unlist(lapply(x, function(x) rlin(x, s=10)))
ylog = unlist(lapply(x, rlog))
ylin3 = unlist(lapply(x, function(x) rlin(x, b=beta2)))

df = data.frame(x=x, ylin1  = ylin1, ylin2=ylin2,  ylog=ylog, ylin3=ylin3)
p1=ggplot(df, aes(x=x, y=ylin1)) +
  xlab("x") +
  ylab("y") + 
  geom_point()

p2 = p1 + geom_abline(intercept=alpha, slope=beta, col="red")

p3 = ggplot(df, aes(x=x, y=ylin2)) +
  xlab("x") +
  ylab("y") + 
  geom_point() + 
  geom_abline(intercept=alpha, slope=beta, col="red")

p4=ggplot(df, aes(x=x, y=ylog)) +
  xlab("x") +
  ylab("y") + 
  geom_point() + 
  stat_function(fun=log, col="red")

p5 = ggplot(df) +
  xlab("x") +
  ylab("y") + 
  geom_point(aes(x=x, y=ylin1)) + 
  geom_point(aes(x=x, y=ylin3)) 


p6 = g= ggplot(df) +
  xlab("x") +
  ylab("y") + 
  geom_point(aes(x=x, y=ylin1), col="red") + 
  geom_point(aes(x=x, y=ylin3), col="blue")

p7 = p6 +
  geom_abline(intercept=alpha, slope=beta,  col="red") +
  geom_abline(intercept=alpha, slope=beta2, col="blue") 

p8 = p6 
for(b in seq(-0.75, 0.75, 0.25)){
  p8 = p8 +
    geom_abline(intercept=alpha, slope=beta+b,  col="red")+
    geom_abline(intercept=alpha, slope=beta2+b, col="blue") 
}

p9 = p6 
for(b in seq(-0.05, 0.05, 0.05)){
  p9 = p9 +
    geom_abline(intercept=alpha, slope=beta+b,  col="red")+
    geom_abline(intercept=alpha, slope=beta2+b, col="blue") 
}
```

.pull-left[
```{r, regression2}

p1
```
]
---
.pull-right[
Dependencies between variables

- Relation between two variables -- **correlation**
]

???

Hmnm try to find some biological example for this
---
.pull-right[
Dependencies between variables

- Relation between two variables -- **correlation**
  ![black box warning](assets/blackbox.jpg)

]
 
???

Actually, not going to throgh this here .. have to wait until the session on this
---
.pull-right[
Dependencies between variables

- Relation between two variables -- **correlation**

- One variable depends on the other -- **regression**
  - **fit** a **linear model** 
]

???

Need some good biological example

---
.pull-right[
Dependencies between variables

- Relation between two variables -- **correlation**

- One variable depends on the other -- **regression**
  - **fit** a **linear model** 
  ![black box warning](assets/blackbox.jpg)
]
---
layout: false
layout: true
# Example: regression

.pull-left[
```{r, regression3}
p2
```
]
---
.pull-right[
## Linear model

- Extend our previous model

    + Variation of $y$ approximately $N(\mu,\sigma)$  
    *and*
    + $\mu = \alpha +\beta x$ 
       
<br>
<br>
<br>
<br>
<br>
<br>
]

???
Maybe this can suffice as hierarchical model example instead...?
---
.pull-right[
## Linear model

- Extend our previous model
    + Variation of $y$ approximately $N(\mu,\sigma)$  
    *and*
    + $\mu = \alpha +\beta x$ 

- Uses
  1. Simulation
      - validating methods
<br>
<br>
<br>

]
---
.pull-right[
## Linear model

- Extend our previous model
    + Variation of $y$ approximately $N(\mu,\sigma)$  
    *and*
    + $\mu = \alpha +\beta x$ 

- Uses
  1. Simulation
      - validating methods
  2. Inference 
      - **regression**
          - optimal $\alpha$ and $\beta$ values


]
???
- $\beta$ tells how strongly y depend on x
- not always this simple
---
layout: false
layout: true
# Example: regression

.pull-left[
```{r, regression4}
p3
```
]

---
.pull-right[
## Linear model

- Extend our previous model
    + Variation of $y$ approximately $N(\mu,\sigma)$  
    *and*
    + $\mu = \alpha +\beta x$ 

- Uses
  1. Simulation
      - validating methods
  2. Inference 
      - **regression**
          - optimal $\alpha$ and $\beta$ values


]

???

- much higher $\sigma=10$ (before $\sigma=1$)
---
layout: false
layout: true
# Example: regression

.pull-left[
```{r, regression5}
p4
```
]
---
.pull-right[
... What about here


]
---
.pull-right[
... What about here

## Non-linear dependencies

]
---
.pull-right[
... What about here

## Non-linear dependencies

- **Generalized linear models (GLMs)**
  ![black box warning](assets/blackbox.jpg)

]

---
layout: false
layout: true
# Example: GWAS
.pull-left[
```{r, gwas, cache=TRUE, warning=FALSE}
chromlengths=c(
  247249719,
  242951149,
  199501827,
  191273063,
  180857866,
  170899992,
  158821424,
  146274826,
  140273252,
  135374737,
  134452384,
  132349534,
  114142980,
  106368585,
  100338915,
  88827254,
  78774742,
  76117153,
  63811651,
  62435964,
  46944323,
  49691432,
  154913754,
  57772954
)

m = length(chromlengths)
n=5000
gwas = data.frame(
  P=unlist(lapply(1:(n*m), function(x) runif(1,0,1))),
  BP=unlist(lapply(chromlengths, function(x) sample(1:x, n, replace =FALSE))),
  CHR = unlist(lapply(seq(1,m), function(x) rep(x, 5000)))
  )

manhattan(gwas, suggestiveline=FALSE, genomewideline=FALSE)

```

]
---
.pull-right[

### Manhattan plot

- Individual association test for each SNP and disease
- $-log_{10}$ P-value against genomic position

]

???
-log(0.05) = 1.30103
- thousands of associations
---

.pull-right[

### Manhattan plot

- Individual association test for each SNP and disease
- $-log_{10}$ P-value against genomic position

#### *Multiple tests*
- Repeated experiments are more likely to succeed under H0


]

???

- Discuss repeated dice throws
- This manhattan plot is actually generated under the null model
---
.pull-right[

### Manhattan plot

- Individual association test for each SNP and disease
- $-log_{10}$ P-value against genomic position

#### *Multiple tests*
- Repeated experiments are more likely to succeed under H0

#### Multiple test correction
- **Bonferroni, False Discovery Rate (FDR)**

]

---
.pull-right[

### Manhattan plot

- Individual association test for each SNP and disease
- $-log_{10}$ P-value against genomic position

#### *Multiple tests*
- Repeated experiments are more likely to succeed under H0

#### Multiple test correction
- **Bonferroni, False Discovery Rate (FDR)**
  ![black box warning](assets/blackbox.jpg)

]


---
layout: false
layout: true
# Multivariate analysis

.pull-left[
```{r, multivariate}
library(plot3D)

nrow = 10000
alpha = 0.0
beta=1.2
gamma = -1.5
stddev = 1
rmul<-function(x, y, n=1, a=alpha, b=beta, c = gamma, s=stddev){
  m = a + b*x + c*y
  return(rnorm(n, m, s))
}

x = runif(nrow, 0, 10)
y = runif(nrow, 0, 10)
z = unlist(lapply(1:nrow, function(i) rmul(x[i], y[i])))
df = data.frame(x=x,y=y, z=z)
#df = data.frame(x=c(x,x),y=c(rep(1, length(x)), rep(2, length(x))), z=c(ylin1, ylin3))
#df = data.frame(x=c(x,x),y=c(rnorm(length(x), mean=1, sd=0.00001), rnorm(length(x), 2, 0.00001)), z=c(ylin1, ylin3))

scatter3D(x=df$x, z=df$z, y=df$y, xlab="x", ylab="y", zlab="z", phi=20, theta=-50, zlim=c(-20,20))
#scatter3D(x=df$x, z=df$z, y=df$y)



# gamma=0.5
# rmlin<-function(x, z, n=1, a=alpha, b=beta, c=gamma, s=2){
#   m = a+b*x+c*x
#   return(rnorm(n, m, s))
# }
# 
# z= rnorm(10000, 1,1)
# ylin4 = unlist(lapply(1:length(z), function(n) rmlin(x[n], z[n])))
# df = data.frame(x=x,z=z, y=ylin4)
# 
# scatter3D(x=x,y=z, z=ylin4,xlab="x", ylab="z", zlab="y", phi=10, theta=40)
```
]

---
.pull-right[
## Example: Multivariate regression

{{content}} 
]

--

 ![black box warning](assets/blackbox.jpg) 
]

---
.pull-right[
## Example: Multivariate regression
- We can visualize models with 3 variables
- and write an equation for this:  
$\begin{cases}y \sim N(\mu, \sigma) \\\mu = \alpha+\beta x + \gamma age\end{cases}$
{{content}}
 
]

--
- For >3 variables, visualization breaks...
- but we can still write it as an equation:  
$\begin{cases}y \sim N(\mu, \sigma) \\\mu = \alpha+\beta x + \gamma age + \delta BMI\end{cases}$

---
layout: false
# Multivariate analysis
.pull-left[
```{r, multivariate2}
library(plot3D)

z = unlist(lapply(1:nrow, function(i) rmul(x[i], y[i], c=0)))
df = data.frame(x=x,y=y, z=z)
scatter3D(x=df$x, z=df$z, y=df$y, xlab="x", ylab="y", zlab="z", phi=20, theta=-50, zlim=c(-20,20))

```
]
.pull-right[
#### Issues with multivariate analysis

1. **Feature selection**
    - Include only the most relevant variables in out model
{{content}}
]
--
2. **Overparameterization**
    - Risk of modeling random noise in data
{{content}}
--
![black box warning](assets/blackbox.jpg) 

{{content}}
--

#### Really Big Data 
- **Machine Learning** on all data
{{content}}


---
layout: false
layout: true
# Big data - Machine learning

## PCA and clustering

.pull-left[
```{r, pca}

pca = prcomp(iris[,1:4], scale=T)
pca_plot <- data.frame(x = pca$x[,"PC1"], y = pca$x[,"PC2"], Groups = factor(x=(iris$Species=="setosa"), labels=c("cluster 1","cluster 2")))
p=ggplot(pca_plot) +
  geom_point(aes(x=x, y=y, color=Groups)) +
  #scale_color_manual(values=mycolors) +
  xlab("PC1") +
  ylab("PC2") +
  theme_bw() +
  theme(title=element_text(face="bold")) 
print(p)


```
]
---
.pull-right[

### Feature selection with PCA

- **linear transformation** of original variables into new *hidden variables*, components/factors

### Clustering
- identify groups based on hidden variables
]
---
.pull-right[

### Feature selection with PCA

- **linear transformation** of original variables into new *hidden variables*, components/factors

### Clustering
- identify groups based on hidden variables
  ![black box warning](assets/blackbox.jpg)

]
---
layout: false
layout: true

# Big data - Machine learning



## Artificial neural network (ANN)

- **Train** ANN to learn feature selection and model selection to predict the right answer.

.pull-left[

{{content}}

]
.pull-right[

```{python pyann1, echo=F, out.height="100%", fig.align='center'}
import numpy as np
import matplotlib.pyplot as plt
from draw_neural_net import draw_neural_net

#--------[1] Input data
#dataset = np.mat('-1 -1 -1; -1 1 1; 1 -1 1; 1 1 -1')
#X_train = dataset
#y_train = np.mat('0; 1; 1; 0')
#-----2-2-1
layer_sizes= (4,5,5,5,3)

output = [ "COW=0", "DOG=1", "CAT=0" ]

fig = plt.figure(figsize=(12, 12))

ax = fig.gca()
ignore=ax.axis('off')
nodeFontSize = 35

draw_neural_net(ax,
                layerSizes=layer_sizes, 
                outPrefix = output,
                nodePrefix = "",
                inNodePrefix = "",
                nodeFontSize=nodeFontSize,
                hideBias = True)
plt.show()
```


```{r, ann2, fig.height=4,eval=FALSE}
layers =list("I"=4, "H1"=5, "H2"=4,"O"=3)
ylab=c("cat=0", "dog=1", "cow=0")
plotAnn(layers, cex=1, withY=T, ylab=ylab)
```
]

---

```{r, ann1, results='asis'} 
for(row in 1:2){
  cat("<br>")
}
cat("\n")
cat('<p style="text-align: center; font-size:2em;">')
  cat("Some Big Data \nmultivariate input X!\n")
cat('</p>')
cat("\n")
```

---
##### Training input  could be, e.g., images (pixels)
```{r, ann3, out.width=200, fig.align='right'} 
#, out.width="50%", fig.align='center', fig.asp=0.5, dpi=600}
knitr::include_graphics('assets/Nora.jpeg')

```

---

#####  ... or training input could be a genome sequence...
```{r, ann5, results='asis'} 
cat(paste0("...", paste0(sample(c("A", "C","G","T"), 27, replace=TRUE), collapse="")))
cat("\n")
for(row in 1:7){
  cat(paste0(sample(c("A", "C","G","T"), 30, replace=TRUE), collapse=""))
  cat("\n")
}
cat(paste0(paste0(sample(c("A", "C","G","T"), 27, replace=TRUE), collapse=""), "..."))
```
---

#####  This is of course a major...
![black box warning](assets/blackbox.jpg)

???
This is of course a major black box


---
layout: false
# Example: phylogeny


```{r, phylogeny1, include = FALSE}
library(ape)
library(ggtree)
library(phangorn)
library(gridExtra)
library(dplyr)
library(tidytree)
library(phytools)

pl=list()
par(mfrow=c(1,2))
data(Laurasiatherian)
subset = c( 39, 28, 47, 20, 38, 21, 34,  8, 23) #sample(5:length(Laurasiatherian), 9)
l= Laurasiatherian[c(1, subset),]

lcol = rep("0", length(l))
names(lcol)=names(l)
lcol["Baboon"]="2"
lcol[names(lcol) %in% c("CaneRat", "Dormouse") ] = "1"


d = dist.ml(l)

doGgtree<-function(mytree, branchlength="branch.length", xlimmax=0.25, leafcol=lcol, title=""){
  fit <- MPR(leafcol, mytree, "Platypus") #
  #fit <- fastAnc(mytree, leafcol, vars=TRUE, CI=TRUE)
  leafcol <- data.frame(node = nodeid(mytree, names(lcol)),
                        trait = leafcol)
  vertexcol <- data.frame(node = row.names(fit), trait = fit[,"lower"])
  vertexcol["19",] = c("19","0")
  col <- rbind(leafcol, vertexcol)
  col$node <- as.numeric(col$node)
  mytree=ladderize(root(mytree, 1, resolve.root=TRUE))
  mytree <- full_join(mytree, col, by = 'node')

  t=  ggtree(mytree, aes(x,y,color=trait), branch.length=branchlength) + 
      geom_tree() + 
      theme_tree() + 
      geom_tiplab(size=3)+ 
      geom_nodelab(size=3)+ 
      scale_color_manual(values=c("0"="black", "2"='red', "1"='green')) +
      ggtitle(title) +
      ggplot2::xlim(0, xlimmax)
  return(t)
}

nj = NJ(d)
PP = c("","","","","","","","","","","",0.60,0.89,0.92,0.97,0.95,0.94, 0.96, 0.54)
p=doGgtree(nj, branchlength="none",xlimmax=10)
p1=doGgtree(nj, branchlength="none", xlimmax=10, title="NJ")
q1=doGgtree(nj, title="NJ")

bnj = bionj(d)
p2=doGgtree(bnj, branchlength="none", xlimmax=10, title="BioNJ")
q2=doGgtree(bnj, title="BioNJ")

p3 = p +  geom_text(aes(label=PP), hjust=-.5)

t = ggtree(nj, aes(x,y)) + 
  geom_tree() +      
  theme_tree()
dummy = ggplot() +
  annotate("text", x = 0, y = 0, size=8, label = "...") + 
  theme_void()

```
.pull-left[
```{r, phylogeny2}
multiplot(p1, p2, ncol=2)
```

]

.pull-right[
## Clustering methods
- works with pairwise distances
- fast
- single solution (point estimate)

### Problem 
- different methods yield different solutions
]

---
# Example: phylogeny

.pull-left[
```{r, phylogeny3}
multiplot(q1, q2, ncol=2)
```
]
.pull-right[
## Robust?
- Using branch lengths as a measure, there is little support for either placement
]
---
# Example: phylogeny

.pull-left[
```{r, phylogeny4}


r = 10
c = 10

pl=list()
m=0
for(i in 1:r){
  for(j in 1:(c-1)){
    m=m+1
    pl[[m]] = t
  }
  m=m+1
  pl[[m]] = dummy
}
grid.arrange(grobs=pl, ncol=c)
#multiplot(grobs=pl, ncol=2)
```
]
.pull-right[
### In fact there are multiple possible trees

- Is one better than the other?
{{content}}
]


--

#### *Frequentist approach*
  - hypothesis test
{{content}}


--

#### *Bayesian approach*
  - assign *posterior probabilities* of each tree given data
  - Look at the whole distribution
  - evaluate statistic of interest
{{content}}
--
![black box warning](assets/blackbox.jpg)

---
# Example: phylogeny
.pull-left[
```{r, phylogeny4a}
# 
# 
# r = 10
# c = 10
# 
# pl=list()
# m=0
# for(i in 1:r){
#   for(j in 1:(c-1)){
#     m=m+1
#     pl[[m]] = t
#   }
#   m=m+1
#   pl[[m]] = dummy
# }
grid.arrange(grobs=pl, ncol=c)
#multiplot(grobs=pl, ncol=2)
```
]

.pull-right[
### *Bayesian approach*
- Here, statistic of interest = Support for individual clades (*clade credibility*, CC)
  - CC of clade a = sum of trees with clade A weighted by their probabilities
  
```{r, phylo5, fig.height=4}
  
plot(p3)
```

  ]
--

- Note. Frequentist approach to clade support: use data resampling (bootstrap)








































