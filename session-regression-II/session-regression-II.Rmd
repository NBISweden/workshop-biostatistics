---
title: 'Session regression II: multiple linear regression'
output:
  html_document:
    df_print: paged
  md_document:
    variant: markdown_github
  pdf_document: default
header-includes: \usepackage{float}
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path="session-regression-II-files/figures/")
knitr::opts_chunk$set(fig.pos = 'H')
knitr::opts_chunk$set(cache.path = "tmp")
knitr::opts_chunk$set(cache = TRUE)
```

# Learning outcomes

After this session, a student should be able to:

- visualize bivariate relationships in small datasets
- fit a linear regression model containing main and interaction effects
- assess the quality of the model fit
- determine if at least one predictor can explain the response
- determine which predictors explain the response
- asses the accuracy of predictions from the model

-------

# Warmup

## [Quiz: revisiting linear model specifications](https://forms.gle/z4qKdrcQj1wqUxBV7)

## Visualizing the Advertising dataset

In this session we will use the [Advertising dataset](http://www-bcf.usc.edu/~gareth/ISL/data.html).
This simple dataset consists of sales data for 200 products along with the amount of money spent 
on TV, radio, and newspaper ads. We would like to know how best to spend advertizing money to
maximize sales.

To begin with, let us familiarize ourselves with the dataset. 
The data are stored in the `data` subdirectory of this session directory.

```{r}
# load the data
ads = read.csv('./data/Advertising.csv')
```

First, we check to see what columns were imported

```{r}
head(ads)
```

It looks like a redundant column of row numbers, `X`, has made it into the table.

The other columns look like numbers. That's good.

Now, let us use the `pairs` function to get a quick overview of linear relationships within the dataset. 

```{r}
# visualize all pairwise relationships
pairs(ads)
```

The pairs plot creates a scatter plot of every pair of variables in a data frame.
First, to clear things up, the variable `X` is uncorrelated with the other columns and does not 
add anything to the dataset. We should probably remove it and replot:

```{r}
ads = ads[-1]
pairs(ads)
```

Ah, that's better.

From the pairs plot we can see that:

1. TV expenditure appears to be correlated with sales
2. As TV expenditure goes up, the variance associated with sales increases as well
3. radio expenditure appears to be correlated with sales
4. newspaper sales do not look very correlated to sales

It looks like more than one variable could be used to explain sales. 
How would we handle this in the simple regression? 

## Excercise: fitting linear regressions on multivariate data

We can fit a linear regression for TV vs Sales this way:
```{r error=TRUE}
lm(Sales ~ TV, data=ads)
```

Oops that did not work! Let's see what's up:

```{r}
names(ads)
```
Ah! sales is lower case:
```{r}

lm(sales ~ TV, data=ads)

```
For every five cents spent on advertising our average sales went up by a dollar.
Pretty sweet! What about radio?

```{r}
lm(sales ~ radio, data=ads)
```
radio appears to help as well, although not as much.
newspaper?

```{r}
lm(sales ~ newspaper, data=ads)
```
newspaper appears to have a similar effect to TV. 
But there seemed to be less correlation in the previous pairs plot.  What's going on?

```{r}
summary(lm(sales ~ TV, data=ads))
summary(lm(sales ~ newspaper, data=ads))
```
The answer is in the $R^2$ values: TV has a much higher $R^2$ than newspaper.

We can fit a linear model for each explanatory variable separately, but we are left
with two problems:

1. How would be combine the three models into a single model to create a single prediction for Sales?
2. The pairs plot shows us that the explanatory variables are correlated. The linear regression
   fits ignore all other explanatory variables, and this can lead to incorrect predictions.

This is where multiple linear regression comes in.
It allows us to create a single model for predicting sales from multiple explanatory variables, and it allows us to
create a model that takes correlation between explanatory variables into account.

# Multiple linear regression model specifications

A multiple linear regression model that incorporates $p$ explanatory variables can be expressed as:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_p + \epsilon$$

where $Y$ is the response variable, $X_j$ is the $j^{th}$ explanatory variable, and 
$\beta_j$ is the $j^{th}$ model coefficient. $\beta_j$ can be interpreted as the average
increase in $Y$ for one unit increase in $X_j$ while holding all other explanatory 
variables fixed.

For the `Advertising` dataset we can express a regression model as:

$$Sales = \beta_0 + \beta_1 newspaper + \beta_2 radio + \beta_3 TV + \epsilon$$

# Estimating regression coefficients

We can estimate the regression coefficients $\hat\beta_j$ from the data in the same
manner as for linear regression. We can then use $\hat\beta_j$ to make predictions $\hat{y}$ 
using the formula:

$$\hat{y} = \hat\beta_0 + \hat\beta_1x_1 + ... + \hat\beta_px_p$$

As in linear regression, we choose $\beta_j$ such that we minimize the residual sum of squares:

$$\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$
, which is equivalent to
$$\text{RSS} = \sum_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1x_{i1} - ... - \hat\beta_px_{ip})^2$$

## Quiz: [What was $y_i$ again?](https://forms.gle/XUxpgxJbkTp1QfDG8)


# Relationship between the response and predictors 
# Model fit 
# Predictions 
# Qualitative predictors 
# Interaction terms
# Non-linear transformation of the predictors 
# Potential problems: non-linearity, collinearity 
# Logistic regression
