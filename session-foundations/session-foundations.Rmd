---
title: "Introduction to statistical theory and concepts"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
require(kableExtra)
library(reshape2)
library(ggplot2)
require(knitr)
require(UsingR)
knitr::opts_chunk$set(fig.width=3.5, fig.height=3.5, echo = FALSE, cache=TRUE, error=FALSE, warnings=FALSE, dpi=600)
options(digits=2)
```

### Learning outcomes

- understand the concept of random variables and probability
- learn about probability density functions and cumulative distribution functions
- compute population mean and variance
- compute sample mean and variance
- use normal distribution
- understand the central limit theorem


# What is statistics?

  Data analysis taking uncertainty into account

- Descriptive statistics: Summary statistics that describe a data set (a study population), e.g. mean, median, mode, variance, standard deviation.}
- Statistical inference: Draw conclusions regarding properties of a population based on observations of a random sample from the population.


# Probability theory
## Random variables

The outcome of a random experiment can be described by a random variable.

Example random variables:

- The weight of a random newborn baby
- The smoking status of a random mother
- The hemoglobin concentration in blood
- The number of mutations in a gene
- BMI of a random man
- Weight status of a random man (underweight, normal weight, overweight, obese)
- The result of throwing a die

Whenever chance is involved in the outcome of an experiment the outcome is a random variable.

A random variable is usually denoted by a capital letter, $X, Y, Z, \dots$. Values collected in an experiment are observations of the random variable, usually denoted by lowercase letters $x, y, z, \dots$.

A random variable can not be predicted exactly, but the probability of all possible outcomes can be described.

The population is the collection of all possible observations of the random variable. Note, the population is not always countable.

A sample is a subset of the population.

## Discrete random variables

A discrete random variable can be described by its *probability mass function*.

- The number of dots on a die

```{r}
kable(matrix(c(1:6,rep(1/6,6)),ncol=6, byrow=TRUE, dimnames=list(c('x','p(x)'), c()))) %>% kable_styling(full_width = FALSE)
```

```{r die, fig.height=3, fig.width=7, fig.cap="Probability mass function of a die.", out.width="45%"}
plot(data.frame(x=1:6, p=1/6) %>% ggplot(aes(x=x, y=p)) + geom_bar(stat="identity") + theme_bw() + ylim(c(0,.25)))
``` 

- The smoking status of a random mother

The random variable has two possible outcomes; non-smoker (0) and smoker (1). The probability of a random mother being a smoker is 0.44.

```{r}
kable(matrix(c("0","0.61","1","0.39"),ncol=2, dimnames=list(c('x','p(x)'), c('non-smoker','smoker')))) %>% kable_styling(full_width = FALSE)
```

- The number of bacterial colonies on a plate

The probability that the random variable, $X$, takes the value $x$ is denoted $P(X=x) = p(x)$. Note that:

1. $0 \leq p(x) \leq 1$, a probability is always between 0 and 1.
2. \sum p(x) = 1, the sum over all possible outcomes is 1.

# Exercise: Dice experiment
   
When throwing 10 dice, how many dice show 6 dots?
  
- Define the random variable of interest
- What are the possible outcomes?

- Which is the most likely number of sixes?
- What is the probability to get exactly 2 sixes when throwing ten dice?
- On average how many sixes do you get when throwing ten dice?
<!-- The law of large numbers states that if the same experiment is performed many times the average of the result will be close to the expected value. -->
- What is the probability to get 4 or more sixes when throwing ten dice?
- Estimate the probability mass function
  
```{r dice, message=FALSE, warning=FALSE, eval=FALSE}
library(googledrive)
library(googlesheets4)
library(ggplot2)
library(reshape2)
require(tidyverse)
httr::set_config(httr::config(http_version = 0))

readplotdice <- function() {
    drive_deauth()
    sheets_deauth()

    X <- read_sheet(as_id("https://docs.google.com/spreadsheets/d/1t3jOucU48MVoCQGTbGyWe9SntFbHlHqGSCiqMptHhhs/edit?usp=sharing"), col_types="i")
    data <- melt(X) %>% filter(value %in% 0:10)
    plot(ggplot(data, aes(x=value, fill=variable)) + geom_histogram(binwidth=1, center=0, color="white") + theme_bw() + theme(legend.position=c(1,1), legend.justification=c(1,1)))
    return(data)
}
dice <- readplotdice()
```  

```{r dice2, out.width="49%", fig.show="hold", eval=FALSE}
y <- replicate(10000, sum(sample(0:1, 10, p=c(5/6,1/6), replace=TRUE)))
ggplot(data.frame(x=y), aes(x=y)) + geom_histogram(color="white", binwidth=1) + theme_bw()
ggplot(data.frame(x=y), aes(x=y)) + geom_histogram(aes(y=stat(density)), color="white", binwidth=1) + theme_bw()
```
### Bernoulli trial
  
A Bernoulli trial is a random experiment with two outcomes; success and failure. The probability of success, $P(success) = p$, is constant. The probability of failure is $P(failure) = 1-p$.
  
  When coding it is convenient to code success as 1 and failure as 0.
  
  The outcome of a Bernoulli trial is a discrete random variable, $X$.
  
```{r}
kable(matrix(c('x','0','1','p(x)','p','1-p'), byrow=TRUE, ncol=3)) %>% kable_styling("striped", full_width = FALSE)
```

### Binomial distribution

Also the number of successes in a series of independent and identical Bernoulli trials is a discrete random variable.
  
  $Y = \sum_{i=0}^n X_i$
  
The probability mass function of $Y$ is called the binomial distribution.

### Continuous random variable

A continuous random variable can be described by its *probability density function*.

```{r normpdf2, out.width="70%", fig.align="center"}
x <- seq(-4,4,.01)
ggplot(data.frame(x=x, fx=dnorm(x)), aes(x,fx)) + geom_line() + theme_bw() + ylab("f(x)")
```

### Example: baby weight

The data set `babies` consists of data for 1236 male babies and their mothers. All babies are born in Oakland in the 1960s.

The weight of random newborn baby is a continuous random variable, lets call it $W$. In this example the entire population is known and can be summarized in a histogram.

```{r wtbabies, out.width="50%", warning=FALSE, message=FALSE}
library(UsingR)
##The weights are originally in ounces, transform to kg
ounce <- 0.0283495231
wt <- babies$wt*ounce
## Plot the distribution in a histogram
#plot(ggplot(data.frame(w=wt), aes(x=w)) + geom_histogram(color="white", binwidth=0.2) + theme_bw() + xlab("weight (kg)"))
```



```{r wtbabieshist, out.width="33%", warning=FALSE, message=FALSE, fig.show="hold", fig.keep="all"}
plot(ggplot(data.frame(w=wt), aes(x=w)) + geom_histogram(color="white", binwidth=0.2) + theme_bw() + xlab("weight (kg)"))

plot(ggplot(data.frame(w=wt), aes(x=w)) + geom_histogram(aes(y=stat(density)), binwidth=0.2, color="white") + theme_bw() + xlab("weight (kg)"))

plot(ggplot(data.frame(w=wt), aes(x=w)) + geom_histogram(aes(y=stat(density)), binwidth=0.2, color="white") + theme_bw() + xlab("weight (kg)") + geom_density())
```

### Probability density function, pdf

  ```{r wtbabiesdens3, out.width="49%", warning=FALSE, message=FALSE, fig.show="hold", fig.keep="all"}
plot(ggplot(data.frame(w=wt), aes(x=w)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density())

df.wt <- density(wt)
df.wt <- data.frame(x=df.wt$x, y=df.wt$y)
plot(ggplot(data.frame(w=wt), aes(x=w)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() + geom_area(data=df.wt %>% filter(x<3.75, x>2.33), aes(x=x, y=y)) + scale_x_continuous(breaks=c(2,2.33,3,3.75,4,5), labels=c('2','a','3','b','4','5')) + geom_hline(yintercept=0) + theme(panel.grid=element_blank()))
```

```{r pdfwtnorm, out.width="100%", eval=TRUE}
w<-seq(1.5,5.5,.01)
df.nwt <- data.frame(w=w, f=dnorm(w, 3.5, 0.5))
#ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("Weight (kg)") + ylab("f(x)")
```

```{r pdfab, eval=FALSE}
ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("Weight (kg)") + ylab("f(x)") + geom_area(data=df.nwt %>% filter(w<3.75, w>2.33)) + scale_x_continuous(breaks=c(2,2.33,3,3.75,4,5), labels=c('2','a','3','b','4','5')) + geom_hline(yintercept=0) + theme(panel.grid=element_blank())
```

  The probability density function, $f(x)$, is defined such that the total area under the curve is 1.

$$
\int_{-\infty}^{\infty} f(x) dx = 1
$$

$P(a \leq X \leq b) = \int_a^b f(x) dx$

# Cumulative distribution function, cdf

The cumulative distribution function, sometimes called just the
distribution function, $F(x)$, is defined as:

$$F(x) = P(X<x) = \int_{-\infty}^x f(x) dx$$

```{r wtpdfcdf, out.width="49%", fig.show="hold"}
plot(ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("x") + ylab("f(x)") + geom_area(data=df.nwt %>% filter(w<4.0)) + annotate("label",label=sprintf("P(X<4.0) = F(4.0) = %.2f", pnorm(4,3.5,0.5)), x=2.7, y=0.4, hjust=0))
df.nwt$F <- pnorm(df.nwt$w, 3.5, 0.5)
plot(ggplot(df.nwt, aes(x=w, y=F)) + geom_line() + xlab("x") + ylab("F(x)") + theme_bw() + geom_point(aes(x=4, y=pnorm(4,3.5,.5))) + annotate("label",label=sprintf("F(4.0)=%.2f", pnorm(4,3.5,.5)), x=4, y=.84, hjust=-0.2))##+ ggtitle("Cumulative distribution function") 
```

$$P(X<x) = F(x)$$

$$P(X \geq x) = 1 - F(x)$$

$$P(a \leq X < b) = F(b) - F(a)$$

# Probability

When the entire population is known, probabilities can be computed by summing the number of observations that fulfil the criteria and divide by the total number.

- The probability of a baby weight above 4.0 kg, $P(W>4.0)$

```{r wtbabiesP4, out.width="50%", warning=FALSE, message=FALSE, fig.cap="Weight distribution"}
plot(ggplot(data.frame(w=wt), aes(x=w, fill=w>4)) + geom_histogram(color="white", binwidth=0.2) + theme_bw() + xlab("weight (kg)") + scale_fill_manual(values=c("grey50", "blue")) + theme_bw() + theme(legend.position="none"))
```

```{r echo=TRUE}
library(UsingR)
##The weights are originally in ounces, transform to kg
ounce <- 0.0283495231
wt <- babies$wt*ounce
## P(W > 4.0)
## Count the number of babies with a weight > 4.0 kg
sum(wt>4)
## How many babies in total
length(wt)
## Fraction of babies with weight > 4.0 kg, this is P(W>4.0)
sum(wt>4)/length(wt)
## Another way to compute P(W>4.0)
mean(wt>4)
```

# Exercise: 

Based on the babies population, compute the following probabilities

 - $P(X<2.6)$
 - $P(2.3<X<4.2)$


### Smoking status of a random mother

```{r}
babies %>% group_by(smoke) %>% summarise(n=n()) %>% mutate("p"=n/sum(n), code=c('never','smokes now', 'until current pregnancy','once did, not now','unknown'))
```
Let $S$ denote the smoking status of a random mother. The probability that a random mother never smoked: $P(S=0) = p(0) = 0.4401$ Note that $S$ is a discrete random variable.


## Conditional probability

Compute the probability that a smoking mother has a baby with a weight below 2.6 kg. 

$$P(W<2.6|S=1)$$

Compute the probability that a mother who never smoked has a baby with a weight below 2.6 kg.

$$P(W<2.6|S=0)$$

## Diagnostic tests

```{r}
kable(matrix(c(98,882,980, 16, 4, 20, 114, 886, 1000), byrow = TRUE, ncol=3, dimnames=list(c("not cancer", "cancer", "total"), c("pos", "neg", "tot")))) %>% kable_styling("striped", full_width = FALSE)
```

- What is the probability of a positive test result from a person with cancer?
- What is the probability of a negative test result from a person without cancer?
- If the test is positive, what is the probability of having cancer?
- If the test is negative, what is the probability of not having cancer?
- Connect the four computed probabilities with the following four tems;
  - Sensitivity
  - Specificity
  - Positive predictive value (PPV)
  - Negative predictive value (NPV)
  
  Discuss in your group!


# Descriptive statistics
## Data types
- Categorical
  - Nominal: named.
    Ex: dead/alive, healthy/sick, WT/mutant, AA/Aa/aa, male/female, red/green/blue
  - Ordinal: named and ordered. Ex: pain (weak, moderate, severe), AA/Aa/aa, very young/young/middle age/old/very old, grade I, II, III, IV
  
Reported as frequencies, proportions, summarized using mode

- Quantitative
  - Interval: no absolute zero, meaningful to compute interval ratios.
    Ex: time, temperature
  - Ratio: absolute zero, meaningful to compute ratios. Ex. height,
    weight, concentration

Often not necessary to distinguish between interval and ratio scale, can be more useful to divide the quantitative scales into

- Discrete: finite or countable infinite values
- Continuous: infinitely many uncountable values

Useful summary statistics include mean, median, variance, standard deviation.

# Descriptive statistics - Measures of location
- Mode: the most common value, can be computed also for categorical data
- Median: The value that divide the ordered data values into two equally sized groups. 50% of the values are below the median.

```{r normpdf, fig.width=2, out.width="20%"}
df <- data.frame(x=rnorm(200, 3.5, 2))
ggplot(df, aes(y=x)) + geom_boxplot() + theme_bw()
```

- Mean: the arithmetic mean, also called the average

# Expected value

The expected value of a random variable, or the population mean, is
  
$$\mu = E[X] = \frac{1}{N}\displaystyle\sum_{i=1}^N x_i,$$
where the sum is over all $N$ data points in the population.

The above formula is probably the most intuitive for finite populations, but for infinite populations other definitions can be used.

For a discrete random variable:
  
$$\mu = E[X] = \displaystyle\sum_{k=1}^K x_k p(x_k),$$

where the sum is taken over all possible outcomes.

For a continuous random variable:

$$\mu = E[X] = \int_{-\infty}^\infty x f(x) dx$$

## Linear transformations and combinations

$$E(aX) = a E(X)$$

$$E(X + Y) = E(X) + E(Y)$$

$$E[aX + bY] = aE[X] + bE[Y]$$

## Examples, all with mean value: 3.50

```{r mean35, warning=FALSE, message=FALSE, fig.width=7, fig.height=4}
#df <- data.frame(x=rnorm(200, 3.5, 2))
df$x <- df$x - mean(df$x) + 3.5
y <- exp(rnorm(200, 0, 1))
y <- y+3.5 - mean(y)
df$y=y

df$z <- rep(c(0,10), c(200*.65, 200*.35))

df$a <- rnorm(200, 3.5, 8)
df$a <- df$a - mean(df$a) + 3.5
b <- exp(rnorm(200, 2, 1))
df$b <- b+3.5 - mean(b)
c <- c(rnorm(150, 10, 3), rnorm(50,0,1))
c <- c + 3.5 - mean(c)
df$c <- c

plot(ggplot(melt(df), aes(x=value)) + geom_histogram(color="white", bins=30)  + facet_wrap(~variable, scale="free") + theme_bw())#+ geom_text(data=data.frame(variable=colnames(df), label=sprintf("mean: %.2f", colMeans(df))), aes(x=8,y=20,label=label))
``` 


# Descriptive statistics - Measures of spread
- Quartiles - the three numbers that divide the data into four equally sized groups.
  - Q1. 25\% of the values are below Q1. Divides the values below the median into equally sized groups.
  - Q2. 50\% of the values are below Q2. Q2 is the median.
  - Q3. 75\% of the values are below Q3. Divides the values above the median into equally sized groups.

```{r boxplot1, fig.width=2, out.width="20%"}
ggplot(df, aes(y=x)) + geom_boxplot() + theme_bw()
```

- IQR: interquartile range: Q3 - Q1
- Variance, $\sigma^2$.
  The variance is the mean squared distance from the mean value ($\bar x$).
- Standard deviation, $\sigma = \sqrt{\sigma^2}$.

# Variance and standard deviation

The variance of a random variable, the population variance, is defined as
    
$$\sigma^2 = var(X) = E[(X-\mu)^2]$$
  
$$\sigma^2 = var(X) = \frac{1}{N} \sum_i^N (x-\mu)^2,$$
where the sum is over all $N$ data points in the population.

$$\sigma^2 = var(X) = E[(X-\mu)^2] = \left\{\begin{array}{ll}
\displaystyle\sum_{k=1}^K (x_k-\mu)^2 p(x_k) & \textrm{if }X\textrm{ discrete} \\
\\
\displaystyle\int_{-\infty}^\infty (x-\mu)^2 f(x) dx & \textrm{if }X\textrm{ continuous}
\end{array}\right.$$

Standard deviation

$$\sigma = \sqrt{var(X)}$$

## Linear transformations and combinations

$$var(aX) = a^2 var(X)$$

For independent random variables X and Y

$$var(aX + bY) = a^2var(X) + b^2var(Y)$$


```{r babies}
ounce <- 0.0283495231
babies$wtkg <- babies$wt*ounce
babies$smoking <- 1*(babies$smoke==1)
babies$smoking[babies$smoke==9] <- NA
babies$smoker <- c("non-smoker", "smoker")[babies$smoking + 1]
babies$age[babies$age==99] <- NA
babies$weightmother <- babies$wt1*0.45359237
babies$weightmother[babies$wt1==999] <- NA
#babies %>% dplyr::select(smoker, age, wtkg, weightmother) %>% group_by(smoker) %>% #summarize_all(.funs=c(m=mean, s=sd), na.rm=TRUE)
N <- as.data.frame(table(babies$parity, babies$smoker))
F <- sapply(split(N$Freq, N$Var2), function(x) x/sum(x))
N <- sapply(split(N$Freq, N$Var2), identity)
``` 

# Exercise: Data summary

Consider the below data and summarize each of the variables.

```{r }
#babies %>% transmute(id=id, smoker=c(smoker="yes", "non-smoker"="no")[smoker], "baby weight" = wtkg, "mother weight"=weightmother, "mother age"=age, gender=sample(c("F", "M"), nrow(babies), replace=TRUE), parity=parity) %>% filter((1:nrow(babies)) %in% sample(1:nrow(babies), 13)) %>%  arrange(smoker) %>% kable() %>% kable_styling("striped")

set.seed(1)
baby <- data.frame(id=1:10, 
                   smoker=rep(c("yes", "no"), each=5), 
                   "baby weight (kg)"=c(2.8,3.2,3.5,2.7,3.3, 3.7,3.3,4.3,3.2,3.0),
                   gender = sample(c("F", "M"), 10, replace=TRUE),
                   "mother weight (kg)" = c(64, 65, 64, 73, 59, 61, 52, 59, 65, 73), 
                   "mother age" = c(21, 27, 31, 32, 39, 26, 27, 21, 28, 33),
                   parity = c(2,1,2,0,3,0,2,0,1,4),
                   married = sample(c("yes", "no"), 10, replace=TRUE),
                   check.names=FALSE)
baby %>% kable() %>% kable_styling("striped", full_width = FALSE)
``` 

# Statistical inference

Draw conclusions regarding properties of a population based on observations of a random sample from the population.

## Sample mean

The sample mean is denoted $m = \bar x$. For a sample of size $n$ the sample mean is:
    
$$m = \bar x = \frac{1}{n}\displaystyle\sum_{i=1}^n x_i$$

When we only have a sample of size $n$, the sample mean $m$ is our best estimate of the population mean. It is possible to show that the sample mean is an unbiased estimate of the sample mean, i.e. the average (over many size $n$ samples) of the sample mean is $\mu$.

  $$E[\bar X] = \frac{1}{n} n E[X] = E[X] = \mu$$
  
## Sample variance

The sample variance is computed as

$$s^2 = \frac{1}{n-1} \sum_{i=1}^n (x-m)^2$$
The sample variance is an unbiased estimate of the population variance.

$$E[s^2] = \sigma^2$$

------

# Normal distribution

The normal distribution (sometimes referred to as the Gaussian distribution) is a common probability distribution and many continuous random variables can be described by the normal distribution or be approximated by the normal distribution.

The normal probability density function

$$f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2}$$

describes the distribution of a normal random variable, $X$, with expected value $\mu$ and standard deviation $\sigma$. In short we write $X \sim N(\mu, \sigma)$.

```{r norm, out.width="50%", fig.show="hold"}
#ggplot(pop.FN, aes(x=Bodyweight)) + geom_histogram(binwidth=1, aes(y=stat(density)), color="white") + theme_bw() + geom_line(data=den.FN, aes(x=x, y=nfx), color="red")
x <- seq(-3.5, 3.5, .1)
dN <- data.frame(x=x, fx=dnorm(x))
plot(ggplot(dN, aes(x=x, y=fx)) + geom_line() + scale_x_continuous(breaks=-3:3, labels=c(expression(mu-3*sigma),expression(mu-2*sigma), expression(mu-1*sigma), expression(mu), expression(mu+sigma), expression(mu + 2*sigma),  expression(mu + 3*sigma))) + xlab("") + ylab("f(x)") + theme_bw())
```

The bell-shaped normal distributions is symmetric around $\mu$ and $f(x) \rightarrow 0$ as $x \rightarrow \infty$ and as $x \rightarrow -\infty$.

As $f(x)$ is well defined, values for the cumulative distribution function $F(x) = \int_{- \infty}^x f(x) dx$ can be computed.

```{r out.width="45%", fig.show="hold"}
dN$Fx <- pnorm(x)
ggplot(dN, aes(x=x, y=fx)) + geom_line() + scale_x_continuous(breaks=-3:3, labels=c(expression(mu-3*sigma),expression(mu-2*sigma), expression(mu-1*sigma), expression(mu), expression(mu+sigma), expression(mu + 2*sigma),  expression(mu + 3*sigma))) + xlab("") + ylab("f(x)") + theme_bw() + ggtitle("Probability density function")
ggplot(dN, aes(x=x, y=Fx)) + geom_line() + scale_x_continuous(breaks=-3:3, labels=c(expression(mu-3*sigma),expression(mu-2*sigma), expression(mu-1*sigma), expression(mu), expression(mu+sigma), expression(mu + 2*sigma),  expression(mu + 3*sigma))) + xlab("") + ylab("F(x)") + theme_bw() + ggtitle("Cumulative distribution function")
```


If $X$ is normally distributed with expected value $\mu$ and
standard deviation $\sigma$ we write:

$$X \sim N(\mu, \sigma)$$

Using transformation rules we can define

$$Z = \frac{X-\mu}{\sigma}, \, Z \sim N(0,1)$$ 

Values of $F(z)$, the standard normal distribution, are tabulated (and easy to compute in R using the function ``pnorm``).

Some value of particular interest:
F(1.64) = 0.95
F(1.96) = 0.975

As the normal distribution is symmetric
F(-1.64) = 0.05
F(-1.96) = 0.025

P(-1.96 < Z < 1.96) = 0.95

<!-- Show table? -->

<!-- dnorm -->

<!-- pnorm -->

# Sum of two normal random variables

If $X \sim N(\mu_1, \sigma_1^2)$ and $Y \sim N(\mu_2, \sigma_2^2)$ are two independent normal random variables, then their sum is also a random variable:

$$X + Y \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$$

and 

$$X - Y \sim N(\mu_1 - \mu_2, \sigma_1^2 + \sigma_2^2)$$
  
# Central limit theorem

> The sum of $n$ independent and equally distributed random variables
> is normally distributed, if $n$ is large enough.

As a result of central limit theorem, the distribution of fractions or mean values of a sample follow the normal distribution, at least if the sample is large enough (a rule of thumb is that the sample size $n>30$).


## Example: Mean BMI

The data set ``fat`` consists measurements for 252 men, let's take a closer look at the BMI.

```{r BMIhist, out.width="60%"}
pl <- ggplot(fat, aes(x=BMI)) + geom_histogram(aes(y=stat(density)), binwidth=2, color="white") + theme_bw()
plot(pl)
```


```{r BMI, echo=TRUE}
## Population mean
mu <- mean(fat$BMI)
mu
## Population variance
sigma2 <- var(fat$BMI)/nrow(fat)*(nrow(fat)-1)
sigma2
## Population standard variance
sigma <- sqrt(sigma2)
sigma
```

Randomly sample 3, 5, 10, 15, 20, 30 men and compute the mean value, $m$. Repeat many times to get the distribution of mean values.

```{r owexample, out.width="70%", fig.width=7, fig.show="hold"}
#hist(fat$BMI)
bmi <- fat$BMI
n <- c(3,5,10,15,20, 30)
rs <- sapply(n, function(k) replicate(10000, mean(sample(bmi, k))))
colnames(rs) <- paste(sprintf("n=%i, m=%.4f", n, colMeans(rs)))
plot(ggplot(melt(rs, varnames=c("rep", "n")), aes(x=value, color=factor(n))) + geom_density() + theme_bw())
```

Note, mean is just the sum divided by the number of samples $n$.
