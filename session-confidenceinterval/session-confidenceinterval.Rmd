---
title: "Confidence intervals"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
require(kableExtra)
library(reshape2)
library(ggplot2)
require(knitr)
require(UsingR)
knitr::opts_chunk$set(fig.width=3.5, fig.height=3.5, echo = FALSE, cache=TRUE, error=FALSE, warnings=FALSE, fig.path="session-confidenceinterval-files/figures/")
options(digits=2)
```

### Learning outcomes

- to understand and define sampling distribution and standard error
- to compute standard error of mean and proportions
- to compute confidence interval of mean and proportions using the normal approximation
- to compute confidence interval of mean using the t-distribution


# Statistical inference

Draw conclusions regarding properties of a population based on observations of a random sample from the population.

In many (most) experiments it is not feasible (or even possible) to examine the entire population. Instead we study a random sample.

# Example: pollen allergy

How large proportion of the Swedish population is allergic to pollen?

Investigate this by randomly selecting 100 persons. It is important to actually sample randomly, ideally every individual should have the same probability of being sampled.

Observe that 33 of the 100 has a pollen allergy. Hence, the observed sample proportion is $p=0.33$.

What does this say about the population proportion $\pi$?

Our best guess would be $\pi \approx p = 0.33$, but how accurate is this?

If the experiment is repeated, i.e. we randomly select 100 people again and study the observed proportion, we will get a different result.

Let's assume that $\pi=0.3$ and simulate taking a sample of 100 many times. If we sample 10000 times we get 10000 estimates of p and can plot the sampling distribution.

```{r }
set.seed(13)
p <- replicate(10000, mean(sample(0:1, 100, p=c(.7,.3), replace=TRUE)))
ggplot(data.frame(p=p), aes(x=p)) + geom_histogram(color="white", binwidth=0.02) + theme_bw()
#hist(p)
mean(p)
```   

Of course, we do not know $\pi$. We only have one sample and one observed sample frequency $p$.


## Bootstrap interval
  
Using bootstrap we can resample (with replacement) from our sample to estimate the uncertainty.
  
```{r CIboot, out.width="70%"}
#x <- sample(0:1, 100, p=c(.7,.3), replace=TRUE)
x <- rep(0:1, c(67,33))
pboot <- replicate(10000, mean(sample(x, replace=TRUE)))
ciboot <- quantile(pboot, c(0.025, 0.975))
ggplot(data.frame(x=pboot), aes(x=x, fill=x>ciboot[1] & x<ciboot[2])) + geom_histogram(color="white", binwidth=0.02) + theme_bw() + theme(legend.position="none") + xlab("p") + geom_line(data=data.frame(x=ciboot, y=500), aes(x=x, y=y), arrow=arrow(ends="both")) + annotate("label", x=mean(ciboot), y=500, label="95%")
```   
  
The 95% bootstrap interval is [`r ciboot`].

The bootstrap is very useful if you do not know the distribution of our sampled propery. But in our example we actually do.


## Standard error

  In one trial (selecting one person) the probability of that person being allergic is $\pi$.
    
  ```{r}
kable(matrix(c('', 'not allergic', 'allergic', 'x','0','1','p(x)','$\\pi$','1 - $\\pi$'), byrow=TRUE, ncol=3)) %>% kable_styling("striped", full_width=FALSE)
```

This is called a Bernoulli trial.

  $E[X] = \sum_k x_k p(x_k) = 0 * (1-\pi) + 1*\pi = \pi$

  $var(X) = E[(X-E[X])^2] = [...] = E[X^2] - E[X] = \pi(1-\pi)$
  
If we randomly select 100 persons, this is equivalent to 100 independent Bernouilly trials.

The fraction of sucesses (1) in a sample of size n is another random variable; $P = \frac{1}{n} \sum_{i=1}^n X_i$
  
  Central limit theorem gives that $P$ is approximately normally distributed.
  
  $$E[P] = \frac{1}{n} \sum_i E[X_i] = \frac{1}{n} n \pi = \pi$$
  
  $$var(P) = \frac{1}{n^2} \sum_i var(X_i) = \frac{1}{n} n \pi (1-\pi) = \frac{\pi(1-\pi)}{n}$$
 
  $$P \sim N\left(\pi, \sqrt{\frac{\pi(1-\pi)}{n}}\right)$$

The standard deviation of the sampling distribution is called the standard error. For proportions the standard error is $SE=\sqrt{\frac{\pi(1-\pi)}{n}})$.

The standard error tells us how accurate the sample estimate is.

The formula for the standard error contains our unknown parameter $\pi$. No problem, let's replace $\pi$ with our best estimate of $\pi$: $p$.

$$
SE = \sqrt{\frac{p(1-p)}{n}})
$$

## Confidence interval

Based on what we know of the normal distribution, we can conclude that 
 
$$Pr(-1.96 < \frac{P-\pi}{\sqrt{\frac{\pi(1-\pi)}{n}}}<1.96) = 0.95$$
We can rewrite this to
$$Pr\left(\pi-1.96\sqrt{\frac{\pi(1-\pi)}{n}}) < P < \pi + 1.96\sqrt{\frac{\pi(1-\pi)}{n}}\right) = 0.95$$
in other words the observed fraction $p$ will fall between $\pi \pm \sqrt{\frac{\pi(1-\pi)}{n}}$ with 95% probability.

The equation can also be rewritten to 
$$Pr\left(P-1.96\sqrt{\frac{\pi(1-\pi)}{n}}) < \pi < P + 1.96\sqrt{\frac{\pi(1-\pi)}{n}}\right) = 0.95$$
The observed confidence interval is what we get when we replace the random variable $P$ with our observed fraction, i.e. our 95% confidence interval is

$$p-1.96\sqrt{\frac{\pi(1-\pi)}{n}}) < \pi < p + 1.96\sqrt{\frac{\pi(1-\pi)}{n}}$$
 But $\pi$ is unknown!
 Solution: replace $\pi$ with the estimate $p$!
 
$$p-1.96\sqrt{\frac{p(1-p)}{n}}) < \pi < p + 1.96\sqrt{\frac{p(1-p)}{n}}$$
$$\pi = p \pm 1.96 \sqrt{\frac{p(1-p)}{n}}$$

A 95% confidence interval will have 95% chance to cover the true value.

```{r CI, fig.width=7, fig.height=3}
ggplot(data.frame(x=1:40, p=p[61:100]) %>% mutate(ymin=p-1.96*sqrt((p*(1-p))/100), ymax=p+1.96*sqrt((p*(1-p))/100)), aes(x=x, ymin=ymin, ymax=ymax, color=0.3>ymin & 0.3<ymax)) + geom_errorbar() + geom_hline(yintercept=0.3) + xlab("") + ylab("p") + theme_bw() + theme(legend.position="none")
```

Back to our example. $p=0.33$ and $SE=\sqrt{\frac{p(1-p)}{n}} = `r sqrt(0.33*(1-0.33)/100)`$.

Hence, the 95% confidence interval is $\pi = 0.33 \pm 1.96 * 0.05 = 0.33 \pm 0.092$ or $(0.33-0.092, 0.33+0.092) = (0.24, 0.42)$


# Expected value and variance of mean
  
  The mean of $n$ independent identically distributed random variables with $E[X_i] = E[X] = \mu$ and $var(X_i) = var(X) = \sigma^2$
  
  $$\bar X = \frac{1}{n}\sum_{i=1}^n X_i$$
  
  is also a random variable.
  
  $$E[\bar X] = \frac{1}{n} n E[X] = E[X] = \mu$$
  
  This shows that the sample mean is an unbiased estimate of the population mean, i.e. the average (over many size $n$ samples) of the sample mean is $\mu$.

$$var(\bar X) = var(\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n^2} var(\sum X_i) = \frac{1}{n^2} \sum var(X_i) = \frac{1}{n^2} n\, var(X) = \frac{\sigma^2}{n}$$

If $X_i \sim N(\mu, \sigma)$ then $\bar X \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right)$. In words this means that if $X_i$ is normally distributed with mean $\mu$ and standard deviation $\sigma$), the the sample mean will also be normally distributed with mean $\mu$ and standard deviation $\sigma$.


## Confidence interval of mean

$$
\begin{aligned}
P(-1.96 \leq \mathbf Z \leq 1.96) = 0.95 \iff \\
P(-1.96 \leq \frac{\mathbf{\bar X} - \mu}{\sigma/\sqrt{n}} \leq 1.96) = 0.95 \iff \\
P(\mathbf{\bar X} - 1.96\frac{\sigma}{\sqrt{n}} \leq \mu \leq \mathbf{\bar X} + 1.96\frac{\sigma}{\sqrt{n}}) = 0.95
\end{aligned}
$$

For a sample of size $n$ with sample mean $m=\bar x$ the 95% confidence interval is 

$$\mu = m \pm 1.96 \frac{\sigma}{\sqrt{n}}$$

Note that according to the central limit theorem, if $n$ is large the above holds even if $X_i$ are not normally distributed, as long as all $X_i$ are idependent and identically distributed. 

Furthermore, if $n$ is large enough, the population standard deviation $\sigma$ can be replaced by the sample standard deviation $s$ and the 95% confidence interval can be computed as

$$\mu = m \pm 1.96 \frac{s}{\sqrt{n}}$$

# Student's t-distribution

What if the sample size is small?

If $\sigma$ is known we can still use the formula

$$\mu = m \pm 1.96 \frac{\sigma}{\sqrt{n}}$$

If $\sigma$ is unknown and we replace it by the sample standard deviation $s$? 

William Gosset, brewery master at Guinness in Dublin in early 1900, saw the need for this and developed the t-distribution.

Remember that for known $\sigma$
$P(\mu-1.96\frac{\sigma}{\sqrt{n}} \leq \bar X \leq \mu+1.96\frac{\sigma}{\sqrt{n}}) = 0.95$

but if we replace $\sigma$ with $s$ the uncertainty increases and we have

$P(\mu-1.96\frac{\sigma}{\sqrt{n}} \leq \bar X \leq \mu+1.96\frac{\sigma}{\sqrt{n}}) < 0.95$

The statistic

$$t = \frac{\bar X - m}{\frac{s}{\sqrt{n}}}$$

follows a t-distribution with $n-1$ degrees of freedom.

```{r}
x <- seq(-4, 4, .1)
n <- c(2,3,5,10,20,30)
dN <- data.frame(x=x, fx=dnorm(x))
dT <- sapply(n, function(nn) dt(x, df=nn-1))
colnames(dT) <- paste("n =", n)
dT <- data.frame(x, dT, check.names=FALSE)
ggplot(melt(dT, id.vars='x'), aes(x=x, y=value, color=variable)) + geom_line(data=dN, aes(x=x,y=fx), color="black", size=2) + geom_line() + scale_color_discrete("") + theme_bw() + theme(panel.grid=element_blank(), legend.position=c(1,1), legend.justification = c(1,1)) + ylab("f(x)")
#+ scale_x_continuous(breaks=-3:3, labels=c(expression(mu-3*sigma),expression(mu-2*sigma), expression(mu-1*sigma), expression(mu), expression(mu+sigma), expression(mu + 2*sigma),  expression(mu + 3*sigma))) + xlab("") + ylab("f(x)") + theme_bw()
```

