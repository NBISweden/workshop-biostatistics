[
["index.html", "Introduction to Biostatistics and Machine Learning Preface", " Introduction to Biostatistics and Machine Learning NBIS 2020-11-16 – 2020-10-20 Preface Work in progress. "],
["probability-theory.html", "Chapter 1 Probability theory 1.1 Random variables 1.2 Simulate distributions 1.3 Parametric discrete distributions 1.4 Conditional probability", " Chapter 1 Probability theory Learning outcomes understand the concept of random variables and probability understand and learn to use resampling to compute probabilities 1.1 Random variables The outcome of a random experiment can be described by a random variable. Whenever chance is involved in the outcome of an experiment the outcome is a random variable. A random variable can not be predicted exactly, but the probability of all possible outcomes can be described. A random variable is usually denoted by a capital letter, \\(X, Y, Z, \\dots\\). Values collected in an experiment are observations of the random variable, usually denoted by lowercase letters \\(x, y, z, \\dots\\). The population is the collection of all possible observations of the random variable. Note, the population is not always countable. A sample is a subset of the population. Example random variables: The weight of a random newborn baby The smoking status of a random mother The hemoglobin concentration in blood The number of mutations in a gene BMI of a random man Weight status of a random man (underweight, normal weight, overweight, obese) The result of throwing a die 1.1.1 Discrete random variables A discrete random number has countable number of outcomes values, such as {1,2,3,4,5,6}; {red, blue, green}; {tiny, small, average, large, huge} or all integers. A discrete random variable can be described by its probability mass function, pmf. The probability that the random variable, \\(X\\), takes the value \\(x\\) is denoted \\(P(X=x) = p(x)\\). Note that: \\(0 \\leq p(x) \\leq 1\\), a probability is always between 0 and 1. \\(\\sum p(x) = 1\\), the sum over all possible outcomes is 1. Example 1 The number of dots on a die When rolling a die the there are six possible outcomes; 1, 2, 3, 4, 5 and 6, each of which have the same probability, if the die is fair. The outcome of one dice roll can be described by a random variable \\(X\\). The probability of a particular outcome \\(x\\) is denoted \\(P(X=x)\\) or \\(p(x)\\). The probability mass function of a fair six-sided die can be summarized in a table; x 1.00 2.00 3.00 4.00 5.00 6.00 p(x) 0.17 0.17 0.17 0.17 0.17 0.17 or in a barplot; Figure 1.1: Probability mass function of a die. Example 2. The smoking status of a random mother The random variable has two possible outcomes; non-smoker (0) and smoker (1). The probability of a random mother being a smoker is 0.39. non-smoker smoker x 0 1 p(x) 0.61 0.39 Example 3 The number of bacterial colonies on a plate Figure 1.2: Probability mass distribution of the number of bacterial colonies on an agar plate. Exercise Urnexempel, räkna sannolikhet? 1.1.2 Continuous random variable A continuous random number is not limited to discrete values, but any continuous number within one or several ranges is possible. Examples: weight, height, speed, intensity, … A continuous random variable can be described by its probability density function, pdf. Figure 1.3: Probability density function of the weight of a newborn baby. The probability density function, \\(f(x)\\), is defined such that the total area under the curve is 1. \\[ \\int_{-\\infty}^{\\infty} f(x) dx = 1 \\] The area under the curve from a to b is the probability that the random variable \\(X\\) takes a value between a and b. \\(P(a \\leq X \\leq b) = \\int_a^b f(x) dx\\) The cumulative distribution function, cdf, sometimes called just the distribution function, \\(F(x)\\), is defined as: \\[F(x) = P(X&lt;x) = \\int_{-\\infty}^x f(x) dx\\] \\[P(X&lt;x) = F(x)\\] As we know that the total probability (over all x) is 1, we can conclude that \\[P(X \\geq x) = 1 - F(x)\\] and thus \\[P(a \\leq X &lt; b) = F(b) - F(a)\\] 1.2 Simulate distributions As seen in previous chapter, once the distribution is known, we can compute probabilities, such as \\(P(X=x), P(X&lt;x)\\) and \\(P(X \\geq x)\\). If the distribution is not known, simulation might be the solution. (#exm:Simulate coin toss) In a single coin toss the probabity of heads is 0.5. In 20 coin tosses, what is the probability of at least 15 heads? Figure 1.4: A coin toss. Urn model with one black ball (heads) and one white ball (tails). A single coin toss can be modelled by an urn with two balls. When a ball is drawn randomly from the urn, the probability to get the black ball (heads) is \\(P(X=H) = 0.5\\). If we want to simulate tossing 20 coins (or one coin 220 times) we can use the same urn model, if the ball is replaced after each draw. In R we can simulate random draws from an urn model using the function sample. # A single coin toss sample(c(&quot;H&quot;, &quot;T&quot;), size=1) ## [1] &quot;H&quot; # Another coin toss sample(c(&quot;H&quot;, &quot;T&quot;), size=1) ## [1] &quot;T&quot; Every time you run the sample a new coin toss is simulated. The argument size tells the function how many balls we want to draw from the urn. To draw 20 balls from the urn, set size=20, remember to replace the ball after each draw! # 20 independent coin tosses (coins &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), size=20, replace=TRUE)) ## [1] &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; ## [20] &quot;H&quot; How many heads did we get in the 20 random draws? # How many heads? sum(coins == &quot;H&quot;) ## [1] 11 We can repeat this experiment (toss 20 coins and count the number of heads) several times to eastimate the distribution of number of heads in 20 coin tosses. To do the same thing several times we use the function replicate. Nheads &lt;- replicate(1000, { coins &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), size=20, replace=TRUE) sum(coins == &quot;H&quot;) }) Plot distribution of the number of heads in a histogram. hist(Nheads, breaks=0:20) Now, let’s get back to the question; when tossing 20 coins, what is the probability of at least 15 heads? \\(P(X \\geq 15)\\) Count how many times out of our 1000 exeriments the number is 15 or greater sum(Nheads &gt;= 15) ## [1] 20 From this we conclude that \\(P(X \\geq 15) =\\) 0.02 Exercise 1.1 In a single coin toss the probabity of heads is 0.5. In 20 coin tosses, what is the probability of exactly 15 heads? what is the probability of less than 7 heads? What is the most probable number of heads? what is the probability of 5 tails or less? what is the probability of 2 heads or less? Exercise 1.2 When rolling 10 six sided dice, what is the probability to get at least 5 sixes? Exercise 1.3 30% of a large populationis allergic to pollen. If you randomly select 12 people to participate in your study, what is the probability than none of them will be allergic to pollen? 1.3 Parametric discrete distributions 1.3.1 Bernoulli trial A Bernoulli trial is a random experiment with two outcomes; success and failure. The probability of success, \\(P(success) = p\\), is constant. The probability of failure is \\(P(failure) = 1-p\\). When coding it is convenient to code success as 1 and failure as 0. The outcome of a Bernoulli trial is a discrete random variable, \\(X\\). x 0 1 p(x) 1-p p 1.3.2 Binomial distribution Also the number of successes in a series of independent and identical Bernoulli trials is a discrete random variable. \\(Y = \\sum_{i=0}^n X_i\\) The probability mass function of \\(Y\\) is called the binomial distribution. 1.3.3 Hypergeometric distribution 1.3.4 Poisson distribution ** Exercise: Dice experiment ** Kankse använda detta som ett illustrativt exempel iställer för som en övning? * When throwing 10 dice, how many dice show 6 dots? Define the random variable of interest What are the possible outcomes? Which is the most likely number of sixes? What is the probability to get exactly 2 sixes when throwing ten dice? On average how many sixes do you get when throwing ten dice? What is the probability to get 4 or more sixes when throwing ten dice? Estimate the probability mass function ** Exercises ** When throwing a fair die, what is the probability to get 4? 5 or more? an odd number? When throwing two dice, what is the probabilty of first getting 2 and then 3 or more? When throwing 20 fair dice, what is the probabaility to get 6 exactly 5 times? to get 6 5 or more times? When the entire population is known, probabilities can be computed by counting the fraction of observations that fulfil the criteria of interest. 1.4 Conditional probability P(X 3.5 | S = 1) 1.4.1 Diagnostic tests - passar bättre i samband med klassificering? pos neg tot not cancer 98 882 980 cancer 16 4 20 total 114 886 1000 What is the probability of a positive test result from a person with cancer? What is the probability of a negative test result from a person without cancer? If the test is positive, what is the probability of having cancer? If the test is negative, what is the probability of not having cancer? Connect the four computed probabilities with the following four tems; Sensitivity Specificity Positive predictive value (PPV) Negative predictive value (NPV) Discuss in your group! "],
["desc.html", "Chapter 2 Descriptive statistics 2.1 Data types 2.2 Measures of location 2.3 Measures of spread 2.4 Random sample", " Chapter 2 Descriptive statistics Learning outcomes: be aware of data types compute measures of location, including mean and median compute measures of spread, including quantiles, variance and standard deviation learn about probability density functions and cumulative distribution functions compute population mean and variance compute sample mean and variance use normal distribution understand the central limit theorem 2.1 Data types Categorical Nominal: named. Ex: dead/alive, healthy/sick, WT/mutant, AA/Aa/aa, male/female, red/green/blue Ordinal: named and ordered. Ex: pain (weak, moderate, severe), AA/Aa/aa, very young/young/middle age/old/very old, grade I, II, III, IV Reported as frequencies, proportions, summarized using mode Quantitative (numeric) Discrete: finite or countable infinite values. Ex. counts, number of cells, number of reads Continuous: infinitely many uncountable values. Ex. height, weight, concentration Useful summary statistics include mean, median, variance, standard deviation. 2.2 Measures of location Mode: the most common value, can be computed also for categorical data Median: The value that divide the ordered data values into two equally sized groups. 50% of the values are below the median. Mean: the arithmetic mean, also called the average 2.2.1 Expected value The expected value of a random variable, or the population mean, is \\[\\mu = E[X] = \\frac{1}{N}\\displaystyle\\sum_{i=1}^N x_i,\\] where the sum is over all \\(N\\) data points in the population. The above formula is probably the most intuitive for finite populations, but for infinite populations other definitions can be used. For a discrete random variable: \\[\\mu = E[X] = \\displaystyle\\sum_{k=1}^K x_k p(x_k),\\] where the sum is taken over all possible outcomes. For a continuous random variable: \\[\\mu = E[X] = \\int_{-\\infty}^\\infty x f(x) dx\\] 2.2.1.1 Linear transformations and combinations \\[E(aX) = a E(X)\\] \\[E(X + Y) = E(X) + E(Y)\\] \\[E[aX + bY] = aE[X] + bE[Y]\\] (#exm:All with mean value: 3.50) Several very different distributions can still have the same mean value. 2.3 Measures of spread Quartiles - the three values that divide the data values into four equally sized groups. Q1. First quartile. 25% of the values are below Q1. Divides the values below the median into equally sized groups. Q2. Second quartile. 50% of the values are below Q2. Q2 is the median. Q3. Third quartile. 75% of the values are below Q3. Divides the values above the median into equally sized groups. IQR: interquartile range: Q3 - Q1 Variance, \\(\\sigma^2\\). The variance is the mean squared distance from the mean value. Standard deviation, \\(\\sigma = \\sqrt{\\sigma^2}\\). 2.3.1 Variance and standard deviation The variance of a random variable, the population variance, is defined as \\[\\sigma^2 = var(X) = E[(X-\\mu)^2]\\] \\[\\sigma^2 = var(X) = \\frac{1}{N} \\sum_{i=1}^N (x-\\mu)^2,\\] where the sum is over all \\(N\\) data points in the population. \\[\\sigma^2 = var(X) = E[(X-\\mu)^2] = \\left\\{\\begin{array}{ll} \\displaystyle\\sum_{k=1}^K (x_k-\\mu)^2 p(x_k) &amp; \\textrm{if }X\\textrm{ discrete} \\\\ \\\\ \\displaystyle\\int_{-\\infty}^\\infty (x-\\mu)^2 f(x) dx &amp; \\textrm{if }X\\textrm{ continuous} \\end{array}\\right.\\] Standard deviation \\[\\sigma = \\sqrt{var(X)}\\] 2.3.1.1 Linear transformations and combinations \\[var(aX) = a^2 var(X)\\] For independent random variables X and Y \\[var(aX + bY) = a^2var(X) + b^2var(Y)\\] 2.4 Random sample In many (most) experiments it is not feasible (or even possible) to examine the entire population. Instead we study a random sample. A random sample is a random subset of of \\(n\\) individuals from a population of size \\(N\\). A simple random sample is a random subset of \\(n\\) individuals from a population population of size \\(N,\\) where every individual has the same probability of being choosen. Notation A random sample \\(x_1,x_2,\\dots,x_n\\) from a distribution, \\(D\\), consists of \\(n\\) observations of the independent random variables \\(X_1, X_2,\\dots,X_n\\) all with the distribution \\(D\\). 2.4.1 The urn model to perform simple random sampling Let every individual in the population be represented by a ball. Tha value on each ball is the measurement we are interested in, for example height, shoe size, hair color, healthy/sick, type of cancer/no cancer, blood glucose value, etc. Draw \\(n\\) balls from the urn, without replacement, to get a random sample of size \\(n\\). 2.4.2 Sample properties Summary statistics can be computed for a sample, such as the sum, proportion, mean and variance. 2.4.2.1 Sample proportion The proportion of a population with a particular property is \\(\\pi\\). The number of individuals with the property in a simple random sample of size \\(n\\) is a random variable \\(X\\). The proportion of individuals a sample with the property is also a random variable; \\[P = \\frac{X}{n}\\] with expected value \\[E[P] = \\frac{E[X]}{n} = \\frac{n\\pi}{n} = \\pi\\] 2.4.3 Sample mean and standard deviation For a particular sample of size \\(n\\); \\(x_1, \\dots, x_n\\), the sample mean is denoted \\(m = \\bar x\\). The sample mean is calculated as; \\[m = \\bar x = \\frac{1}{n}\\displaystyle\\sum_{i=1}^n x_i\\] and the sample variance as; \\[s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x-m)^2\\] Note that the mean of \\(n\\) independent identically distributed random variables, \\(X_i\\) is itself a random variable; \\[\\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i,\\] If \\(X_i \\sim N(\\mu, \\sigma)\\) then \\(\\bar X \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\). When we only have a sample of size \\(n\\), the sample mean \\(m\\) is our best estimate of the population mean. It is possible to show that the sample mean is an unbiased estimate of the population mean, i.e. the average (over many size \\(n\\) samples) of the sample mean is \\(\\mu\\). \\[E[\\bar X] = \\frac{1}{n} n E[X] = E[X] = \\mu\\] Similarly, the sample variance is an unbiased estimate of the population variance. 2.4.4 Standard error Eventhough the sample can be used to calculate unbiased estimates of the population value, the sample estimate will not be perfect. The standard deviation of the sampling distribution (the distribution of sample estimates) is called the standard error. For the sample mean, \\(\\bar X\\), the variance is \\[E[(\\bar X - \\mu)] = var(\\bar X) = var(\\frac{1}{n}\\sum_i X_i) = \\frac{1}{n^2} \\sum_i var(X_i) = \\frac{1}{n^2} n var(X) = \\frac{\\sigma^2}{n}\\] The standard error of the mean is thus; \\[SEM = \\frac{\\sigma}{\\sqrt{n}}\\] Replacing \\(\\sigma\\) with the sample standard deviation, \\(s\\), we get an estimate of the standard deviation of the mean; \\[SEM \\approx \\frac{s}{\\sqrt{n}}\\] An alternative definition of standard error of the mean is actually \\[SEM = \\frac{s}{\\sqrt{n}}\\] Exercise: Data summary Consider the below data and summarize each of the variables. id smoker baby weight (kg) gender mother weight (kg) mother age parity married 1 yes 2.8 F 64 21 2 yes 2 yes 3.2 M 65 27 1 yes 3 yes 3.5 F 64 31 2 yes 4 yes 2.7 F 73 32 0 yes 5 yes 3.3 M 59 39 3 yes 6 no 3.7 F 61 26 0 no 7 no 3.3 F 52 27 2 no 8 no 4.3 F 59 21 0 no 9 no 3.2 M 65 28 1 no 10 no 3.0 M 73 33 4 yes "],
["parametric-continuous-distributions.html", "Chapter 3 Parametric continuous distributions 3.1 Normal distribution 3.2 Central limit theorem 3.3 \\(\\chi^2\\)-distribution 3.4 F-distribution 3.5 t-distribution", " Chapter 3 Parametric continuous distributions 3.1 Normal distribution The normal distribution (sometimes referred to as the Gaussian distribution) is a common probability distribution and many continuous random variables can be described by the normal distribution or be approximated by the normal distribution. The normal probability density function \\[f(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\] describes the distribution of a normal random variable, \\(X\\), with expected value \\(\\mu\\) and standard deviation \\(\\sigma\\). In short we write \\(X \\sim N(\\mu, \\sigma)\\). The bell-shaped normal distributions is symmetric around \\(\\mu\\) and \\(f(x) \\rightarrow 0\\) as \\(x \\rightarrow \\infty\\) and as \\(x \\rightarrow -\\infty\\). As \\(f(x)\\) is well defined, values for the cumulative distribution function \\(F(x) = \\int_{- \\infty}^x f(x) dx\\) can be computed. If \\(X\\) is normally distributed with expected value \\(\\mu\\) and standard deviation \\(\\sigma\\) we write: \\[X \\sim N(\\mu, \\sigma)\\] Using transformation rules we can define \\[Z = \\frac{X-\\mu}{\\sigma}, \\, Z \\sim N(0,1)\\] Values for the cumulative standard normal distribution, \\(F(z)\\), are tabulated (and easy to compute in R using the function pnorm). Some value of particular interest: F(1.64) = 0.95 F(1.96) = 0.975 As the normal distribution is symmetric F(-1.64) = 0.05 F(-1.96) = 0.025 P(-1.96 &lt; Z &lt; 1.96) = 0.95 3.1.1 Sum of two normal random variables If \\(X \\sim N(\\mu_1, \\sigma_1)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2)\\) are two independent normal random variables, then their sum is also a random variable: \\[X + Y \\sim N(\\mu_1 + \\mu_2, \\sqrt{\\sigma_1^2 + \\sigma_2^2})\\] and \\[X - Y \\sim N(\\mu_1 - \\mu_2, \\sqrt{\\sigma_1^2 + \\sigma_2^2})\\] This can be extended to the case with \\(n\\) independent and identically distributed random varibles \\(X_i\\) (\\(i=1 \\dots n\\)). If all \\(X_i\\) are normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), \\(X_i \\in N(\\mu, \\sigma)\\), then the sum of all \\(n\\) random variables will also be normally distributed with mean \\(n\\mu\\) and standard deviation \\(\\sqrt{n} \\sigma\\). 3.2 Central limit theorem The sum of \\(n\\) independent and equally distributed random variables is normally distributed, if \\(n\\) is large enough. As a result of central limit theorem, the distribution of fractions or mean values of a sample follow the normal distribution, at least if the sample is large enough (a rule of thumb is that the sample size \\(n&gt;30\\)). (#exm:Mean BMI) In a population of 252 men we can study the distribution of BMI. ##Population mean mu &lt;- mean(fat$BMI) mu ## [1] 25 ##Population variance sigma2 &lt;- var(fat$BMI)/nrow(fat)*(nrow(fat)-1) sigma2 ## [1] 13 ##Population standard variance sigma &lt;- sqrt(sigma2) sigma ## [1] 3.6 Randomly sample 3, 5, 10, 15, 20, 30 men and compute the mean value, \\(m\\). Repeat many times to get the distribution of mean values. Note, mean is just the sum divided by the number of samples \\(n\\). 3.3 \\(\\chi^2\\)-distribution If \\(X_i \\in N(0,1)\\) then \\(Y = \\sum_{i=1}^n X_i^2\\) is a \\(\\chi^2\\) distributed random variable, \\(Y \\in \\chi^2(n)\\) with \\(n\\) degrees of freedom (df). Example, variance is \\(\\chi^2\\) distributed. 3.4 F-distribution The ratio of two \\(\\chi^2\\)-distributed variables is F-distributed Example, the ratio of two variances is F-distributed 3.5 t-distribution The ratio of a normally distributed variable and a \\(\\chi^2\\)-distributed variable is t-distributed. Example, the ratio between mean and variance is t-distributed. "],
["statistical-inference.html", "Chapter 4 Statistical Inference 4.1 Hypothesis test 4.2 Interval estimate 4.3 Parametric distributions 4.4 Confidence interval", " Chapter 4 Statistical Inference Learning outcomes: understand the concept random sample to define null and alternative hypothesis to perform a hypothesis test using resampling to perform a t-test to understand and define sampling distribution and standard error to compute standard error of mean and proportions to compute confidence interval of mean and proportions using the normal approximation to compute confidence interval of mean using the t-distribution Statistical inference is to draw conclusions regarding properties of a population based on observations of a random sample from the population. 4.1 Hypothesis test In a hypothesis test a hypothesis is evaluated based on a random sample. The hypotheses that are tested are assumptions about properties of the population, such as proportion, mean, mean difference etc. 4.1.1 The null and alternative hypothesis There are two hypotheses involved in a hypothesis test, the null hypothesis, \\(H_0\\), and the alternative hypothesis, \\(H_1\\). The null hypothesis is in general neutral, “no change”, “no difference between groups”, “no association”. In general we want to show that \\(H_0\\) is false. The alternative hypothesis expresses what the researcher is interested in “the treatment has an effect”, “there is a difference between groups”, “there is an association”. The alternative hypothesis can also be directional “the treatment has a positive effect”. 4.1.2 To perform a hypothesis test Define null \\(H_0\\) and \\(H_1\\) Select appropriate test statistic, \\(T\\), and compute the observed value, \\(t_{obs}\\) Assume that the \\(H_0\\) is true and compute the sampling distribution of \\(T\\). Compare the observed value, \\(t_{obs}\\), with the computed sampling distribution under \\(H_0\\) and compute a p-value. The p-value is the probability of observing a value at least as extreme as the observed value, if \\(H_0\\) is true. Based on the p-value either accept or reject \\(H_0\\). Definition 4.1 Sampling distribution A sampling distribution is the distribution of a statistic of a large number of samples drawn from a specific population. Definition 4.2 p-value The p-value is the probability of the observed value, or something more extreme, if the null hypothesis is true. 4.1.3 Significance level and error types H0 is true H0 is false Accept H0 Type II error, miss Reject H0 Type I error, false alarm The significance level, \\(\\alpha\\) = P(false alarm) = P(Reject H0|H0 is true) 4.1.3.1 Simulation example Let’s assume we know that the proportion of pollen allergy in Sweden is \\(0.3\\). We suspect that the number of pollen allergic has increased in Uppsala in the last couple of years and want to investigate this. Observe 100 people from Uppsala, 42 of these were allergic to pollen. Is there a reason to believe that the proportion of pollen allergic in Uppsala \\(\\pi &gt; 0.3\\)? Null and alternative hypotheses \\(H_0:\\) The proportion of pollen allergy in Uppsala is the same as in Sweden as a whole. \\(H_1:\\) The proportion of pollen allergy in Uppsala is not the same as in Sweden as a whole. or expressed differently; \\[H_0:\\, \\pi=\\pi_0\\] \\[H_1:\\, \\pi&gt;\\pi_0\\] where \\(\\pi\\) is the unknown proportion of pollen allergy in the Uppsala population that. \\(\\pi_0 = 0.3\\) is the proportion of pollen allergy in Sweden. Test statistic Here we are interested in the proportion of pollen allergy in our Uppsala sample. An appropriate test statistic could be the number of pollen allergic in a sample of size \\(n=100\\), \\(X\\). As an alternative we can use the proportion of pollen allergic in a sample of size \\(n\\), \\[P = \\frac{X}{n}\\] Let’s use \\(P\\) as our test statistic and compute the observed value, \\(p_{obs}\\). In our sample of 100 people from Uppsala the proportion allergic to pollen is \\(p=42/100=0.42\\). Null distribution The sampling distribution of \\(P\\) under \\(H_0\\) (i.e. when the null hypothesis is true) is what we call the null distribution. Our \\(H_0\\) state that \\(\\pi=0.3\\). We can model this using an urn model as follows; Figure 4.1: An urn model of the null hypothesis \\(\\pi=0.3\\). The black balls represent allergic and the white balls non-allergic. Using this model, we can simulate taking a sample of 100 many times. ## Urn rep(c(0, 1), c(7, 3)) ## [1] 0 0 0 0 0 0 0 1 1 1 ## Sample 100 times with replacement sample(rep(c(0, 1), c(7, 3)), 100, replace=TRUE) ## [1] 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 ## [38] 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 ## [75] 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 ## Compute proportion of samples that are allergic (1) sum(sample(rep(c(0, 1), c(7, 3)), 100, replace=TRUE))/100 ## [1] 0.36 ## Draw samples of size 100 and compute proporion allergic 100000 times p &lt;- replicate(100000, mean(sample(rep(c(0, 1), c(7, 3)), 100, replace=TRUE))) Finally plot the distribution Figure 4.2: The sampling distribution. Compute p-value Compare the observed value, \\(p_{obs} = 0.42\\) to the null distribution. Figure 4.3: The sampling distribution. The observed value is marked by a red vertical line. The p-value is the probability of getting the observed value or higher, if the null hypothesis is true. Use the null distribution to calculate the p-value, \\(P(P \\geq 0.42|H_0)\\). ## How many times sum(p &gt;= 0.42) ## [1] 715 ## p-value sum(p &gt;= 0.42)/length(p) ## [1] 0.0072 p = \\(P(P \\geq 0.42|H_0)\\) = 0.00715 Accept or reject \\(H_0\\)? 4.1.3.2 Example, permutation test Do high fat diet lead to increased body weight? Study setup: Order 24 female mice from a lab. Randomly assign 12 of the 24 mice to receive high-fat diet, the remaining 12 are controls (ordinary diet). Measure body weight after one week. Null and alternative hypotheses \\[ \\begin{aligned} H_0: \\mu_2 = \\mu_1 \\iff \\mu_2 - \\mu_1 = 0\\\\ H_1: \\mu_2&gt;\\mu_1 \\iff \\mu_2-\\mu_1 &gt; 0 \\end{aligned} \\] where \\(\\mu_2\\) is the (unknown) mean body weight of the high-fat mouse population and \\(\\mu_1\\) is the mean body-weight of the control mouse population. Studied population: Female mice that can be ordered from a lab. Test statistic Here we are interested in the mean difference between high-fat and control mice. Mean weight of 12 (randomly selected) mice on ordinary diet, \\(\\bar X_1\\). \\(E[\\bar X_1] = E[X_1] = \\mu_1\\) Mean weight of 12 (randomly selected) mice on high-fat diet, \\(\\bar X_2\\). \\(E[\\bar X_2] = E[X_2] = \\mu_2\\) The mean difference is also a random variable: \\(D = \\bar X_2 - \\bar X_1\\) The observed values are summarized below; high-fat 25 30 23 18 31 24 39 26 36 29 24 32 ordinary 27 25 22 23 25 37 24 26 22 26 30 24 Mean weight of control mice (ordinary diet): \\(\\bar x_1 = 25.73\\) Mean weight of mice on high-fat diet: \\(\\bar x_2 = 28.21\\) Difference in mean weights: \\(d_{obs} = \\bar x_2 - \\bar x_1 = 2.49\\) Null distribution If high-fat diet has no effect, i.e. if \\(H_0\\) was true, the result would be as if all mice were given the same diet. What can we expect if all mice are fed with the same type of food? This can be accomplished using permutation The 24 mice were initially from the same population, depending on how the mice are randomly assigned to high-fat and normal group, the mean weights would differ, even if the two groups were treated the same. Assume \\(H_0\\) is true, i.e. assume all mice are equivalent and Randomly reassign 12 of the 24 mice to ‘high-fat’ and the remaining 12 to ‘control’. Compute difference in mean weights If we repeat 1-2 many times we get the sampling distribution when \\(H_0\\) is true, the so called null distribution, of difference in mean weights. Compute p-value What is the probability to get an at least as extreme mean difference as our observed value, \\(d_{obs}\\), if \\(H_0\\) was true? \\[P(\\bar X_2 - \\bar X_2 \\geq d_{obs} | H_0) = \\]0.126 Conclusion? 4.2 Interval estimate How large proportion of the Uppsala population is allergic to pollen? Investigate this by randomly selecting 100 persons. It is important to actually sample randomly, ideally every individual should have the same probability of being sampled. Observe that 42 of the 100 has a pollen allergy. Hence, the observed sample proportion is \\(p=0.42\\). What does this say about the population proportion \\(\\pi\\)? Our best guess would be \\(\\pi \\approx p = 0.42\\), but how accurate is this? If the experiment is repeated, we get a slightly different result. 4.2.1 Bootstrap interval Using bootstrap we can sample with replacement from our sample to estimate the uncertainty. Put the entire sample in an urn! Figure 4.4: An urn model with 42 allergy (black) and 58 non-allergy (white). The black balls represent allergic and the white balls non-allergic. Sample from the urn with replacement to compute the bootstrap distribution. Using the bootstrap distribution the uncertainty of our estimate of \\(\\pi\\) can be estimated. The 95% bootstrap interval is [0.24, 0.42]. The bootstrap is very useful if you do not know the distribution of our sampled propery. But in our example we actually do. 4.3 Parametric distributions In previous chapters we have computed the sampling distribution using resampling techniques to be able to perform hypothesis tests or compute interval estimates. If the sampling distribution was already known (or could be computed based on a few assumptions) resampling would not be necessary. Let’s get back to the pollen example! Assume that the proportion of pollen allergy in Sweden is known to be \\(0.3\\). Observe 100 people from Uppsala, 42 of these were allergic to pollen. Is there a reason to believe that the proportion of pollen allergic in Uppsala \\(\\pi &gt; 0.3\\)? The number of allergic individuals in a sample of size \\(n\\) is \\(X\\) and the proportion of allergic persons is \\(P = X/n\\). \\(X\\) is binomially distributed, but here we can use the Central limit theorem The sum of \\(n\\) independent and equally distributed random variables is normally distributed, if \\(n\\) is large enough. As a result of the central limit theorem, the distribution of number or proportion of allergic individuals in a sample of size \\(n\\) is approximately normal. At least if the sample is large enough. A rule of thumb is that the sample size should be \\(n&gt;30\\). Here, the sample size is 100! The normal distribution has two parameters, mean and standard deviation. The mean of the sampling distribution under \\(H_0\\) is \\(\\pi\\). The standard deviation of the sampling distribution, the so called standard error, is for proportions \\[SE=\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\] Hence, \\[P \\sim N\\left(\\pi, \\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\right)\\] The formula for the standard error contains our unknown parameter \\(\\pi\\). No problem, let’s replace \\(\\pi\\) with our best estimate of \\(\\pi\\): \\(p\\)! \\[ SE = \\sqrt{\\frac{p(1-p)}{n}} \\] 4.3.1 One sample, proportions If \\(H_0\\) is true \\(\\pi=\\pi_0\\) and \\[P \\sim N\\left(\\pi_0, \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\right)\\] Test statistic \\[Z = \\frac{P-\\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}}\\] Replace \\(P\\) with our observed value \\(p=0.42\\) and compute our observed \\[Z_{obs} = \\frac{0.42-0.3}{\\sqrt{\\frac{0.3(1-0.3)}{100}}} = 2.62\\] The p-value is the probability of the observed value, or something more extreme, if the null hypothesis is true. \\[p = P(P&gt;\\pi_0) = P(Z&gt;Z_{obs}) = P(Z&gt;2.62) = [table]\\] Conclusion? 4.3.2 One sample, mean If \\[X \\sim N(\\mu, \\sigma)\\] then \\[\\bar X \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\] If \\(\\sigma\\) is known \\[Z = \\frac{\\bar X - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)\\] For small \\(n\\) and unknown \\(\\sigma\\) \\[t = \\frac{\\bar X - \\mu}{\\frac{s}{\\sqrt{n}}},\\] where \\(t\\) is t-distributed with \\(df=n-1\\) degrees of freedom. The hemoglobin value (Hb) in women is on average 140 g/L. You observe the following Hb values in a set of five men: 154, 140, 147, 162, 172. Assume that Hb is normally distributed. Is there a reason to believe that the mean Hb value in men differ from that in women? 4.3.3 Two samples, known \\(\\sigma_1\\) and \\(\\sigma_2\\) \\[H_0: \\mu_2 = \\mu_1\\] \\[H_1: \\mu_2&gt;\\mu_1\\] Let’s assume that both mouse body weights in control and treatment groups are independent and normally distributed, with unknown mean, but known standard deviations, \\(\\sigma_1=3.4\\) and \\(\\sigma_2=5.1\\). \\[ \\begin{aligned} X_1 \\sim N(\\mu_1, \\sigma_1) \\\\ X_2 \\sim N(\\mu_2, \\sigma_2) \\end{aligned} \\] If follows that the sample means are \\[ \\begin{aligned} \\bar X_1 \\sim N(\\mu_1, \\sigma_1/\\sqrt{n_1}) \\\\ \\bar X_2 \\sim N(\\mu_2, \\sigma_2/\\sqrt{n_2}) \\end{aligned} \\] The mean difference \\(D = \\bar X_2 - \\bar X_1\\) is thus also normally distributed: \\[D = \\bar X_2 - \\bar X_1 = N\\left(\\mu_2-\\mu_1, \\sqrt{\\frac{\\sigma_2^2}{n_2} + \\frac{\\sigma_1^2}{n_1}}\\right)\\] If \\(H_0\\) is true: \\[D = \\bar X_2 - \\bar X_1 = N\\left(0, \\sqrt{\\frac{\\sigma_2^2}{n_2} + \\frac{\\sigma_1^2}{n_1}}\\right)\\] The test statistic: \\[Z = \\frac{\\bar X_2 - \\bar X_1 - 0}{\\sqrt{\\frac{\\sigma_2^2}{n_2} + \\frac{\\sigma_1^2}{n_1}}}\\] is standard normal, i.e. \\(Z \\sim N(0,1)\\). 4.3.4 Unknown standard deviations What if the population standard deviations are not known? If the sample sizes are large, we can replace the known standard deviations with our sample standard deviations and according to the central limit theorem assume that and procede as before. Here \\(n_1=n_2=12\\) which is not very large. For small sample sizes we can use Student’s t-test, which requires us to assume that \\(X_1\\) and \\(X_2\\) both are normally distributed and have equal variances. With these assumptions we can compute the pooled variance \\[ s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2} \\] and the test statistic \\[t = \\frac{\\bar X_1 - \\bar X_2 - 0}{\\sqrt{s_p^2(\\frac{1}{n_1} + \\frac{1}{n_2})}}\\] \\(t\\) is t-distributed with \\(n_1+n_2-2\\) degrees of freedom. The t-test is implemented in R. # Student&#39;s t-test with pooled variances t.test(xHF, xN, var.equal=TRUE, alternative=&quot;greater&quot;) ## ## Two Sample t-test ## ## data: xHF and xN ## t = 1, df = 22, p-value = 0.1 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -1.1 Inf ## sample estimates: ## mean of x mean of y ## 28 26 # Unequal variances with Welch approximation to the degrees of freedom (the default) t.test(xHF, xN, var.equal=FALSE, alternative=&quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: xHF and xN ## t = 1, df = 20, p-value = 0.1 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -1.1 Inf ## sample estimates: ## mean of x mean of y ## 28 26 4.4 Confidence interval Based on what we know of the normal distribution, we can conclude that \\[Pr\\left(-1.96 &lt; \\frac{P-\\pi}{SE}&lt;1.96\\right) = 0.95\\] We can rewrite this to \\[Pr\\left(\\pi-1.96 SE &lt; P &lt; \\pi + 1.96 SE\\right) = 0.95\\] in other words sample fraction \\(p\\) will fall between \\(\\pi \\pm 1.96 SE\\) with 95% probability. The equation can also be rewritten to \\[Pr\\left(P-1.96 SE &lt; \\pi &lt; P + 1.96 SE\\right) = 0.95\\] The observed confidence interval is what we get when we replace the random variable \\(P\\) with our observed fraction, i.e. our 95% confidence interval is \\[p-1.96\\sqrt{\\frac{p(1-p)}{n}}) &lt; \\pi &lt; p + 1.96\\sqrt{\\frac{p(1-p)}{n}}\\] \\[\\pi = p \\pm 1.96 \\sqrt{\\frac{p(1-p)}{n}}\\] A 95% confidence interval will have 95% chance to cover the true value. Pollen allergy, CI Back to our example. \\(p=0.33\\) and \\(SE=\\sqrt{\\frac{p(1-p)}{n}} = 0.05\\). Hence, the 95% confidence interval is \\[\\pi = 0.33 \\pm 1.96 * 0.05 = 0.33 \\pm 0.092\\] or \\[(0.33-0.092, 0.33+0.092) = (0.24, 0.42)\\] – How can we get a narrower confidence interval? Here we computed a 95% interval, what if we want a 90% confidence interval? or a 99% confidence interval? 4.4.0.0.1 Confidence interval of sample mean \\[ \\begin{aligned} P(-1.96 \\leq \\mathbf Z \\leq 1.96) = 0.95 \\iff \\\\ P(-1.96 \\leq \\frac{\\mathbf{\\bar X} - \\mu}{\\sigma/\\sqrt{n}} \\leq 1.96) = 0.95 \\iff \\\\ P(\\mathbf{\\bar X} - 1.96\\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\mathbf{\\bar X} + 1.96\\frac{\\sigma}{\\sqrt{n}}) = 0.95 \\end{aligned} \\] "],
["multiple-testing.html", "Chapter 5 Multiple testing 5.1 Error types 5.2 Bonferroni correction 5.3 Benjamini-Hochbegs FDR 5.4 ‘Adjusted’ p-values 5.5 Example, 10000 independent tests (e.g. genes)", " Chapter 5 Multiple testing 5.1 Error types H0 is true H0 is false Accept H0 Type II error, miss Reject H0 Type I error, false alarm H0 is true H0 is false Accept H0 TN FN Reject H0 FP TP Significance level \\[P(\\mbox{reject }\\,H_0 | H_0 \\,\\mbox{is true}) = P(\\mbox{type I error}) = \\alpha\\] Statistical power \\[P(\\mbox{reject } H_0 | H_1 \\mbox{ is true}) = P(\\mbox{reject } H_0 | H_0 \\mbox{ is false}) = 1 - P(\\mbox{type II error})\\] ### Perform one test: P(One type I error) = \\(\\alpha\\) P(No type I error) = \\(1 - \\alpha\\) 5.1 Perform \\(m\\) independent tests: P(No type I errors in \\(m\\) tests) = \\((1 - \\alpha)^m\\) P(At least one type I error in \\(m\\) tests) = \\(1 - (1 - \\alpha)^m\\) FWER: family-wise error rate, probability of one or more false positive, e.g. Bonferroni, Holm FDR: false discovery rate, proportion of false positives among hits, e.g. Benjamini-Hochberg, Storey 5.2 Bonferroni correction To achieve a family-wise error rate of \\(\\leq \\alpha\\) when performing \\(m\\) tests, declare significance and reject the null hypothesis for any test with \\(p \\leq \\alpha/m\\). Objections: too conservative 5.3 Benjamini-Hochbegs FDR H0 is true H0 is false Accept H0 TN FN Reject H0 FP TP The false discovery rate is the proportion of false positives among ‘hits’, i.e. \\(\\frac{FP}{TP+FP}\\). Benjamini-Hochberg’s method control the FDR level, \\(\\gamma\\), when performing \\(m\\) independent tests, as follows: Sort the p-values \\(p_1 \\leq p_2 \\leq \\dots \\leq p_m\\). Find the maximum \\(j\\) such that \\(p_j \\leq \\gamma \\frac{j}{m}\\). Declare significance for all tests \\(1, 2, \\dots, j\\). 5.4 ‘Adjusted’ p-values Sometimes an adjusted significance threshold is not reported, but instead ‘adjusted’ p-values are reported. Using Bonferroni’s method the adjusted p-values are: \\(\\tilde p_i = \\min(m p_i, 1)\\). A feature’s adjusted p-value represents the smallest FWER at which the null hypothesis will be rejected, i.e. the feature will be deemed significant. Benjamini-Hochberg’s ‘adjusted’ p-values are called \\(q\\)-values: \\(q_i = \\min(\\frac{m}{i} p_i, 1)\\) A feature’s \\(q\\)-value can be interpreted as the lowest FDR at which the corresponding null hypothesis will be rejected, i.e. the feature will be deemed significant. 5.5 Example, 10000 independent tests (e.g. genes) p-value adj p (Bonferroni) q-value (B-H) 1.7e-08 0.00 0.00 5.8e-08 0.00 0.00 3.4e-07 0.00 0.00 9.1e-07 0.01 0.00 1e-06 0.01 0.00 2.4e-06 0.02 0.00 2.3e-05 0.23 0.03 3.6e-05 0.36 0.04 0.00022 1.00 0.23 0.00023 1.00 0.23 0.00073 1.00 0.66 0.0032 1.00 1.00 0.0045 1.00 1.00 0.0087 1.00 1.00 0.0089 1.00 1.00 0.012 1.00 1.00 0.014 1.00 1.00 0.045 1.00 1.00 0.08 1.00 1.00 0.23 1.00 1.00 "]
]
