# Statistical Inference

Learning outcomes:

- understand the concept random sample
- to define null and alternative hypothesis
- to perform a hypothesis test using resampling
- to perform a t-test
- to understand and define sampling distribution and standard error
- to compute standard error of mean and proportions
- to compute confidence interval of mean and proportions using the normal approximation
- to compute confidence interval of mean using the t-distribution



Statistical inference is to draw conclusions regarding properties of a population based on observations of a random sample from the population.

## Hypothesis test
 
In a hypothesis test a hypothesis is evaluated based on a random sample.

The hypotheses that are tested are assumptions about properties of the population, such as proportion, mean, mean difference etc.


### The null and alternative hypothesis

There are two hypotheses involved in a hypothesis test, the null hypothesis, $H_0$, and the alternative hypothesis, $H_1$.

The null hypothesis is in general neutral, "no change", "no difference between groups", "no association". In general we want to show that $H_0$ is false.

The alternative hypothesis expresses what the researcher is interested in "the treatment has an effect", "there is a difference between groups", "there is an association". The alternative hypothesis can also be directional "the treatment has a positive effect".

### To perform a hypothesis test

1. Define null $H_0$ and $H_1$
2. Select appropriate test statistic, $T$, and compute the observed value, $t_{obs}$
3. Assume that the $H_0$ is true and compute the sampling distribution of $T$.
4. Compare the observed value, $t_{obs}$, with the computed sampling distribution under $H_0$ and compute a p-value. The p-value is the probability of observing a value at least as extreme as the observed value, if $H_0$ is true.
5. Based on the p-value either accept or reject $H_0$.

```{definition, "samplingdistribution", echo=TRUE}
**Sampling distribution**

A sampling distribution is the distribution of a statistic of a large number of samples drawn from a specific population.
```

```{definition, echo=TRUE}
**p-value**
  
The p-value is the probability of the observed value, or something more extreme, if the null hypothesis is true.
```

### Significance level and error types

```{r}
kable(matrix(c("", "H0 is true", "H0 is false", "Accept H0", "", "Type II error, miss", "Reject H0", "Type I error, false alarm", ""), byrow=3, ncol=3))
```

The significance level, $\alpha$ = P(false alarm) = P(Reject H0|H0 is true)



#### Simulation example

Let's assume we know that the proportion of pollen allergy in Sweden is $0.3$. We suspect that the number of pollen allergic has increased in Uppsala in the last couple of years and want to investigate this.

Observe 100 people from Uppsala, 42 of these were allergic to pollen. Is there a reason to believe that the proportion of pollen allergic in Uppsala $\pi > 0.3$?

##### Null and alternative hypotheses {-}

$H_0:$ The proportion of pollen allergy in Uppsala is the same as in Sweden as a whole.

$H_1:$ The proportion of pollen allergy in Uppsala is not the same as in Sweden as a whole.

or expressed differently;

$$H_0:\, \pi=\pi_0$$

$$H_1:\, \pi>\pi_0$$
where $\pi$ is the unknown proportion of pollen allergy in the Uppsala population that. $\pi_0 = 0.3$ is the proportion of pollen allergy in Sweden.

##### Test statistic {-}

Here we are interested in the proportion of pollen allergy in our Uppsala sample. An appropriate test statistic could be the number of pollen allergic in a sample of size $n=100$, $X$. As an alternative we can use the proportion of pollen allergic in a sample of size $n$, 

$$P = \frac{X}{n}$$

Let's use $P$ as our test statistic and compute the observed value, $p_{obs}$. In our sample of 100 people from Uppsala the proportion allergic to pollen is $p=42/100=0.42$.

##### Null distribution {-}

The sampling distribution of $P$ under $H_0$ (i.e. when the null hypothesis is true) is what we call the null distribution.

Our $H_0$ state that $\pi=0.3$. We can model this using an urn model as follows;

```{r pollenurn, echo=FALSE, fig.cap="An urn model of the null hypothesis $\\pi=0.3$. The black balls represent allergic and the white balls non-allergic.", out.width = "20%", fig.align="center"}
knitr::include_graphics("figures/pollenurn.png")
```

Using this model, we can simulate taking a sample of 100 many times.

```{r, echo=TRUE}
## Urn
rep(c(0, 1), c(7, 3))
## Sample 100 times with replacement
sample(rep(c(0, 1), c(7, 3)), 100, replace=TRUE)
## Compute proportion of samples that are allergic (1)
sum(sample(rep(c(0, 1), c(7, 3)), 100, replace=TRUE))/100
## Draw samples of size 100 and compute proporion allergic 100000 times
p <- replicate(100000, mean(sample(rep(c(0, 1), c(7, 3)), 100, replace=TRUE)))
```

Finally plot the distribution

```{r pollensampledistr, out.width="45%", fig.align="center", fig.cap="The sampling distribution."}
#set.seed(13)
#p <- replicate(100000, mean(sample(rep(c(0, 1), c(7, 3)), 100, replace=TRUE)))
ggplot(data.frame(p=p), aes(x=p)) + geom_histogram(color="white", binwidth=0.02) + theme_bw()
```   

##### Compute p-value {-}

Compare the observed value, $p_{obs} = 0.42$ to the null distribution.

```{r, out.width="50%", fig.align="center", fig.cap="The sampling distribution. The observed value is marked by a red vertical line."}
ggplot(data.frame(p=p), aes(x=p)) + geom_histogram(color="white", binwidth=0.02) + geom_vline(xintercept=0.42, color="red") + theme_bw()
```

The p-value is the probability of getting the observed value or higher, if the null hypothesis is true.

Use the null distribution to calculate the p-value, $P(P \geq 0.42|H_0)$.

```{r, echo=TRUE}
## How many times 
sum(p >= 0.42)
## p-value
sum(p >= 0.42)/length(p)
```

p = $P(P \geq 0.42|H_0)$ = `r format(mean(p>=0.42), digits=4)` 


##### Accept or reject $H_0$? {-}

#### Example, permutation test

##### Do high fat diet lead to increased body weight? {-}

Study setup:

1. Order 24 female mice from a lab.
2. Randomly assign 12 of the 24 mice to receive high-fat diet, the
  remaining 12 are controls (ordinary diet).
3. Measure body weight after one week.

##### Null and alternative hypotheses {-}
$$
\begin{aligned}
H_0: \mu_2 = \mu_1 \iff \mu_2 - \mu_1 = 0\\
H_1: \mu_2>\mu_1 \iff \mu_2-\mu_1 > 0
\end{aligned}
$$

where $\mu_2$ is the (unknown) mean body weight of the high-fat mouse population and $\mu_1$ is the mean body-weight of the control mouse population.

Studied population: Female mice that can be ordered from a lab.

```{r mice, echo=FALSE}
## Download the mouse population
mp <- read.csv("https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv")
## Select all female mice
pop.F <- mp %>% filter(Sex=="F")
## Select all the female mice on high fat diet
pop.F.hf <- (pop.F %>% filter(Diet=="hf"))[, "Bodyweight"]
## Select all the female mice on ordinary diet
pop.F.n <- (pop.F %>% filter(Diet=="chow"))[, "Bodyweight"]
```

##### Test statistic {-}

Here we are interested in the mean difference between high-fat and control mice.

Mean weight of 12 (randomly selected) mice on ordinary diet, $\bar X_1$. $E[\bar X_1] = E[X_1] = \mu_1$
  
Mean weight of 12 (randomly selected) mice on high-fat diet, $\bar X_2$. $E[\bar X_2] = E[X_2] = \mu_2$

The mean difference is also a random variable: $D = \bar X_2 - \bar X_1$

The observed values are summarized below;

```{r miceobs, echo=F}
## Select the seed so that we all get the same random mice!
set.seed(1)
## Select 12 HF mice
xHF <- sample(pop.F.hf, 12)
## Select 12 O mice
xN <- sample(pop.F.n, 12)
```

```{r}
kable(rbind("high-fat"=xHF, "ordinary"=xN), digits=1) %>% kable_styling(font_size=14)
```

```{r }
##Compute mean body weights of the two samples
mHF <- mean(xHF)
mN <- mean(xN)
## Compute mean difference
dobs <- mHF - mN
```

Mean weight of control mice (ordinary diet): $\bar x_1 = `r sprintf("%.2f", mN)`$

Mean weight of mice on high-fat diet: $\bar x_2 = `r sprintf("%.2f", mHF)`$

Difference in mean weights: $d_{obs} = \bar x_2 - \bar x_1 = `r dobs`$

##### Null distribution {-}

If high-fat diet has no effect, i.e. if $H_0$ was true, the result would be as if all mice were given the same diet. What can we expect if all mice are fed with the same type of food? 

This can be accomplished using permutation

The 24 mice were initially from the same population, depending on how the mice are randomly assigned to high-fat and normal group, the mean weights would differ, even if the two groups were treated the same.

Assume $H_0$ is true, i.e. assume all mice are equivalent and
  
1. Randomly reassign 12 of the 24 mice to 'high-fat' and the remaining 12 to 'control'.
2. Compute difference in mean weights

If we repeat 1-2 many times we get the sampling distribution when $H_0$ is true, the so called null distribution, of difference in mean weights.


```{r permtest, echo=F, out.width="50%"}
## All 24 body weights in a vector
x <- c(xHF, xN)
## Mean difference
dobs <- mean(x[1:12]) - mean(x[13:24])
#dobs
## Permute
y <- sample(x)
##Mean difference
#mean(y[1:12]) - mean(y[13:24])
dnull.perm <- replicate(n = 100000, {
  y <- sample(x)
  ##Mean difference
  mean(y[1:12]) - mean(y[13:24])
})
ggplot(data.frame(d=dnull.perm), aes(x=d)) + geom_histogram(bins=25, color="white") + theme_bw() + geom_vline(xintercept=dobs, color="red")
## Compute the p-value
#mean(dnull.perm>dobs)
```

##### Compute p-value {-}

What is the probability to get an at least as extreme mean difference as our observed value, $d_{obs}$, if $H_0$ was true?

$$P(\bar X_2 - \bar X_2 \geq d_{obs} | H_0) = $$`r sprintf("%.3g",mean(dnull.perm>=dobs))`

Conclusion?







## Interval estimate

- How large proportion of the Uppsala population is allergic to pollen?
- Investigate this by randomly selecting 100 persons. It is important to actually sample randomly, ideally every individual should have the same probability of being sampled.
- Observe that 42 of the 100 has a pollen allergy. Hence, the observed sample proportion is $p=0.42$.
- What does this say about the population proportion $\pi$?
- Our best guess would be $\pi \approx p = 0.42$, but how accurate is this?
- If the experiment is repeated, we get a slightly different result.

### Bootstrap interval
  
Using bootstrap we can sample with replacement from our sample to estimate the uncertainty.
  
Put the entire sample in an urn!

```{r pollenurn42, echo=FALSE, fig.cap="An urn model with 42 allergy (black) and 58 non-allergy (white). The black balls represent allergic and the white balls non-allergic.", out.width = "20%", fig.align="center"}
knitr::include_graphics("figures/pollenurn42.png")
```

Sample from the urn with replacement to compute the bootstrap distribution.

```{r CIboot, out.width="45%", fig.align="center"}
x <- rep(0:1, c(67,33))
pboot <- replicate(100000, mean(sample(x, replace=TRUE)))
ciboot <- quantile(pboot, c(0.025, 0.975))
ggplot(data.frame(x=pboot), aes(x=x, fill=x>ciboot[1] & x<ciboot[2])) + geom_histogram(color="white", binwidth=0.02) + theme_bw() + theme(legend.position="none") + xlab("p") + geom_line(data=data.frame(x=ciboot, y=5000), aes(x=x, y=y), arrow=arrow(ends="both")) + annotate("label", x=mean(ciboot), y=5000, label="95%")
```   
  
Using the bootstrap distribution the uncertainty of our estimate of $\pi$ can be estimated.

The 95% bootstrap interval is [`r ciboot`].

The bootstrap is very useful if you do not know the distribution of our sampled propery. But in our example we actually do.


<!-- ## Parametric hypothesis tests -->

## Parametric distributions

In previous chapters we have computed the sampling distribution using resampling techniques to be able to perform hypothesis tests or compute interval estimates. If the sampling distribution was already known (or could be computed based on a few assumptions) resampling would not be necessary.

Let's get back to the pollen example!

Assume that the proportion of pollen allergy in Sweden is known to be $0.3$. Observe 100 people from Uppsala, 42 of these were allergic to pollen. Is there a reason to believe that the proportion of pollen allergic in Uppsala $\pi > 0.3$?

The number of allergic individuals in a sample of size $n$ is $X$ and the proportion of allergic persons is $P = X/n$. $X$ is binomially distributed, but here we can use the Central limit theorem

  The sum of $n$ independent and equally distributed random variables
  is normally distributed, if $n$ is large enough.

As a result of the central limit theorem, the distribution of number or proportion of allergic individuals in a sample of size $n$ is approximately normal. At least if the sample is large enough. A rule of thumb is that the sample size should be $n>30$.

Here, the sample size is 100!

The normal distribution has two parameters, mean and standard deviation.

The mean of the sampling distribution under $H_0$ is $\pi$. The standard deviation of the sampling distribution, the so called standard error, is for proportions

$$SE=\sqrt{\frac{\pi(1-\pi)}{n}}$$
Hence,

$$P \sim N\left(\pi, \sqrt{\frac{\pi(1-\pi)}{n}}\right)$$


The formula for the standard error contains our unknown parameter $\pi$. No problem, let's replace $\pi$ with our best estimate of $\pi$: $p$!

$$
SE = \sqrt{\frac{p(1-p)}{n}}
$$



### One sample, proportions
If $H_0$ is true $\pi=\pi_0$ and

$$P \sim N\left(\pi_0, \sqrt{\frac{\pi_0(1-\pi_0)}{n}}\right)$$
Test statistic $$Z = \frac{P-\pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}$$

Replace $P$ with our observed value $p=0.42$ and compute our observed
$$Z_{obs} = \frac{0.42-0.3}{\sqrt{\frac{0.3(1-0.3)}{100}}} = `r zobs <- (.42-.3)/sqrt(.3*.7/100);format(zobs, digits=3)`$$

The p-value is the probability of the observed value, or something more extreme, if the null hypothesis is true.

$$p = P(P>\pi_0) = P(Z>Z_{obs}) = P(Z>`r format(zobs, digits=3)`) = [table]$$

Conclusion?




### One sample, mean


If $$X \sim N(\mu, \sigma)$$ then $$\bar X \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right)$$


If $\sigma$ is known

$$Z = \frac{\bar X - \mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1)$$

For small $n$ and unknown $\sigma$

$$t = \frac{\bar X - \mu}{\frac{s}{\sqrt{n}}},$$
where $t$ is t-distributed with $df=n-1$ degrees of freedom.


The hemoglobin value (Hb) in women is  on average 140 g/L. You observe the following Hb values in a set of five men: 154, 140, 147, 162, 172. Assume that Hb is normally distributed. Is there a reason to believe that the mean Hb value in men differ from that in women? 


### Two samples, known $\sigma_1$ and $\sigma_2$

$$H_0: \mu_2 = \mu_1$$
$$H_1: \mu_2>\mu_1$$

Let's assume that both mouse body weights in control and treatment groups are independent and normally distributed, with unknown mean, but known standard deviations, $\sigma_1=3.4$ and $\sigma_2=5.1$.

$$
\begin{aligned}
X_1 \sim N(\mu_1, \sigma_1) \\
X_2 \sim N(\mu_2, \sigma_2)
\end{aligned}
$$

If follows that the sample means are 

$$
\begin{aligned}
\bar X_1 \sim N(\mu_1, \sigma_1/\sqrt{n_1}) \\
\bar X_2 \sim N(\mu_2, \sigma_2/\sqrt{n_2})
\end{aligned}
$$

The mean difference $D = \bar X_2 - \bar X_1$ is thus also normally distributed:

$$D = \bar X_2 - \bar X_1 = N\left(\mu_2-\mu_1, \sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}\right)$$


If $H_0$ is true: $$D = \bar X_2 - \bar X_1 = N\left(0, \sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}\right)$$

The test statistic: $$Z = \frac{\bar X_2 - \bar X_1 - 0}{\sqrt{\frac{\sigma_2^2}{n_2} + \frac{\sigma_1^2}{n_1}}}$$ is standard normal, i.e. $Z \sim N(0,1)$.

```{r echo=F, eval=FALSE}
## Our observed value
dobs
## The p-value
1-pnorm(dobs, mean=0, sd=sqrt(5.1^2/12 + 3.4^2/12))
1-pnorm((dobs-0)/sqrt(5.1^2/12 + 3.4^2/12))
pnorm(dobs, mean=0, sd=sqrt(5.1^2/12 + 3.4^2/12), lower.tail=FALSE)
pnorm((dobs-0)/sqrt(5.1^2/12 + 3.4^2/12), lower.tail=FALSE)
```
### Unknown standard deviations

What if the population standard deviations are not known?


If the sample sizes are large, we can replace the known standard deviations with our sample standard deviations and according to the central limit theorem assume that 

<!-- $$Z = \frac{\bar X_2 - \bar X_1}{\sqrt{s_2^2/\sqrt{n_2} + s_1^2/\sqrt{n_1}}} \sim N(0,1)$$ -->
and procede as before.



Here $n_1=n_2=12$ which is not very large.

For small sample sizes we can use Student's t-test, which requires us to assume that $X_1$ and $X_2$ both are normally distributed and have equal variances. With these assumptions we can compute the pooled variance

$$
s_p^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}
$$

and the test statistic

$$t = \frac{\bar X_1 - \bar X_2 - 0}{\sqrt{s_p^2(\frac{1}{n_1} + \frac{1}{n_2})}}$$

$t$ is t-distributed with $n_1+n_2-2$ degrees of freedom.

The t-test is implemented in R.

```{r ttesteqvar, echo=TRUE}
# Student's t-test with pooled variances
t.test(xHF, xN, var.equal=TRUE, alternative="greater")
```

```{r ttestuneqvar, echo=TRUE}
# Unequal variances with Welch approximation to the degrees of freedom (the default)
t.test(xHF, xN, var.equal=FALSE, alternative="greater")
```


## Confidence interval

Based on what we know of the normal distribution, we can conclude that 
 
$$Pr\left(-1.96 < \frac{P-\pi}{SE}<1.96\right) = 0.95$$
We can rewrite this to
$$Pr\left(\pi-1.96 SE < P < \pi + 1.96 SE\right) = 0.95$$
in other words sample fraction $p$ will fall between $\pi \pm 1.96 SE$ with 95% probability.



The equation can also be rewritten to 
$$Pr\left(P-1.96 SE < \pi < P + 1.96 SE\right) = 0.95$$
The observed confidence interval is what we get when we replace the random variable $P$ with our observed fraction, i.e. our 95% confidence interval is

$$p-1.96\sqrt{\frac{p(1-p)}{n}}) < \pi < p + 1.96\sqrt{\frac{p(1-p)}{n}}$$
$$\pi = p \pm 1.96 \sqrt{\frac{p(1-p)}{n}}$$

A 95% confidence interval will have 95% chance to cover the true value.

```{r CI, fig.width=7, fig.height=3}
ggplot(data.frame(x=1:40, p=p[61:100]) %>% mutate(ymin=p-1.96*sqrt((p*(1-p))/100), ymax=p+1.96*sqrt((p*(1-p))/100)), aes(x=x, ymin=ymin, ymax=ymax, color=0.3>ymin & 0.3<ymax)) + geom_errorbar() + geom_hline(yintercept=0.3) + xlab("") + ylab("p") + theme_bw() + theme(legend.position="none")
```

##### Pollen allergy, CI {-}

Back to our example. $p=0.33$ and $SE=\sqrt{\frac{p(1-p)}{n}} = `r sqrt(0.33*(1-0.33)/100)`$.

Hence, the 95% confidence interval is 
$$\pi = 0.33 \pm 1.96 * 0.05 = 0.33 \pm 0.092$$
or
$$(0.33-0.092, 0.33+0.092) = (0.24, 0.42)$$
--

- How can we get a narrower confidence interval?
- Here we computed a 95% interval, what if we want a 90% confidence interval?
- or a 99% confidence interval?




##### Confidence interval of sample mean


$$
\begin{aligned}
P(-1.96 \leq \mathbf Z \leq 1.96) = 0.95 \iff \\
P(-1.96 \leq \frac{\mathbf{\bar X} - \mu}{\sigma/\sqrt{n}} \leq 1.96) = 0.95 \iff \\
P(\mathbf{\bar X} - 1.96\frac{\sigma}{\sqrt{n}} \leq \mu \leq \mathbf{\bar X} + 1.96\frac{\sigma}{\sqrt{n}}) = 0.95
\end{aligned}
$$
  
  
