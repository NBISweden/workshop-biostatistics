```{r, include=FALSE}
require(tidyverse)
require(ggplot2)
require(reshape2)
require(knitr)
require(kableExtra)
knitr::opts_chunk$set(fig.width=3.5, fig.height=3.5, echo = FALSE, cache=TRUE, error=FALSE, warnings=FALSE, dpi=600)
options(digits=2)
```


# Probability theory

Learning outcomes

- understand the concept of random variables
- understand the concept of probability
- understand and learn to use resampling to compute probabilities
- understand the concept probability mass function
- understand the concept probability density function
- understand the concept cumulative distribution functions
- use normal distribution
- understand the central limit theorem

## Introduction to probability

### Random variables

The outcome of a random experiment can be described by a *random variable*.

Whenever chance is involved in the outcome of an experiment the outcome is a random variable.

A random variable can not be predicted exactly, but the probability of all possible outcomes can be described.

A random variable is usually denoted by a capital letter, $X, Y, Z, \dots$. Values collected in an experiment are *observations* of the random variable, usually denoted by lowercase letters $x, y, z, \dots$.

The *population* is the collection of all possible observations of the random variable. Note, the population is not always countable.

A *sample* is a subset of the population.

Example random variables and probabilites:

- The weight of a random newborn baby, $W$. $P(W>4.0kg)$
- The smoking status of a random mother, $S$. $P(S=1)$
- The hemoglobin concentration in blood, $Hb$. $P(Hb<125 g/L)$
- The number of mutations in a gene
- BMI of a random man
- Weight status of a random man (underweight, normal weight, overweight, obese)
- The result of throwing a die


Conditional probability can be written for example $P(W \geq 3.5 | S = 1)$, which is the probability that $X \geq 3.5$ if $S = 1$, in words "the probability that a smoking mother has a baby with birth weight of 3.5 kg or more".

### The urn model

The urn model is a simple model commonly used in statistics and probability. In the urn model real objects (such as people, mice, cells, genes, molecules, etc) are represented by balls of different colors. A fair coin can be represented by an urn with two balls representing the coins two sides. A group of people can be modelled in an urn model, if age is the variable of interest, we write the age of each person on the balls. If we instead are interested in if the people are allergic to pollen or not, we color the balls according to allergy status.

```{r urns, echo=FALSE, fig.cap="Urn models of a fair coin, age of a group of people, pollen alllergy status of a group of people.", out.width = '20%', fig.align="center", fig.show="hold"}
knitr::include_graphics("figures/coinurn.png")
knitr::include_graphics("figures/ageurn.png")
knitr::include_graphics("figures/pollenageurn.png")
```

By drawing balls from the urn with (or without) replacement probabilities and other properties of the model can be inferred.


 


## Discrete random variables

A discrete random number has countable number of outcomes values, such as {1,2,3,4,5,6}; {red, blue, green}; {tiny, small, average, large, huge} or all integers.

A discrete random variable can be described by its *probability mass function*, pmf.

The probability that the random variable, $X$, takes the value $x$ is denoted $P(X=x) = p(x)$. Note that:
  
1. $0 \leq p(x) \leq 1$, a probability is always between 0 and 1.
2. $\sum p(x) = 1$, the sum over all possible outcomes is 1.

```{example, name="The number of dots on a die", label="rolldie", echo=TRUE}
When rolling a die the there are six possible outcomes; 1, 2, 3, 4, 5 and 6, each of which have the same probability, if the die is fair. The outcome of one dice roll can be described by a random variable $X$. The probability of a particular outcome $x$ is denoted $P(X=x)$ or $p(x)$. 
```

The probability mass function of a fair six-sided die can be summarized in a table;

```{r}
kable(matrix(c(1:6,rep(1/6,6)),ncol=6, byrow=TRUE, dimnames=list(c('x','p(x)'), c()))) %>% kable_styling(full_width = FALSE)
```

or in a barplot;

```{r die, fig.height=3, fig.width=7, fig.cap="Probability mass function of a die.", out.width="45%", fig.align='center'}
plot(data.frame(x=1:6, p=1/6) %>% ggplot(aes(x=x, y=p)) + geom_bar(stat="identity") + theme_bw() + ylim(c(0,.25)))
``` 


```{example, name="The smoking status of a random mother", label="moke"}
The random variable has two possible outcomes; non-smoker (0) and smoker (1). The probability of a random mother being a smoker is 0.39.
```

```{r}
kable(matrix(c("0","0.61","1","0.39"),ncol=2, dimnames=list(c('x','p(x)'), c('non-smoker','smoker')))) %>% kable_styling(full_width = FALSE)
```

```{example, name="CFU", label="bacteria", echo=TRUE}
The number of bacterial colonies on a plate is a random number.
```

```{r CFU, fig.cap="Probability mass distribution of the number of bacterial colonies on an agar plate.", fig.height=3, fig.width=7}
x=1:50
ggplot(data.frame(x=x, fx=dpois(x, lambda=25)), aes(x,fx)) + geom_bar(stat="identity") + theme_bw() + ylab("p(x)")
```

#### Expected value

When the probability mass function is know the *expected value* of the random variable can be computed.

$$E[X] = \mu = \sum_{i=1}^N x_i p(x_i)$$
For a *uniform distribution*, where every object has the same probability (in the urn model, every object is represented by one ball), the expected value can be computed as the sum of all objects divided by the total number of objects;

$$E[X] = \mu = \frac{1}{N}\sum_{i=1}^N x_i$$

#### Variance

The variance is a measure of spread and is defined as the expected value of the squared distance from the population mean;

$$var(X) = \sigma^2 = E[(X-\mu)^2] = \sum_{i=1}^n (x_i-\mu)^2 p(x) dx$$


### Simulate distributions

Once the distribution is known, we can compute probabilities, such as $P(X=x), P(X<x)$ and $P(X \geq x)$. If the distribution is not known, simulation might be the solution.

```{example, name="Simulate coin toss", label="cointoss", echo=TRUE}
In a single coin toss the probabity of heads is 0.5. In 20 coin tosses, what is the probability of at least 15 heads?
```

The outcome of a single coin toss is a random variable, $X$ with two possible outcomes $\{H, T\}$. We know that $P(X=H) = 0.5$. The radnom variable of interest is the number of heads in 20 coin tosses, $Y$. The probability that we need to compute is $P(Y \geq 15)$.

```{r coinurn, echo=FALSE, fig.cap="A coin toss. Urn model with one black ball (heads) and one white ball (tails).", out.width = '20%', fig.align="center"}
knitr::include_graphics("figures/coinurn.png")
```

A single coin toss can be modelled by an urn with two balls. When a ball is drawn randomly from the urn, the probability to get the black ball (heads) is $P(X=H) = 0.5$.

If we want to simulate tossing 20 coins (or one coin 20 times) we can use the same urn model, if the ball is replaced after each draw.

In R we can simulate random draws from an urn model using the function `sample`.

```{r coin, echo=TRUE}
# A single coin toss
sample(c("H", "T"), size=1)
# Another coin toss
sample(c("H", "T"), size=1)
```

Every time you run the sample a new coin toss is simulated. 

The argument `size` tells the function how many balls we want to draw from the urn. To draw 20 balls from the urn, set `size=20,` remember to replace the ball after each draw!

```{r coins, echo=TRUE}
# 20 independent coin tosses
(coins <- sample(c("H", "T"), size=20, replace=TRUE))
```

How many heads did we get in the 20 random draws?

```{r, echo=TRUE}
# How many heads?
sum(coins == "H")
```

We can repeat this experiment (toss 20 coins and count the number of heads) several times to estimate the distribution of number of heads in 20 coin tosses.

To do the same thing several times we use the function `replicate.`

To simulate tossing 20 coins and counting the number of heads 10000 times, do the following;

```{r Nheads, echo=TRUE}
Nheads <- replicate(10000, {
  coins <- sample(c("H", "T"), size=20, replace=TRUE)
  sum(coins == "H")
})
```

Plot distribution of the number of heads in a histogram.

```{r histNheads, echo=TRUE}
hist(Nheads, breaks=0:20)
```

Now, let's get back to the question; when tossing 20 coins, what is the probability of at least 15 heads?

$P(X \geq 15)$

Count how many times out of our `r length(Nheads)` exeriments the number is 15 or greater

```{r, echo=TRUE}
sum(Nheads >= 15)
```

From this we conclude that

$P(X \geq 15) =$ `r sum(Nheads>=15)`/`r length(Nheads)` = `r sum(Nheads>=15)/length(Nheads)`



### Parametric discrete distributions

#### Bernoulli trial

A Bernoulli trial is a random experiment with two outcomes; success and failure. The probability of success, $P(success) = p$, is constant. The probability of failure is $P(failure) = 1-p$.

When coding it is convenient to code success as 1 and failure as 0.

The outcome of a Bernoulli trial is a discrete random variable, $X$.

```{r}
kable(matrix(c('x','0','1','p(x)','1-p','p'), byrow=TRUE, ncol=3)) %>% kable_styling("striped", full_width = FALSE)
```

Using the definitions of expected value and variance it can be shown that;

$$E[X] = p\\
var(X) = p(1-p)$$

#### Binomial distribution

The number of successes in a series of independent and identical Bernoulli trials is a discrete random variable, $X$.

<!-- $X = \sum_{i=0}^n Z_i,$ -->

The probability mass function of $X$ is called the binomial distribution. In short we use the notation;

$$X \in Bin(n, p)$$
Using the definition of expected value and variance it can be shown that;


The probability mass function is

$$P(X=k) = {n \choose k} p^k (1-p)^{n-k}$$

$$E[X] = np\\
var(X) = np(1-p)$$

The binomial distribution occurs when sampling $n$ objects **with** replacement from an urn with objects of two types, of which the interesting type has probability $p$.

The probability mass function, $P(X=k)$ can be computed using the R function `dbinom` and the cumulative distribution function $P(X \leq k)$ can be computed using `pbinom`.

#### Hypergeometric distribution

The hypergeometric distribution occurs when sampling $n$ objects **without** replacement from an urn with $N$ objects of two types, of which the interesting type has probability $p$.

The probability density function

$$P(X=k) = \frac{{Np \choose x} {N-Np \choose n-x}}{N \choose n}$$
can be computed in R using `dhyper` and the cumulative distribution function $P(X \ leq k)$ can be computed using `phyper`.

#### Poisson distribution

The Poisson distribution describe the number of times a rare event occurs in a large number of trials.

A rare disease has a very low probability for a single individual. The number of individuals in a large population that catch the disease in a certain time perion can be modelled using the Poisson distribution.

The probability mass function;

$$P(X=k) = \frac{\mu}{k!}e^{-\mu},$$
where $\mu$ is the expected value.


  
## Exercises

### Introduction to probability

```{exercise, probcoin, echo=TRUE}
When tossing a fair coin
  a) what is the probability of heads?
  b) what is the probability of tails?
```

```{exercise, probdie, echo=TRUE}
When tossing a fair six-sided die
  a) what is the probability of getting 6?
  b) what is the probability of an even number?
  c) what is the probability of getting 3 or more?
  d) what is the expected value of dots on the die´.
```

### Simulation

```{exercise, "Cointoss", echo=TRUE}
In a single coin toss the probability of heads is 0.5.

In 20 coin tosses,   

  a) what is the probability of exactly 15 heads?
  b) what is the probability of less than 7 heads?
  c) What is the most probable number of heads?
  d) what is the probability of 5 tails or less?
  e) what is the probability of 2 heads or less?
```

```{exercise, "Dice", echo=TRUE}
When rolling 10 six-sided dice, study the number of sixes.

 a) Define the random variable of interest
 b) What are the possible outcomes?
 c) Using simulation, estimate the probability mass function
 d) what is the probability to get at least 5 sixes?
 e) Which is the most likely number of sixes?
 f) hat is the probability to get exactly 2 sixes?
 g) On average how many sixes do you get when throwing ten dice?
  <!-- The law of large numbers states that if the same experiment is performed many times the average of the result will be close to the expected value. -->
```

```{exercise, "Cards", echo=TRUE}
A deck of cards consists of 52 cards; 13 diamonds, 13 spades, 13 hearts and 13 clubs. When five cards are randomly selected (a poker hand), what is the probability of getting all hearts?
```

```{exercise, label="Pollen", echo=TRUE}
30% of a large population is allergic to pollen. If you randomly select 3 people to participate in your study, what is the probability than none of them will be allergic to pollen?
```

```{r, eval=FALSE}
## Solution using 100 replicates
x <- replicate(100, sum(sample(0:1, size=3, replace=TRUE, prob=c(0.7,0.3))))
table(x)
mean(x==0)
## Solution using 1000 replicates
x <- replicate(1000, sum(sample(0:1, size=3, replace=TRUE, prob=c(0.7,0.3))))
table(x)
mean(x==0)
## Solution using 100000 replicates
x <- replicate(100000, sum(sample(0:1, size=3, replace=TRUE, prob=c(0.7,0.3))))
table(x)
mean(x==0)
## Solution using the Binomial distribution
pbinom(0, 3, 0.3)
```

```{exercise, label="Pollen2", echo=TRUE}
In a class of 20 students, 6 are allergic to pollen. If you randomly select 3 of the students to participate in your study, what is the probability than none of them will be allergic to pollen?
```

```{exercise, label="Pollen3", echo=TRUE}
Of the 200 persons working at a company, 60 are allergic to pollen. If you randomly select 3 people to participate in your study, what is the probability that none of them are allergic to pollen?
```

```{exercise, label="Pollen4", echo=TRUE}
Compare your results in \@ref(exr:Pollen), \@ref(exr:Pollen2) and \@ref(exr:Pollen3). Did you get the same results? Wy/why not?
```

### Parametric discrete distributions

```{exercise, label="pollencont", echo=TRUE}
Do exercises \@ref(exr:Pollen), \@ref(exr:Pollen2) and \@ref(exr:Pollen3) again, but using parametric distributions. Compare your results.
```

```{exercise, label="GSEA", name="Gene set enrichment analysis", echo=TRUE}
You have analyzed 20000 genes and a bioinformatician you are collaborating with has sent you a list of 1000 genes that she says are important. You are interested in a particular pathway A. 200 genes in pathway A are represented among the 20000 genes, 20 of these are in the bioinformaticians important list.

If the bioinformatician selected the 1000 genes at random, what is the probability to see 20 or more genes from pathway A in this list?
```


### Conditional probability

```{exercise, label="diagnostictests", echo=TRUE}
**Diagnostic tests**
```
  
```{r}
kable(matrix(c(98,882,980, 16, 4, 20, 114, 886, 1000), byrow = TRUE, ncol=3, dimnames=list(c("not cancer", "cancer", "total"), c("pos", "neg", "tot")))) %>% kable_styling("striped", full_width = FALSE)
```

a) What is the probability of a positive test result from a person with cancer?
b) What is the probability of a negative test result from a person without cancer?
c) If the test is positive, what is the probability of having cancer?
d) If the test is negative, what is the probability of not having cancer?
e) Connect the four computed probabilities with the following four tems;
  - Sensitivity
  - Specificity
  - Positive predictive value (PPV)
  - Negative predictive value (NPV)


## Continuous random variable

A continuous random number is not limited to discrete values, but any continuous number within one or several ranges is possible.

Examples: weight, height, speed, intensity, ...

A continuous random variable can be described by its *probability density function*, pdf.

```{r, fig.show="hold", fig.cap="Probability density function of the weight of a newborn baby.", fig.height=4, fig.width=7}
# ounce <- 0.0283495231
# pound <- 0.45359237
# ggplot(babies, aes(x=wt*ounce)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() #+ ggtitle("Weight (kg) of a newborn baby"))
#plot(ggplot(fat, aes(x=weight*pound)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() + ggtitle("Weight (kg) of a man"))
##Simulate weights
baby <- data.frame(smoke=sample(1000, x=0:2, prob=c(0.45,0.45,0.1), replace=TRUE)) %>% mutate(m=c(3.4, 3.0, 2.5)[smoke+1], wt=rnorm(n(), mean=m, sd=0.5))
ggplot(baby, aes(x=wt)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() 
```

```{r normpdf2, out.width="70%", fig.align="center", eval=FALSE}
x <- seq(-4,4,.01)
ggplot(data.frame(x=x, fx=dnorm(x)), aes(x,fx)) + geom_line() + theme_bw() + ylab("f(x)")
```

  The probability density function, $f(x)$, is defined such that the total area under the curve is 1.

$$
  \int_{-\infty}^{\infty} f(x) dx = 1
$$
  
```{r wtbabiesdens3, out.width="49%", warning=FALSE, message=FALSE, fig.show="hold", fig.keep="all"}
ggplot(baby, aes(x=wt)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() 
#plot(ggplot(data.frame(w=wt), aes(x=w)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density())

df.wt <- density(baby$wt)
df.wt <- data.frame(x=df.wt$x, y=df.wt$y)
plot(ggplot(baby, aes(x=wt)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() + geom_area(data=df.wt %>% filter(x<3.75, x>2.33), aes(x=x, y=y)) + scale_x_continuous(breaks=c(2,2.33,3,3.75,4,5), labels=c('2','a','3','b','4','5')) + geom_hline(yintercept=0) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_line(color = c("grey92", NA, "grey92", NA, "grey92", "grey92"))))
#plot(ggplot(data.frame(w=wt), aes(x=w)) + theme_bw() + xlab("x") + ylab("f(x)") + geom_density() + geom_area(data=df.wt %>% filter(x<3.75, x>2.33), aes(x=x, y=y)) + scale_x_continuous(breaks=c(2,2.33,3,3.75,4,5), labels=c('2','a','3','b','4','5')) + geom_hline(yintercept=0) + theme(panel.grid=element_blank()))
```

```{r pdfwtnorm, out.width="100%", eval=TRUE}
w<-seq(1.5,5.5,.01)
df.nwt <- data.frame(w=w, f=dnorm(w, 3.5, 0.5))
#ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("Weight (kg)") + ylab("f(x)")
```

```{r pdfab, eval=FALSE}
ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("Weight (kg)") + ylab("f(x)") + geom_area(data=df.nwt %>% filter(w<3.75, w>2.33)) + scale_x_continuous(breaks=c(2,2.33,3,3.75,4,5), labels=c('2','a','3','b','4','5')) + geom_hline(yintercept=0) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_line(color = c("grey92", NA, "grey92", NA, "grey92", "grey92")))
```

The area under the curve from a to b is the probability that the random variable $X$ takes a value between a and b.

$P(a \leq X \leq b) = \int_a^b f(x) dx$
  
<!-- #### Cumulative distribution function, cdf -->
  
The *cumulative distribution function*, cdf, sometimes called just the
distribution function, $F(x)$, is defined as:
  
  $$F(x) = P(X \leq x) = \int_{-\infty}^x f(x) dx$$
  
```{r wtpdfcdf, out.width="49%", fig.show="hold"}
plot(ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("x") + ylab("f(x)") + geom_area(data=df.nwt %>% filter(w<4.0)) + annotate("label",label=sprintf("P(X<4.0) = F(4.0) = %.2f", pnorm(4,3.5,0.5)), x=2.7, y=0.4, hjust=0))
df.nwt$F <- pnorm(df.nwt$w, 3.5, 0.5)
plot(ggplot(df.nwt, aes(x=w, y=F)) + geom_line() + xlab("x") + ylab("F(x)") + theme_bw() + geom_point(aes(x=4, y=pnorm(4,3.5,.5))) + annotate("label",label=sprintf("F(4.0)=%.2f", pnorm(4,3.5,.5)), x=4, y=.84, hjust=-0.2))##+ ggtitle("Cumulative distribution function") 
```

$$P(X \leq x) = F(x)$$
  
  As we know that the total probability (over all x) is 1, we can conclude that 

$$P(X > x) = 1 - F(x)$$
  and thus

$$P(a < X \leq b) = F(b) - F(a)$$
  
### Parametric continuous distributions

Two important parameters of a distribution is the expected value, $\mu$, that describe the distributions location and the variance, $\sigma^2$, that the spread.

The expected value, or population mean, is defined as;

$$E[X] = \mu = \int_{-\infty}^\infty x f(x) dx$$
We will learn more about the expected value and how to estimate a population mean from a sample later in the course.

The variance is defined as the expected value of the squared distance from the population mean;

$$\sigma^2 = E[(X-\mu)^2] = \int_{-\infty}^\infty (x-\mu)^2 f(x) dx$$

The square root of the variance is called the standard deviation, $\sigma$.

### Normal distribution

The normal distribution (sometimes referred to as the Gaussian distribution) is a common probability distribution and many continuous random variables can be described by the normal distribution or be approximated by the normal distribution.

The normal probability density function

$$f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2}$$
  
describes the distribution of a normal random variable, $X$, with expected value $\mu$ and standard deviation $\sigma$, $e$ and $\pi$ are two common mathematical constants, $e \approx 2.71828$ and $\pi \approx 3.14159$.

In short we write $X \sim N(\mu, \sigma)$.

```{r norm, out.width="50%", fig.show="hold", fig.align="center"}
#ggplot(pop.FN, aes(x=Bodyweight)) + geom_histogram(binwidth=1, aes(y=stat(density)), color="white") + theme_bw() + geom_line(data=den.FN, aes(x=x, y=nfx), color="red")
x <- seq(-3.5, 3.5, .1)
dN <- data.frame(x=x, fx=dnorm(x))
plot(ggplot(dN, aes(x=x, y=fx)) + geom_line() + scale_x_continuous(breaks=-3:3, labels=c(expression(mu-3*sigma),expression(mu-2*sigma), expression(mu-1*sigma), expression(mu), expression(mu+sigma), expression(mu + 2*sigma),  expression(mu + 3*sigma))) + xlab("") + ylab("f(x)") + theme_bw())
```

The bell-shaped normal distributions is symmetric around $\mu$ and $f(x) \rightarrow 0$ as $x \rightarrow \infty$ and as $x \rightarrow -\infty$.

As $f(x)$ is well defined, values for the cumulative distribution function $F(x) = \int_{- \infty}^x f(x) dx$ can be computed.

```{r out.width="45%", fig.show="hold"}
dN$Fx <- pnorm(x)
ggplot(dN, aes(x=x, y=fx)) + geom_line() + scale_x_continuous(breaks=-3:3, labels=c(expression(mu-3*sigma),expression(mu-2*sigma), expression(mu-1*sigma), expression(mu), expression(mu+sigma), expression(mu + 2*sigma),  expression(mu + 3*sigma))) + xlab("") + ylab("f(x)") + theme_bw() + ggtitle("Probability density function")
ggplot(dN, aes(x=x, y=Fx)) + geom_line() + scale_x_continuous(breaks=-3:3, labels=c(expression(mu-3*sigma),expression(mu-2*sigma), expression(mu-1*sigma), expression(mu), expression(mu+sigma), expression(mu + 2*sigma),  expression(mu + 3*sigma))) + xlab("") + ylab("F(x)") + theme_bw() + ggtitle("Cumulative distribution function")
```


If $X$ is normally distributed with expected value $\mu$ and
standard deviation $\sigma$ we write:
  
  $$X \sim N(\mu, \sigma)$$
  
  Using transformation rules we can define

$$Z = \frac{X-\mu}{\sigma}, \, Z \sim N(0,1)$$ 
  
Values for the cumulative standard normal distribution, $F(z)$, are tabulated and easy to compute in R using the function ``pnorm``.

```{r FZ, out.width="50%", fig.cap="The shaded area under hte curve is the tabulated value $P(Z \\leq z) = F(z)$."}
w<-seq(-3.5,3.5,.01)
df.nwt <- data.frame(w=w, f=dnorm(w))
plot(ggplot(df.nwt, aes(x=w, y=f)) + geom_line() + theme_bw() + xlab("x") + ylab("f(x)") + geom_area(data=df.nwt %>% filter(w<0.7)) + annotate("label",label=sprintf("P(Z<z) = F(z)"), x=-1.0, y=0.1, hjust=0) + scale_x_continuous(breaks=c(-2,0,+.7,2), labels=c("-2","0","z","2")) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_line(color = c("grey92", "grey92", NA, "grey92"))))
```


```{r}
z <- sapply(seq(-3.4, -3.49, -0.01), function(x) seq(x,0,0.1))
z <- -z[nrow(z):1,]
z <- sapply(seq(0, 0.09, 0.01), function(x) seq(x,3.49,0.1))
zscore.df <- apply(pnorm(z), 2, function(x) sprintf("%.4f",x))
row.names(zscore.df) <- sprintf("%.1f", z[,1])
colnames(zscore.df) <- seq(0,0.09,0.01)
kable(zscore.df, caption="Normal distribution. The table gives $F(z) = P(Z \\leq z)$ for $Z \\in N(0,1)$.") %>% kable_styling()
```

Some value of particular interest:
  
$$F(1.64) = 0.95\\
F(1.96) = 0.975$$

As the normal distribution is symmetric F(-z) = 1 - F(z) 

$$F(-1.64) = 0.05\\
F(-1.96) = 0.025$$

$$P(-1.96 < Z < 1.96) = 0.95$$

<!-- Show table? -->
  
  <!-- dnorm -->
  
  <!-- pnorm -->
  
#### Sum of two normal random variables
  
  If $X \sim N(\mu_1, \sigma_1)$ and $Y \sim N(\mu_2, \sigma_2)$ are two independent normal random variables, then their sum is also a random variable:
  
  $$X + Y \sim N(\mu_1 + \mu_2, \sqrt{\sigma_1^2 + \sigma_2^2})$$
  
  and 

$$X - Y \sim N(\mu_1 - \mu_2, \sqrt{\sigma_1^2 + \sigma_2^2})$$
  This can be extended to the case with $n$ independent and identically distributed random varibles $X_i$ ($i=1 \dots n$). If all $X_i$ are normally distributed with mean $\mu$ and standard deviation $\sigma$, $X_i \in N(\mu, \sigma)$, then the sum of all $n$ random variables will also be normally distributed with mean $n\mu$ and standard deviation $\sqrt{n} \sigma$.


### Central limit theorem

```{theorem, label="CLT", echo=TRUE}
The sum of $n$ independent and equally distributed random variables
is normally distributed, if $n$ is large enough.
```


As a result of central limit theorem, the distribution of fractions or mean values of a sample follow the normal distribution, at least if the sample is large enough (a rule of thumb is that the sample size $n>30$).


<!-- ```{example, "Mean BMI", eval=FALSE} -->
  <!-- Percentage of body fat, age, weight, height, BMI and ten body circumference -->
  <!-- measurements are recorded for 252 men. Consider these 252 as a population and compute the population mean ans standard deviation. -->
  <!-- ``` -->
  
```{example, name="Mean BMI", label="BMdistr", echo=TRUE}
In a population of 252 men we can study the distribution of BMI.
```

```{r fatdata}
fat <- read.table("http://jse.amstat.org/datasets/fat.dat.txt")
colnames(fat) <- c("case","body.fat","body.fat.siri","density","age","weight","height","BMI","ffweight","neck","chest","abdomen","hip","thigh","knee","ankle" ,"bicep","forearm","wrist" )
```

```{r BMIhist, out.width="60%"}
pl <- ggplot(fat, aes(x=BMI)) + geom_histogram(aes(y=stat(density)), binwidth=2, color="white") + theme_bw()
plot(pl)
```


```{r BMI, echo=TRUE}
##Population mean
(mu <- mean(fat$BMI))
##Population variance
(sigma2 <- var(fat$BMI)/nrow(fat)*(nrow(fat)-1))
##Population standard variance
(sigma <- sqrt(sigma2))
```

Randomly sample 3, 5, 10, 15, 20, 30 men and compute the mean value, $m$. Repeat many times to get the distribution of mean values.

```{r owexample, out.width="70%", fig.width=7, fig.show="hold"}
#hist(fat$BMI)
bmi <- fat$BMI
n <- c(3,5,10,15,20, 30)
rs <- sapply(n, function(k) replicate(10000, mean(sample(bmi, k))))
colnames(rs) <- paste(sprintf("n=%i, m=%.4f", n, colMeans(rs)))
plot(ggplot(melt(rs, varnames=c("rep", "n")), aes(x=value, color=factor(n))) + geom_density() + theme_bw() + facet_wrap(~n))
```

Note, mean is just the sum divided by the number of samples $n$.

### $\chi^2$-distribution

The random variable $Y = \sum_{i=1}^n X_i^2$ is $\chi^2$ distributed with $n-1$ degrees of freedom, if $X_i$ are independent identically distributed random variables $X_i \in N(0,1)$.

In short $Y \in \chi^2(n-1)$.

```{r Xdistr, fig.width=7, fig.height=7, fig.cap="The $\\chi^2$-distribution."}
x <- seq(-0,5, .01)
dX <- data.frame(x=x, n=rep(c(2, 3,4,5,7, 10), each=length(x))) %>%
  mutate(fx=dchisq(x, df=n-1))
#dT <- data.frame(x=x, fx=dt(x, df=n-1))
ggplot(dX, aes(x=x, y=fx, color=factor(paste0("n=",n), levels=paste0("n=",sort(unique(n)))))) + geom_line() + theme_bw() + scale_color_discrete("") + ylim(c(0,0.5))
```

```{example, label="chisq", echo=FALSE}
$\frac{(n-1)S^2}{\sigma^2} is $\chi^2$ distributed with $n-1$ degrees of freedom.
```
<!-- Example. $\chi^2$-test for variance -->
  
### F-distribution
  
The ratio of two $\chi^2$-distributed variables divided by their degrees of freedom is F-distributed

```{r Fdistr, fig.width=7, fig.height=7, fig.cap="The F-distribution"}
x <- seq(-0,5, .01)
dX <- data.frame(x=x, n1=rep(c(2,4,10), each=length(x)), n2=rep(c(2,4,10), each=length(x)*3)) %>%
  mutate(fx=df(x, df1=n1-1, df2=n2-1))
#dT <- data.frame(x=x, fx=dt(x, df=n-1))
ggplot(dX, aes(x=x, y=fx, color=factor(sprintf("n1=%i, n2=%i",n1, n2), levels=sprintf("n1=%i, n2=%i",rep(sort(unique(n1)), each=3), sort(unique(n2)))))) + geom_line() + theme_bw() + scale_color_discrete("") + ylim(c(0,0.8))
```

```{example, label="Fdist", echo=TRUE}
The ratio of two sample variances is F-distributed
```
<!-- Example. F-test of equality of variances -->
  
### t-distribution
  
The ratio of a normally distributed variable and a $\chi^2$-distributed variable is t-distributed.

```{r exampletdistr, fig.width=7, fig.height=7, fig.cap="The t-distribution."}
x <- seq(-3.5,3.5, .01)
dT <- data.frame(x=x, n=rep(c(2, 3,5,7, 10, 15, 20, 30), each=length(x))) %>%
  mutate(fx=dt(x, df=n-1))
#dT <- data.frame(x=x, fx=dt(x, df=n-1))
ggplot(dT, aes(x=x, y=fx, color=factor(paste0("n=",n), levels=paste0("n=",sort(unique(n)))))) + geom_line() + theme_bw() + scale_color_discrete("")
```

```{example, name="t-distribution", label="tdistr", echo=TRUE}
The ratio between sample mean and sample variance is t-distributed.
```

## Exercises

### Normal distribution

```{exercise, label="normtable", name="Exercise in using the normal table", echo=TRUE}
  
Let $Z \sim N(0,1)$ be a standard normal random variable, and compute;

  a. $P(Z<1.64)$
  b. $P(Z>-1.64)$
  c. $P(-1.96<Z)$
  d. $P(Z<2.36)$
  e. An $a$ such that $P(Z<a) = 0.95$
  f. An $b$ such that $P(Z>b) = 0.975$
```

```{exercise, label="ztransform", name="Exercise in standardization/transformation", echo=TRUE}
If $X \sim N(3,2)$, compute the probabilities
  
  a. $P(X<5)$
  b. $P(3<X<5)$
  c. $P(X \geq 7)$
```


