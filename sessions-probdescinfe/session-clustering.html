<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Clustering | Probability theory, inference and clustering</title>
  <meta name="description" content="4 Clustering | Probability theory, inference and clustering" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Clustering | Probability theory, inference and clustering" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Clustering | Probability theory, inference and clustering" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="session-inference.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="session-probability.html"><a href="session-probability.html"><i class="fa fa-check"></i><b>1</b> Probability theory</a><ul>
<li class="chapter" data-level="1.1" data-path="session-probability.html"><a href="session-probability.html#introduction-to-probability"><i class="fa fa-check"></i><b>1.1</b> Introduction to probability</a><ul>
<li class="chapter" data-level="1.1.1" data-path="session-probability.html"><a href="session-probability.html#axioms-of-probability"><i class="fa fa-check"></i><b>1.1.1</b> Axioms of probability</a></li>
<li class="chapter" data-level="1.1.2" data-path="session-probability.html"><a href="session-probability.html#common-rules-and-theorems"><i class="fa fa-check"></i><b>1.1.2</b> Common rules and theorems</a></li>
<li class="chapter" data-level="1.1.3" data-path="session-probability.html"><a href="session-probability.html#conditional-probability"><i class="fa fa-check"></i><b>1.1.3</b> Conditional probability</a></li>
<li class="chapter" data-level="1.1.4" data-path="session-probability.html"><a href="session-probability.html#random-variables"><i class="fa fa-check"></i><b>1.1.4</b> Random variables</a></li>
<li class="chapter" data-level="1.1.5" data-path="session-probability.html"><a href="session-probability.html#the-urn-model"><i class="fa fa-check"></i><b>1.1.5</b> The urn model</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="session-probability.html"><a href="session-probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>1.2</b> Discrete random variables</a><ul>
<li class="chapter" data-level="1.2.1" data-path="session-probability.html"><a href="session-probability.html#expected-value"><i class="fa fa-check"></i><b>1.2.1</b> Expected value</a></li>
<li class="chapter" data-level="1.2.2" data-path="session-probability.html"><a href="session-probability.html#variance"><i class="fa fa-check"></i><b>1.2.2</b> Variance</a></li>
<li class="chapter" data-level="1.2.3" data-path="session-probability.html"><a href="session-probability.html#simulate-distributions"><i class="fa fa-check"></i><b>1.2.3</b> Simulate distributions</a></li>
<li class="chapter" data-level="1.2.4" data-path="session-probability.html"><a href="session-probability.html#parametric-discrete-distributions"><i class="fa fa-check"></i><b>1.2.4</b> Parametric discrete distributions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="session-probability.html"><a href="session-probability.html#exercises-discrete-random-variables"><i class="fa fa-check"></i>Exercises: Discrete random variables</a><ul>
<li class="chapter" data-level="" data-path="session-probability.html"><a href="session-probability.html#solutions-discrete-random-variables"><i class="fa fa-check"></i>Solutions: Discrete random variables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="session-probability.html"><a href="session-probability.html#continuous-random-variable"><i class="fa fa-check"></i><b>1.3</b> Continuous random variable</a><ul>
<li class="chapter" data-level="1.3.1" data-path="session-probability.html"><a href="session-probability.html#parametric-continuous-distributions"><i class="fa fa-check"></i><b>1.3.1</b> Parametric continuous distributions</a></li>
<li class="chapter" data-level="1.3.2" data-path="session-probability.html"><a href="session-probability.html#normal-distribution"><i class="fa fa-check"></i><b>1.3.2</b> Normal distribution</a></li>
<li class="chapter" data-level="1.3.3" data-path="session-probability.html"><a href="session-probability.html#central-limit-theorem"><i class="fa fa-check"></i><b>1.3.3</b> Central limit theorem</a></li>
<li class="chapter" data-level="1.3.4" data-path="session-probability.html"><a href="session-probability.html#chi2-distribution"><i class="fa fa-check"></i><b>1.3.4</b> <span class="math inline">\(\chi^2\)</span>-distribution</a></li>
<li class="chapter" data-level="1.3.5" data-path="session-probability.html"><a href="session-probability.html#f-distribution"><i class="fa fa-check"></i><b>1.3.5</b> F-distribution</a></li>
<li class="chapter" data-level="1.3.6" data-path="session-probability.html"><a href="session-probability.html#t-distribution"><i class="fa fa-check"></i><b>1.3.6</b> t-distribution</a></li>
<li class="chapter" data-level="1.3.7" data-path="session-probability.html"><a href="session-probability.html#distributions-in-r-1"><i class="fa fa-check"></i><b>1.3.7</b> Distributions in R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="session-probability.html"><a href="session-probability.html#exercises-continuous-random-variables"><i class="fa fa-check"></i>Exercises: Continuous random variables</a><ul>
<li class="chapter" data-level="" data-path="session-probability.html"><a href="session-probability.html#solutions"><i class="fa fa-check"></i>Solutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="session-descstats.html"><a href="session-descstats.html"><i class="fa fa-check"></i><b>2</b> Descriptive statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="session-descstats.html"><a href="session-descstats.html#data-types"><i class="fa fa-check"></i><b>2.1</b> Data types</a></li>
<li class="chapter" data-level="2.2" data-path="session-descstats.html"><a href="session-descstats.html#measures-of-location"><i class="fa fa-check"></i><b>2.2</b> Measures of location</a><ul>
<li class="chapter" data-level="2.2.1" data-path="session-descstats.html"><a href="session-descstats.html#expected-value-1"><i class="fa fa-check"></i><b>2.2.1</b> Expected value</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="session-descstats.html"><a href="session-descstats.html#measures-of-spread"><i class="fa fa-check"></i><b>2.3</b> Measures of spread</a><ul>
<li class="chapter" data-level="2.3.1" data-path="session-descstats.html"><a href="session-descstats.html#variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.3.1</b> Variance and standard deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="session-descstats.html"><a href="session-descstats.html#random-sample"><i class="fa fa-check"></i><b>2.4</b> Random sample</a><ul>
<li class="chapter" data-level="2.4.1" data-path="session-descstats.html"><a href="session-descstats.html#the-urn-model-to-perform-simple-random-sampling"><i class="fa fa-check"></i><b>2.4.1</b> The urn model to perform simple random sampling</a></li>
<li class="chapter" data-level="2.4.2" data-path="session-descstats.html"><a href="session-descstats.html#sample-properties"><i class="fa fa-check"></i><b>2.4.2</b> Sample properties</a></li>
<li class="chapter" data-level="2.4.3" data-path="session-descstats.html"><a href="session-descstats.html#sample-mean-and-standard-deviation"><i class="fa fa-check"></i><b>2.4.3</b> Sample mean and standard deviation</a></li>
<li class="chapter" data-level="2.4.4" data-path="session-descstats.html"><a href="session-descstats.html#standard-error"><i class="fa fa-check"></i><b>2.4.4</b> Standard error</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="session-descstats.html"><a href="session-descstats.html#exercises-descriptive-statistics"><i class="fa fa-check"></i>Exercises: Descriptive statistics</a><ul>
<li class="chapter" data-level="" data-path="session-descstats.html"><a href="session-descstats.html#solutions-descriptive-statistics"><i class="fa fa-check"></i>Solutions: Descriptive statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="session-inference.html"><a href="session-inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="session-inference.html"><a href="session-inference.html#hypothesis-test"><i class="fa fa-check"></i><b>3.1</b> Hypothesis test</a><ul>
<li class="chapter" data-level="3.1.1" data-path="session-inference.html"><a href="session-inference.html#the-null-and-alternative-hypothesis"><i class="fa fa-check"></i><b>3.1.1</b> The null and alternative hypothesis</a></li>
<li class="chapter" data-level="3.1.2" data-path="session-inference.html"><a href="session-inference.html#to-perform-a-hypothesis-test"><i class="fa fa-check"></i><b>3.1.2</b> To perform a hypothesis test</a></li>
<li class="chapter" data-level="3.1.3" data-path="session-inference.html"><a href="session-inference.html#significance-level-and-error-types"><i class="fa fa-check"></i><b>3.1.3</b> Significance level and error types</a></li>
<li class="chapter" data-level="3.1.4" data-path="session-inference.html"><a href="session-inference.html#hypothesis-test-simulation-examples"><i class="fa fa-check"></i><b>3.1.4</b> Hypothesis test, simulation examples</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="session-inference.html"><a href="session-inference.html#exercises-hypothesis-tests-resampling"><i class="fa fa-check"></i>Exercises: Hypothesis tests, resampling</a></li>
<li class="chapter" data-level="3.2" data-path="session-inference.html"><a href="session-inference.html#parametric-tests"><i class="fa fa-check"></i><b>3.2</b> Parametric tests</a><ul>
<li class="chapter" data-level="3.2.1" data-path="session-inference.html"><a href="session-inference.html#one-sample-proportions"><i class="fa fa-check"></i><b>3.2.1</b> One sample, proportions</a></li>
<li class="chapter" data-level="3.2.2" data-path="session-inference.html"><a href="session-inference.html#one-sample-mean"><i class="fa fa-check"></i><b>3.2.2</b> One sample, mean</a></li>
<li class="chapter" data-level="3.2.3" data-path="session-inference.html"><a href="session-inference.html#two-samples-proportions"><i class="fa fa-check"></i><b>3.2.3</b> Two samples, proportions</a></li>
<li class="chapter" data-level="3.2.4" data-path="session-inference.html"><a href="session-inference.html#two-samples-mean"><i class="fa fa-check"></i><b>3.2.4</b> Two samples, mean</a></li>
<li class="chapter" data-level="3.2.5" data-path="session-inference.html"><a href="session-inference.html#variance-1"><i class="fa fa-check"></i><b>3.2.5</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="session-inference.html"><a href="session-inference.html#exercises-hypothesis-tests-parametric"><i class="fa fa-check"></i>Exercises: Hypothesis tests, parametric</a><ul>
<li class="chapter" data-level="" data-path="session-inference.html"><a href="session-inference.html#solutions-hypothesis-tests-parametric"><i class="fa fa-check"></i>Solutions: Hypothesis tests, parametric</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="session-inference.html"><a href="session-inference.html#point-and-interval-estimates"><i class="fa fa-check"></i><b>3.3</b> Point and interval estimates</a><ul>
<li class="chapter" data-level="3.3.1" data-path="session-inference.html"><a href="session-inference.html#bootstrap-interval"><i class="fa fa-check"></i><b>3.3.1</b> Bootstrap interval</a></li>
<li class="chapter" data-level="3.3.2" data-path="session-inference.html"><a href="session-inference.html#confidence-interval"><i class="fa fa-check"></i><b>3.3.2</b> Confidence interval</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="session-inference.html"><a href="session-inference.html#exercises-interval-estimates"><i class="fa fa-check"></i>Exercises: Interval estimates</a><ul>
<li class="chapter" data-level="3.3.3" data-path="session-inference.html"><a href="session-inference.html#solutions-1"><i class="fa fa-check"></i><b>3.3.3</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="session-inference.html"><a href="session-inference.html#multiple-testing"><i class="fa fa-check"></i><b>3.4</b> Multiple testing</a><ul>
<li class="chapter" data-level="3.4.1" data-path="session-inference.html"><a href="session-inference.html#error-types"><i class="fa fa-check"></i><b>3.4.1</b> Error types</a></li>
<li class="chapter" data-level="3.4.2" data-path="session-inference.html"><a href="session-inference.html#bonferroni-correction"><i class="fa fa-check"></i><b>3.4.2</b> Bonferroni correction</a></li>
<li class="chapter" data-level="3.4.3" data-path="session-inference.html"><a href="session-inference.html#benjamini-hochbergs-fdr"><i class="fa fa-check"></i><b>3.4.3</b> Benjamini-Hochbergs FDR</a></li>
<li class="chapter" data-level="3.4.4" data-path="session-inference.html"><a href="session-inference.html#adjusted-p-values"><i class="fa fa-check"></i><b>3.4.4</b> ‘Adjusted’ p-values</a></li>
<li class="chapter" data-level="" data-path="session-inference.html"><a href="session-inference.html#example-10000-independent-tests-e.g.genes"><i class="fa fa-check"></i>Example, 10000 independent tests (e.g. genes)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="session-clustering.html"><a href="session-clustering.html"><i class="fa fa-check"></i><b>4</b> Clustering</a><ul>
<li class="chapter" data-level="4.1" data-path="session-clustering.html"><a href="session-clustering.html#k-means"><i class="fa fa-check"></i><b>4.1</b> K-means</a></li>
<li class="chapter" data-level="4.2" data-path="session-clustering.html"><a href="session-clustering.html#dissimilarity-matrix"><i class="fa fa-check"></i><b>4.2</b> Dissimilarity matrix</a></li>
<li class="chapter" data-level="4.3" data-path="session-clustering.html"><a href="session-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>4.3</b> Hierarchical clustering</a><ul>
<li class="chapter" data-level="4.3.1" data-path="session-clustering.html"><a href="session-clustering.html#agglomerative-clustering"><i class="fa fa-check"></i><b>4.3.1</b> Agglomerative clustering</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="session-clustering.html"><a href="session-clustering.html#exercises-clustering"><i class="fa fa-check"></i>Exercises: Clustering</a><ul>
<li class="chapter" data-level="4.3.2" data-path="session-clustering.html"><a href="session-clustering.html#solutions-2"><i class="fa fa-check"></i><b>4.3.2</b> Solutions</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Probability theory, inference and clustering</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="session-clustering" class="section level1">
<h1><span class="header-section-number">4</span> Clustering</h1>
<p>Clustering is about grouping objects together according to similarity. The objects are grouped into clusters so that objectes within the same cluster are more similar to one another than to objects in other clusters.</p>
<p>Clustering is usually performed based on a set of <span class="math inline">\(n\)</span> objects, each with <span class="math inline">\(p\)</span> measurements. A single object <span class="math inline">\(i\)</span> can thus be described by the vector <span class="math inline">\({\mathbf x}_i = [x_{i1}, x_{i2}, \dots, x_{ip}]\)</span>.</p>
<p>With only <span class="math inline">\(p=2\)</span> measurements these measurements can easily be plotted and we could illustrate clusters by colors.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters"></span>
<img src="sessions-probdescinfe_files/figure-html/clusters-1.png" alt="Three clusters" width="70%" />
<p class="caption">
Figure 4.1: Three clusters
</p>
</div>
<p>Clustering is commonly used for data exploration and to identify substructures in a data set.</p>
<p>There are many types of clustering algorithms, here we will only discuss <em>K-means</em> and <em>hierarchical clustering</em>.</p>
<div id="k-means" class="section level2">
<h2><span class="header-section-number">4.1</span> K-means</h2>
<p>The K-means algorith aims to dive all objects into exactly <span class="math inline">\(K\)</span> clusters. <span class="math inline">\(K\)</span> has to be given to the algorithm. The K-means algorithm minimize the variance within clusters, by iteratively assigning each object to the cluster with the closest mean (centroid).</p>
<p>The centroid of cluster <span class="math inline">\(k\)</span> is the arithmetic mean of all <span class="math inline">\(n_k\)</span> objects in the cluster</p>
<p><span class="math display">\[{\mathbf m}_k = \frac{1}{n_k} \sum_{i=1}^{n_k} {\mathbf x}_{i}\]</span></p>
<p>The algorithm can be performed as follows;</p>
<ol start="0" style="list-style-type: decimal">
<li>Initialization. Select <span class="math inline">\(k\)</span> initial centroids.</li>
<li>Assign each object to the closest centroid (in terms of squared Euclidean distance).
The squared Euclidean distance between an object (a data point) and a cluster centroid <span class="math inline">\(m_k\)</span> is
<span class="math inline">\(d_i = \sum_{j=1}^p (x_{ij} - m_{kj})^2\)</span>. By assigning each object to closest centroid the total within cluster sum of squares (WSS) is minimized.
<span class="math display">\[WSS = \sum_{k=1}^K\sum_{i \in C_k}\sum_{j=1}^p (x_{ij} - m_{kj})^2\]</span>
<!-- For every object ${\mathbf x}_i$ assign the object to the cluster $\arg \min_l \sum_j (x_{ij} - c_{lj})^2$ --></li>
<li>Update the centroids for each of the <span class="math inline">\(k\)</span> clusters by computing the centroid for all the objects in each of the clusters.</li>
<li>Repeat 1-2 until convergence</li>
</ol>
<p>The initial centroids can be selected in several different ways. Two common methods are;</p>
<ul>
<li>Forgy’s method: Select <span class="math inline">\(K\)</span> data points as initial centroids</li>
<li>Randomly assign each data point to one out of <span class="math inline">\(K\)</span> clusters and compute the centroids for these initial clusters.</li>
</ul>
</div>
<div id="dissimilarity-matrix" class="section level2">
<h2><span class="header-section-number">4.2</span> Dissimilarity matrix</h2>
<p>All clustering algorithms need a measure of similarity or dissimilarity between objects. As a similarity can be transformed to a dissimilarity, we will here focus on dissimilaities.</p>
<p>Dissimilarities between all pairs of objects can be described in a dissimilarity matrix. Most algorithms are based on symmetric dissimilarities, i.e. when the dissimilarity between a and b is the same as between b and a. Also, most algorithm require non-negative dissimilarities.</p>
<p>K-means uses the squared Euclidean distance as a dissimilarity measure, but there of course other ways to measure the dissimilarity between two objects (data points).</p>
<!-- An objects can usually be described by a set of $p$ measurements, for $n$ objects we have the measurements $x_{ij},$ where $i=1,\dots,n$ and $j=1,\dots,p$. -->
<p>Common dissimilarity measures include;</p>
<p><em>Euclidean distance</em>
<span class="math display">\[d_{euc} (x, y) = \sqrt{\sum_{j=1}^{p} (x_j - y_j)^2}\]</span>
<em>Squared Euclidean distance</em>
<span class="math display">\[d_{euc} (x, y) = \sum_{j=1}^{p} (x_j - y_j)^2\]</span></p>
<p><em>Manhattan distance</em>
<span class="math display">\[d_{man} (x, y) = \sqrt{\sum_{j=1}^{p} |x_j - y_j|}\]</span>
<em>Pearson correlation distance</em></p>
<p>Pearson’s correlation is a similarity measure</p>
<p><span class="math display">\[r = \frac{\sum_{j=1}^p(x_j-\bar x)(y_i-\bar y)}{\sqrt{\sum_{j=1}^p(x_j-\bar x)^2\sum_{j=1}^p(y_j-\bar y)^2}}\]</span></p>
<p>Using a transformation we can compute a Pearson’s correlation distance</p>
<p><span class="math display">\[d_{pear}(x,y) = \sqrt{1-r}\]</span></p>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2><span class="header-section-number">4.3</span> Hierarchical clustering</h2>
<p>Hierarchical clustering does not require the number of clusters to be specified. Instead of creating a single set of clusters it creates a hierarchy of clusterings based on pairwise dissimilarities.</p>
<div class="figure"><span id="fig:dendro"></span>
<img src="sessions-probdescinfe_files/figure-html/dendro-1.png" alt="A hierarchical cluster dendrogram." width="4200" />
<p class="caption">
Figure 4.2: A hierarchical cluster dendrogram.
</p>
</div>
<p>There are two strategies for hierarchical clustering <em>agglomerative</em> (bottom-up) and <em>divisive</em> (top-down).</p>
<p>The agglomerative strategy starts att the bottom with all objects in separate clusters and at each level merge a pair of clusters. The merged pair of clusters are those with the smallest dissimilarity.</p>
<p>The divisive strategy starts at the top with all objects in a single cluster and at each level one cluster is split into two. The split is chosen to produce the two groups with the largest possible dissimilarity.</p>
<p>With <span class="math inline">\(n\)</span> objects to cluster both strategies will produce a dendrogram representing the <span class="math inline">\(n-1\)</span> levels in the hierarchy. Each level represent a specific clustering of the objects into disjoint clusters. The heights of the branches in the dendrogram are proportional to the dissimilarity of the merged/split clusters.</p>
<div id="agglomerative-clustering" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Agglomerative clustering</h3>
<p>Agglomerative clustering starts with all objects in separate clusters and at each level merge the pair of clusters with the smallest dissimilarity. The pairwise dissilimarities between objects are known, but a method for computing dissimilarity between clusters is needed, as so called <em>linkage method</em>.</p>
<p>The dissimilarity between two clusters A and B with objects <span class="math inline">\(a_1, \dots, a_{n_A}\)</span> and <span class="math inline">\(b_1, \dots, b_{n_B}\)</span>, respectively, can be computed using one of several linkage methods.</p>
<div id="single-linkage" class="section level4 unnumbered">
<h4>Single linkage</h4>
<p>Single linkage takes as a cluster dissimilarity the distance between the two closest objects in the two clusters.</p>
<p><span class="math display">\[d_{sl}(A, B) = \min_{i,j} d(a_i, b_j)\]</span></p>
</div>
<div id="complete-linkage" class="section level4 unnumbered">
<h4>Complete linkage</h4>
<p>Complete linkage takes as a cluster dissimilarity the distance between the two objects furthest apart in the two clusters.</p>
<p><span class="math display">\[d_{cl}(A, B) = \max_{i,j} d(a_i, b_j)\]</span></p>
</div>
<div id="average-linkage" class="section level4 unnumbered">
<h4>Average linkage</h4>
<p>Average linkage takes as a cluster dissimilarity the average distance between the objects in the in the two clusters.</p>
<p><span class="math display">\[d_{al}(A, B) = \frac{1}{n_A n_B}\sum_i\sum_j d(a_i, b_j)\]</span></p>
<p>Single, complete and average linkage are the most common linkage methods and these can be combined with any pairwise dissimilarity measures.</p>
</div>
<div id="wards-linkage" class="section level4 unnumbered">
<h4>Ward’s linkage</h4>
<p>Ward’s linkage method minimize the within variance, by merging clusters with the minimum increase in within sum of squares.</p>
<p><span class="math display">\[d_{wl}(A, B) = \sum_{i=1}^{n_A} (a_i - m_{A \cup B})^2 + \sum_{i=1}^{n_B} (b_i - m_{A \cup B})^2 - \sum_{i=1}^{n_A} (a_i - m_{A})^2 - \sum_{i=1}^{n_B} (b_i - m_{B})^2\]</span>
where <span class="math inline">\(m_A, m_B, m_{A \cup B}\)</span> are the center of the clusters <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(A \cup B\)</span>, respectively.</p>
<p>Note that Ward’s linkage method should not be combined with any dissimilarity matrix as it is based on the squared Euclidean distance. In the R function <code>hclust</code> either the Euclidean or squared Euclidean distance can be used in combination with the linkage <code>method='ward.D'</code> or <code>method='ward.D2</code>, respectively.</p>
<div class="figure"><span id="fig:dendromulti"></span>
<img src="sessions-probdescinfe_files/figure-html/dendromulti-1.png" alt="Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods." width="4200" /><img src="sessions-probdescinfe_files/figure-html/dendromulti-2.png" alt="Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods." width="4200" /><img src="sessions-probdescinfe_files/figure-html/dendromulti-3.png" alt="Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods." width="4200" /><img src="sessions-probdescinfe_files/figure-html/dendromulti-4.png" alt="Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods." width="4200" />
<p class="caption">
Figure 4.3: Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods.
</p>
</div>
</div>
</div>
</div>
<div id="exercises-clustering" class="section level2 unnumbered">
<h2>Exercises: Clustering</h2>

<div class="exercise">
<span id="exr:five" class="exercise"><strong>Exercise 4.1  </strong></span>Based on a two dimensional data set you will investigate K-means clustering.
</div>

<p>Download the two-dimensional <a href="clusterdata.csv">data set</a>.</p>
<ol style="list-style-type: lower-alpha">
<li>Try to cluster the data using the k-means algorithm (function <code>kmeans</code> in R). Use Forgy’s method for initialization and select a value for <span class="math inline">\(k\)</span>. If you get a message that the algorithm did not converge in 10 iterations, try increasing the number of iterations</li>
<li>Plot the data and color by cluster id</li>
<li>Run the same analysis again, do you get the same results? (You can compare two vectors of class identities using the function <code>table</code>.)</li>
<li>By setting the argument <code>nstart</code> to 4 the algorithm will automatically try four different (random) starting points.</li>
<li>Try different values for <span class="math inline">\(k\)</span>, run the k-means algorithm and collect the WSS (‘tot.withinss’). Plot WSS vs k. Thw WSS is always decreasing as k increases, but the curve can still give you a hint of which <span class="math inline">\(k\)</span> to choose. The Elbow method for selecting <span class="math inline">\(k\)</span> is to look at this curve and choose the <span class="math inline">\(k\)</span> that you find at the bend of the curve, at the ‘elbow’.
Which <span class="math inline">\(k\)</span> would you pick based on this?</li>
</ol>

<div class="exercise">
<p><span id="exr:NCI60" class="exercise"><strong>Exercise 4.2  </strong></span>The NCI60 data set consists of gene expression values for 6830 genes for 64 cell lines.</p>
Using this data set investigate a few hierarchical clustering distances and linkage methods.
</div>

<p>The data can be downloaded in R using the following command</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" data-line-number="1">nci.data &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data.csv&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;,&quot;</span>,<span class="dt">row.names=</span><span class="dv">1</span>,<span class="dt">header=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb107-2" data-line-number="2">nci.label &lt;-<span class="st"> </span><span class="kw">scan</span>(<span class="st">&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.label&quot;</span>, <span class="dt">what=</span><span class="st">&quot;&quot;</span>)</a></code></pre></div>
<ul>
<li>What is the size of the data matrix? Do every column represent a gene or a cell line?</li>
<li>Compute the Euclidean distance between cell lines.
This can be accomplished using the function <code>dist</code>. Read the help text <code>?dist</code>. This function computes the distance between rows of the input data matrix. If the rows represent cell lines, you can run <code>dist(nci.data)</code>, but if your cell lines are represented by columns you need to transpose the data matrix first <code>dist(t(nci.data))</code>.</li>
<li>Cluster the cell lines using complete linkage hierarchical clustering, use the function <code>hclust</code>.</li>
<li>Plot the result! (If you read the help text about <code>hclust</code> you should know how to plot)
<ul>
<li>Try changing the labels to something more informative (such as <code>label=nci.label</code>).</li>
<li>Investigate the argument <code>hang</code>, what happens if you set it to -1?</li>
</ul></li>
<li>Try the linkage methods “single”, “average” and “ward.D” in addition to “complete”. Compare the results. Which method do you think is ‘best’?</li>
<li>Pick the tree resulting from the method you think is ‘best’. How many clusters are there?
You can cut the tree on any level to get between 1 and 64 clusters. The function <code>cutree</code> cuts either on a specific height (dissimilarity) or to get a specific number of clusters.</li>
<li>Compute the Pearson correlation between the cell lines, use the function <code>cor</code></li>
<li>Convert the correlation matrix to a dissimilarity matrix and cluster using complete or average linkage.</li>
</ul>
<div id="solutions-2" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Solutions</h3>
<p><a href="session-clustering.html#exr:five">4.1</a></p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1"><span class="co">## Load the data</span></a>
<a class="sourceLine" id="cb108-2" data-line-number="2">df &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;clusterdata.csv&quot;</span>)</a>
<a class="sourceLine" id="cb108-3" data-line-number="3"></a>
<a class="sourceLine" id="cb108-4" data-line-number="4"><span class="co">## K-means with k=5, this uses Forgy&#39;s method, you can also try the default Hariigan-Wong</span></a>
<a class="sourceLine" id="cb108-5" data-line-number="5">km1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df[, <span class="kw">c</span>(<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>)], <span class="dt">centers=</span><span class="dv">5</span>, <span class="dt">algorithm =</span><span class="st">&quot;Forgy&quot;</span>)<span class="op">$</span>cluster</a></code></pre></div>
<pre><code>## Warning: did not converge in 10 iterations</code></pre>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" data-line-number="1"><span class="co">## Letter is just to name the clusters, A, B, C, ...</span></a>
<a class="sourceLine" id="cb110-2" data-line-number="2">df<span class="op">$</span>km1 &lt;-<span class="st"> </span>LETTERS[km1]</a>
<a class="sourceLine" id="cb110-3" data-line-number="3"></a>
<a class="sourceLine" id="cb110-4" data-line-number="4"><span class="co">##Plot</span></a>
<a class="sourceLine" id="cb110-5" data-line-number="5">df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y, <span class="dt">color=</span>km1)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</a></code></pre></div>
<p><img src="sessions-probdescinfe_files/figure-html/exrfive-1.png" width="6000" /></p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" data-line-number="1"><span class="co">##Run kmeans again</span></a>
<a class="sourceLine" id="cb111-2" data-line-number="2">km2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df[, <span class="kw">c</span>(<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>)], <span class="dt">centers=</span><span class="dv">5</span>, <span class="dt">algorithm =</span><span class="st">&quot;Forgy&quot;</span>)<span class="op">$</span>cluster</a>
<a class="sourceLine" id="cb111-3" data-line-number="3">df<span class="op">$</span>km2 &lt;-<span class="st"> </span>LETTERS[km2]</a>
<a class="sourceLine" id="cb111-4" data-line-number="4">df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y, <span class="dt">color=</span>km2)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</a></code></pre></div>
<p><img src="sessions-probdescinfe_files/figure-html/exrfive-2.png" width="6000" /></p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" data-line-number="1"><span class="co">## Automatically try several different starting points</span></a>
<a class="sourceLine" id="cb112-2" data-line-number="2">km3 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df[, <span class="kw">c</span>(<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>)], <span class="dt">centers=</span><span class="dv">5</span>, <span class="dt">algorithm =</span><span class="st">&quot;Forgy&quot;</span>, <span class="dt">nstart=</span><span class="dv">10</span>)<span class="op">$</span>cluster</a></code></pre></div>
<pre><code>## Warning: did not converge in 10 iterations

## Warning: did not converge in 10 iterations</code></pre>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" data-line-number="1">df<span class="op">$</span>km3 &lt;-<span class="st"> </span>LETTERS[km3]</a>
<a class="sourceLine" id="cb114-2" data-line-number="2">df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y, <span class="dt">color=</span>km3)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</a></code></pre></div>
<p><img src="sessions-probdescinfe_files/figure-html/exrfive-3.png" width="6000" /></p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1"><span class="co">## Try different k and compute within sum of squares</span></a>
<a class="sourceLine" id="cb115-2" data-line-number="2">WSS &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="cf">function</span>(k) <span class="kw">kmeans</span>(df[, <span class="kw">c</span>(<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>)], <span class="dt">centers=</span>k, <span class="dt">algorithm =</span><span class="st">&quot;Forgy&quot;</span>, <span class="dt">nstart=</span><span class="dv">10</span>, <span class="dt">iter.max=</span><span class="dv">20</span>)<span class="op">$</span>tot.withinss)</a></code></pre></div>
<pre><code>## Warning: did not converge in 20 iterations</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1"><span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, WSS)</a></code></pre></div>
<p><img src="sessions-probdescinfe_files/figure-html/exrfive-4.png" width="6000" /></p>
<p><a href="session-clustering.html#exr:NCI60">4.2</a></p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1">nci.data &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data.csv&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;,&quot;</span>,<span class="dt">row.names=</span><span class="dv">1</span>,<span class="dt">header=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb118-2" data-line-number="2">nci.label &lt;-<span class="st"> </span><span class="kw">scan</span>(<span class="st">&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.label&quot;</span>, <span class="dt">what=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb118-3" data-line-number="3"></a>
<a class="sourceLine" id="cb118-4" data-line-number="4"><span class="co">## Size of data matrix (first number is rows, second number of columns)</span></a>
<a class="sourceLine" id="cb118-5" data-line-number="5"><span class="kw">dim</span>(nci.data)</a></code></pre></div>
<pre><code>## [1] 6830   64</code></pre>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1"><span class="co">## Euclidean distance</span></a>
<a class="sourceLine" id="cb120-2" data-line-number="2">deucl &lt;-<span class="st"> </span><span class="kw">dist</span>(<span class="kw">t</span>(nci.data), <span class="dt">method =</span> <span class="st">&quot;euclidean&quot;</span>)</a>
<a class="sourceLine" id="cb120-3" data-line-number="3"></a>
<a class="sourceLine" id="cb120-4" data-line-number="4"><span class="co">## Complete linkage</span></a>
<a class="sourceLine" id="cb120-5" data-line-number="5">hcl &lt;-<span class="st"> </span><span class="kw">hclust</span>(deucl, <span class="dt">method =</span> <span class="st">&quot;complete&quot;</span>)</a>
<a class="sourceLine" id="cb120-6" data-line-number="6"></a>
<a class="sourceLine" id="cb120-7" data-line-number="7"><span class="co">## plot the dendrogram</span></a>
<a class="sourceLine" id="cb120-8" data-line-number="8"><span class="kw">plot</span>(hcl, <span class="dt">labels=</span>nci.label)</a></code></pre></div>
<p><img src="sessions-probdescinfe_files/figure-html/exrnci-1.png" width="6000" /></p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb121-1" data-line-number="1"><span class="kw">plot</span>(hcl, <span class="dt">labels=</span>nci.label, <span class="dt">hang=</span><span class="op">-</span><span class="dv">1</span>)</a></code></pre></div>
<p><img src="sessions-probdescinfe_files/figure-html/exrnci-2.png" width="6000" /></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1"><span class="co">## Compute using other linkage methods</span></a>
<a class="sourceLine" id="cb122-2" data-line-number="2">hsl &lt;-<span class="st">  </span><span class="kw">hclust</span>(deucl, <span class="dt">method =</span> <span class="st">&quot;single&quot;</span>)</a>
<a class="sourceLine" id="cb122-3" data-line-number="3">hal &lt;-<span class="st">  </span><span class="kw">hclust</span>(deucl, <span class="dt">method =</span> <span class="st">&quot;average&quot;</span>)</a>
<a class="sourceLine" id="cb122-4" data-line-number="4">hwl &lt;-<span class="st">  </span><span class="kw">hclust</span>(deucl, <span class="dt">method =</span> <span class="st">&quot;ward.D&quot;</span>)</a>
<a class="sourceLine" id="cb122-5" data-line-number="5"></a>
<a class="sourceLine" id="cb122-6" data-line-number="6"><span class="co">##Plot using same command as above</span></a>
<a class="sourceLine" id="cb122-7" data-line-number="7"></a>
<a class="sourceLine" id="cb122-8" data-line-number="8"><span class="co">## Pearson correlation</span></a>
<a class="sourceLine" id="cb122-9" data-line-number="9">r &lt;-<span class="st"> </span><span class="kw">cor</span>(nci.data, <span class="dt">method =</span> <span class="st">&quot;pearson&quot;</span>)</a>
<a class="sourceLine" id="cb122-10" data-line-number="10">dcor &lt;-<span class="st"> </span><span class="kw">as.dist</span>(<span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">-</span>r))</a>
<a class="sourceLine" id="cb122-11" data-line-number="11">hcor &lt;-<span class="st"> </span><span class="kw">hclust</span>(dcor, <span class="dt">method=</span><span class="st">&quot;complete&quot;</span>)</a>
<a class="sourceLine" id="cb122-12" data-line-number="12"><span class="kw">plot</span>(hcor, <span class="dt">labels=</span>nci.label)</a></code></pre></div>
<p><img src="sessions-probdescinfe_files/figure-html/exrnci-3.png" width="6000" /></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="session-inference.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
